{
  "id": "72711cc9-1916-4be6-8e69-fd8af3d1fb45",
  "title": "Azure Architecture Review",
  "category": [
    {
      "id": "63821ae5-4124-4a4e-9a1a-872cc17fa8da",
      "name": "Resiliency",
      "questions": [
        {
          "id": "199f4a63-51c4-447e-bffe-5ca72999b7e6",
          "title": "Which business metrics have you defined for your application?",
          "explanation": "",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Define your own target SLAs for each workload in your solution so you can determine whether the architecture meets the business requirements.\n\n### Consider cost and complexity\n\nEverything else being equal, higher availability is better. But as you strive for more nines, the cost and complexity grow. An uptime of 99.99% translates to about five minutes of total downtime per month. Is it worth the additional complexity and cost to reach five nines? The answer depends on the business requirements.\n\nHere are some other considerations when defining an SLA:\n\n- To achieve four nines (99.99%), you can't rely on manual intervention to recover from failures. The application must be self-diagnosing and self-healing.\n- Beyond four nines, it's challenging to detect outages quickly enough to meet the SLA.\n- Think about the time window that your SLA is measured against. The smaller the window, the tighter the tolerances. It doesn't make sense to define your SLA in terms of hourly or daily uptime.\n- Consider the MTBF and MTTR measurements. The higher your SLA, the less frequently the service can go down and the quicker the service must recover.\n- Get agreement from your customers for the availability targets of each piece of your application, and document it. Otherwise, your design may not meet the customers' expectations.\n\n### Identify dependencies\n\nPerform dependency-mapping exercises to identify internal and external dependencies. Examples include dependencies relating to security or identity, such as Active Directory, or third-party services such as a payment provider or e-mail messaging service.\n\nPay particular attention to external dependencies that might be a single point of failure or cause bottlenecks. If a workload requires 99.99% uptime but depends on a service with a 99.9% SLA, that service can't be a single point of failure in the system. One remedy is to have a fallback path in case the service fails. Alternatively, take other measures to recover from a failure in that service.\n\nThe following table shows the potential cumulative downtime for various SLA levels.\n\n| **SLA** | **Downtime per week** | **Downtime per month** | **Downtime per year** |\n|---------|-----------------------|------------------------|-----------------------|\n| 99%     | 1.68 hours            | 7.2 hours              | 3.65 days             |\n| 99.9%   | 10.1 minutes          | 43.2 minutes           | 8.76 hours            |\n| 99.95%  | 5 minutes             | 21.6 minutes           | 4.38 hours            |\n| 99.99%  | 1.01 minutes          | 4.32 minutes           | 52.56 minutes         |\n| 99.999% | 6 seconds             | 25.9 seconds           | 5.26 minutes          |",
              "priority": "medium",
              "title": "You have set your workload availability targets",
              "id": "5ef838c4-65f5-4b58-86a5-87816c352969",
              "heading": "Workload availability targets"
            },
            {
              "id": "43dbf753-9ef5-41b0-8321-e28baae6de2f",
              "title": "You have identified how long the workload can be down for, and how much data it's acceptable to lose in a disaster.",
              "answer_tooltip": "",
              "output": "Derive these values by conducting a risk assessment, and make sure you understand the cost and risk of downtime and data loss. These are nonfunctional requirements of a system and should be dictated by business requirements.\n\n- **Recovery time objective (RTO)** is the maximum acceptable time an application is unavailable after an incident.\n- **Recovery point objective (RPO)** is the maximum duration of data loss that's acceptable during a disaster.\n\nIf the MTTR value of *any* critical component in a highly available setup exceeds the system RTO, a failure in the system might cause an unacceptable business disruption. That is, you can't restore the system within the defined RTO.",
              "priority": "low",
              "heading": "Recovery metrics"
            },
            {
              "id": "7830153d-5353-40ea-8689-f576d12a0eab",
              "title": "You have identified your redundancy requirements and SLAs",
              "answer_tooltip": "Use these measures to plan for redundancy and determine customer SLAs.",
              "output": "Use these measures to plan for redundancy and determine customer SLAs.\n\n- **Mean time to recover (MTTR)** is the average time it takes to restore a component after a failure.\n- **Mean time between failures (MTBF)** is the how long a component can reasonably expect to last between outages.",
              "priority": "medium",
              "heading": "Availability metrics"
            },
            {
              "id": "a453c16c-13c8-4aa0-9f27-e26b77747138",
              "title": "You understand Azure Service Level Agreements (SLAs) and how they interact with your workload.",
              "answer_tooltip": "An agreement on how much downtime is acceptible for a given service.  Should be defined for both internal and external",
              "output": "In Azure, the [Service Level Agreement](https://azure.microsoft.com/en-us/support/legal/sla/) describes Microsoft's commitments for uptime and connectivity. If the SLA for a particular service is 99.9%, you should expect the service to be available 99.9% of the time. Different services have different SLAs.\n\nThe Azure SLA also includes provisions for obtaining a service credit if the SLA is not met, along with specific definitions of *availability* for each service. That aspect of the SLA acts as an enforcement policy.\n\n### Composite SLAs\n\n*Composite SLAs* involve multiple services supporting an application, each with differing levels of availability. For example, consider an App Service web app that writes to Azure SQL Database. At the time of this writing, these Azure services have the following SLAs:\n\n- App Service web apps = 99.95%\n- SQL Database = 99.99%\n\nWhat is the maximum downtime you would expect for this application? If either service fails, the whole application fails. The probability of each service failing is independent, so the composite SLA for this application is 99.95% × 99.99% = 99.94%. That's lower than the individual SLAs, which isn't surprising because an application that relies on multiple services has more potential failure points.\n\nYou can improve the composite SLA by creating independent fallback paths. For example, if SQL Database is unavailable, put transactions into a queue to be processed later.\n\n![Composite SLA](_images/composite-sla.png)\n\nWith this design, the application is still available even if it can't connect to the database. However, it fails if the database and the queue both fail at the same time. The expected percentage of time for a simultaneous failure is 0.0001 × 0.001, so the composite SLA for this combined path is:\n\n- Database *or* queue = 1.0 − (0.0001 × 0.001) = 99.99999%\n\nThe total composite SLA is:\n\n- Web app *and* (database *or* queue) = 99.95% × 99.99999% = \\~99.95%\n\nThere are tradeoffs to this approach. The application logic is more complex, you are paying for the queue, and you need to consider data consistency issues.\n\n### SLAs for multiregion deployments\n\n*SLAs for multiregion deployments* involve a high-availability technique to deploy the application in more than one region and use Azure Traffic Manager to fail over if the application fails in one region.\n\nThe composite SLA for a multiregion deployment is calculated as follows:\n\n- *N* is the composite SLA for the application deployed in one region.\n- *R* is the number of regions where the application is deployed.\n\nThe expected chance that the application fails in all regions at the same time is ((1 − N) \\^ R). For example, if the single-region SLA is 99.95%:\n\n- The combined SLA for two regions = (1 − (1 − 0.9995) \\^ 2) = 99.999975%\n- The combined SLA for four regions =  (1 − (1 − 0.9995) \\^ 4)  = 99.999999%\n\nThe [SLA for Traffic Manager](https://azure.microsoft.com/en-us/support/legal/sla/traffic-manager/v1_0/) is also a factor. Failing over is not instantaneous in active-passive configurations, which can result in downtime during a failover. See [Traffic Manager endpoint monitoring and failover](/azure/traffic-manager/traffic-manager-monitoring).",
              "priority": "low",
              "heading": "Understand service-level agreements"
            }
          ],
          "display_logic": [],
          "heading": "Business Metrics"
        },
        {
          "id": "16b80f68-d855-4c45-b162-6477bf13fe9a",
          "title": "How are you managing your data?",
          "explanation": "",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "answer_tooltip": "",
              "output": "### Azure SQL Database\n\nSQL Database automatically performs a combination of full database backups weekly, differential database backups hourly, and transaction log backups every five - ten minutes to protect your business from data loss. Use point-in-time restore to restore a database to an earlier time. For more information, see:\n\n- [Recover an Azure SQL database using automated database backups](/azure/sql-database/sql-database-recovery-using-backups)\n\n- [Overview of business continuity with Azure SQL Database](/azure/sql-database/sql-database-business-continuity)\n\n### SQL Server on VMs\n\nFor SQL Server running on VMs, there are two options: traditional backups and log shipping. Traditional backups enables you to restore to a specific point in time, but the recovery process is slow. Restoring traditional backups requires starting with an initial full backup, and then applying any backups taken after that. The second option is to configure a log shipping session to delay the restore of log backups (for example, by two hours). This provides a window to recover from errors made on the primary.\n\n### Azure Cosmos DB\n\nAzure Cosmos DB automatically takes backups at regular intervals. Backups are stored separately in another storage service, and those backups are globally replicated for resiliency against regional disasters. If you accidentally delete your database or collection, you can file a support ticket or call Azure support to restore the data from the last automatic backup. For more information, see [Automatic online backup and restore with Azure Cosmos DB](/azure/cosmos-db/online-backup-and-restore).\n\n### Azure Database for MySQL, Azure Database for PostgreSQL\n\nWhen using Azure Database for MySQL or Azure Database for PostgreSQL, the database service automatically makes a backup of the service every five minutes. Using this automatic backup feature you may restore the server and all its databases into a new server to an earlier point-in-time. For more information, see:\n\n- [How to back up and restore a server in Azure Database for MySQL by using the Azure portal](/azure/mysql/howto-restore-server-portal)\n\n- [How to backup and restore a server in Azure Database for PostgreSQL using the Azure portal](/azure/postgresql/howto-restore-server-portal)",
              "priority": "medium",
              "title": "Database's are architected for resiliency",
              "id": "a50d8886-9beb-493e-87da-fc752cf3cf23",
              "heading": "Database resiliency"
            },
            {
              "id": "f7d4b058-15d1-42f5-9ee2-362b3b68ee0d",
              "title": "Databases are replicated geographically when appropriate",
              "answer_tooltip": "Application becomes unavailable in case of outage or unavailability of primary database.",
              "output": "### SQL Database\n\nAzure SQL Database provides two types of recovery: geo-restore and active geo-replication.\n\n#### Geo-restore\n\n[Geo-restore](/azure/sql-database/sql-database-recovery-using-backups/#geo-restore) is also available with Basic, Standard, and Premium databases. It provides the default recovery option when the database is unavailable because of an incident in the region where your database is hosted. Similar to point-in-time restore, geo-restore relies on database backups in geo-redundant Azure storage. It restores from the geo-replicated backup copy, and therefore is resilient to the storage outages in the primary region. For more information, see [Restore an Azure SQL Database or failover to a secondary](/azure/sql-database/sql-database-disaster-recovery).\n\n#### Active geo-replication\n\n[Active geo-replication](/azure/sql-database/sql-database-geo-replication-overview) is available for all database tiers. It’s designed for applications that have more aggressive recovery requirements than geo-restore can offer. Using active geo-replication, you can create up to four readable secondaries on servers in different regions. You can initiate failover to any of the secondaries. In addition, active geo-replication can be used to support the application upgrade or relocation scenarios, as well as load balancing for read-only workloads. For details, see [Configure active geo-replication for Azure SQL Database and initiate failover](/azure/sql-database/sql-database-geo-replication-portal). Refer to [Designing globally available services using Azure SQL Database](/azure/sql-database/sql-database-designing-cloud-solutions-for-disaster-recovery) and [Managing rolling upgrades of cloud applications by using SQL Database active geo-replication](/azure/sql-database/sql-database-manage-application-rolling-upgrade) for details on how to design and implement applications and applications upgrade without downtime.\n\n### SQL Server on Azure Virtual Machines\n\nA variety of options are available for recovery and high availability for SQL Server 2012 (and later) running in Azure Virtual Machines. For more information, see [High availability and disaster recovery for SQL Server in Azure Virtual Machines](/azure/virtual-machines/windows/sql/virtual-machines-windows-sql-high-availability-dr).",
              "priority": "medium",
              "heading": "Distribute data geographically"
            },
            {
              "id": "69748949-e0f4-46bf-8b99-b49bbc2ebf07",
              "title": "Data consistency and concurrency are documented",
              "answer_tooltip": "Transactions are queued, affecting application availability when load starts to increase.",
              "output": "Practice appropriate consistency and isolation level while making database connection.",
              "priority": "medium",
              "heading": "Data consistancy and concurrancy"
            },
            {
              "id": "d9805b44-8d42-4009-8db5-227a5e249d1a",
              "title": "Storage is architected for resiliency",
              "answer_tooltip": "Inability to serve user requests when storage and/or database service fails.",
              "output": "Azure Storage provides data resiliency through automated replicas. However, this does not prevent application code or users from corrupting data, whether accidentally or maliciously. Maintaining data fidelity in the face of application or user error requires more advanced techniques, such as copying the data to a secondary storage location with an audit log.\n\n- **Block blobs**. Create a point-in-time snapshot of each block blob. For more information, see [Creating a Snapshot of a Blob](/rest/api/storageservices/creating-a-snapshot-of-a-blob). For each snapshot, you are only charged for the storage required to store the differences within the blob since the last snapshot state. The snapshots are dependent on the existence of the original blob they are based on, so a copy operation to another blob or even another storage account is advisable. This ensures that backup data is properly protected against accidental deletion. You can use [AzCopy](/azure/storage/common/storage-use-azcopy) or [Azure PowerShell](/azure/storage/common/storage-powershell-guide-full) to copy the blobs to another storage account.\n\n- **Files**. Use [share snapshots](/azure/storage/files/storage-snapshots-files), or use AzCopy or PowerShell to copy your files to another storage account.\n\n- **Tables**. Use AzCopy to export the table data into another storage account in another region.",
              "priority": "medium",
              "heading": "Storage resiliency"
            },
            {
              "id": "7d4dd0b4-6552-47e3-96c5-9b98aef3b50c",
              "title": "Using seperate user accounts for production and backup databases",
              "answer_tooltip": "A single user can maliciously delete _all_ data (original and backup) resulting in compromised backup.",
              "output": "Keep user permissions separate between production and backup data.",
              "priority": "medium",
              "heading": "Backup credential isolation"
            },
            {
              "id": "5b251e1d-3352-4f3f-8ac6-edbadb0c1471",
              "title": "Failover and fallback processes are orchestrated and tested",
              "answer_tooltip": "Inability to verify that an operator following the processes can successfully fail over and fail back the data source.",
              "output": "Use auto-failover and active geo-replication for SQL Database. Employ Azure Managed Database Services for built-in resiliency.",
              "priority": "medium",
              "heading": "Automated failover and failback"
            }
          ],
          "display_logic": [],
          "heading": "Data Management"
        },
        {
          "id": "ccd41e5e-b572-4d0e-a487-1f5fd9c440a7",
          "title": "How are you managing errors & failures?",
          "explanation": "Ensuring your application can recover from errors is critical when working in a distributed system",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "id": "3128430d-7c25-49da-97eb-643d29f1149c",
              "title": "Retries for transient errors are impelmented and logged",
              "answer_tooltip": "Possibility of an issue or failure remaining hidden by an application's retry logic.",
              "output": "Track the number of transient exceptions and retries over time to uncover issues or failures in your application's retry logic. A trend of increasing exceptions over time may indicate that the service is having an issue and may fail. For more information, see [Retry service specific guidance](../best-practices/retry-service-specific.md).",
              "priority": "medium",
              "heading": "Transient failure handling"
            },
            {
              "id": "5c44424c-38f4-45a8-8d38-57cb34869f29",
              "title": "Request timeouts are configured",
              "answer_tooltip": "Very short timeouts can cause excessive retry operations for services and resources that have considerable latency. Very long timeouts can cause blocking if a large number of requests are queued, waiting for a service or resource to respond.",
              "output": "Set SQL Connection timeout to 30s. Use guidance on troubleshoot, diagnose, and prevent SQL connection errors and transient errors for SQL Database.",
              "priority": "medium",
              "heading": "Request timeouts"
            },
            {
              "id": "2d348cc5-c6e0-4f9d-a29a-827f57527e5f",
              "title": "Implemented the \"Circuit Breaker\" pattern to prevent cascading failures",
              "answer_tooltip": "Failure in one part of the system can lead to cascading failures, resulting in many operations becoming blocked while holding onto critical system resources, such as memory, database connections.",
              "output": "Use the Circuit Breaker pattern.",
              "priority": "medium",
              "heading": "Cascading Failures"
            },
            {
              "id": "309f1127-3a9e-4876-b5dd-91bade63f789",
              "title": "Application components are split with seperate health probes",
              "answer_tooltip": "Inability to detect and avoid sending requests to failed instances minimizes availability.",
              "output": "Configure and test health probes for your load balancers and traffic managers. Ensure that your health endpoint checks the critical parts of the system and responds appropriately.\n\n- For [Azure Traffic Manager](/azure/traffic-manager/traffic-manager-overview/), the health probe determines whether to fail over to another region. Your health endpoint should check any critical dependencies that are deployed within the same region.\n- For [Azure Load Balancer](/azure/load-balancer/load-balancer-overview/), the health probe determines whether to remove a VM from rotation. The health endpoint should report the health of the VM. Don't include other tiers or external services. Otherwise, a failure that occurs outside the VM will cause the load balancer to remove the VM from rotation.\n\nFor guidance on implementing health monitoring in your application, see [Health Endpoint Monitoring pattern](../patterns/health-endpoint-monitoring.md).",
              "priority": "medium",
              "heading": "Application Health Probes"
            },
            {
              "id": "c9dbb912-a194-4b28-9f04-1ebb17eb711c",
              "title": "Command and Query Responsibility Segregation (CQRS) is implemented on data stores",
              "answer_tooltip": "Command (INSERT/UPDATE) and Query (SELECT) operations target the same resource (database) affecting availability.",
              "output": "Use the CQRS pattern in Azure.",
              "priority": "medium",
              "heading": "Command and Query Responsibility Segregation (CQRS)"
            }
          ],
          "display_logic": [],
          "heading": "App Design - Error Handling"
        },
        {
          "id": "0b34cfb4-fe80-4179-a339-885eec73a5d3",
          "title": "How are you handling DR (Backup & Restore) for this workload?",
          "explanation": "*Disaster recovery* is the process of restoring application functionality in the wake of a catastrophic loss.\n\nYour tolerance for reduced functionality during a disaster is a business decision that varies from one application to the next. It might be acceptable for some applications to be completely unavailable or to be partially available with reduced functionality or delayed processing for a period of time. For other applications, any reduced functionality is unacceptable.",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "answer_tooltip": "",
              "output": "For each dependent service, you should understand the implications of a service disruption and the way that the application will respond. Many services include features that support resiliency and availability, so evaluating each service independently is likely to improve your disaster recovery plan. For example, Azure Event Hubs supports [failing over](/azure/event-hubs/event-hubs-geo-dr#setup-and-failover-flow) to the secondary namespace.",
              "priority": "medium",
              "title": "You have a plan for dependency failures",
              "id": "33fe1493-7dbd-48de-ab6a-d6635c9c7c68",
              "heading": "Dependent service outage"
            },
            {
              "answer_tooltip": "",
              "output": "When parts of the Azure network are inaccessible, you might not be able to access your application or data. In this situation, we recommend designing the disaster recovery strategy to run most applications with reduced functionality.\n\nIf reducing functionality isn't an option, the remaining options are application downtime or failover to an alternate region.\n\nIn a reduced functionality scenario:\n\n- If your application can't access its data because of an Azure network outage, you might be able to run locally with reduced application functionality by using cached data.\n- You might be able to store data in an alternate location until connectivity is restored.",
              "priority": "medium",
              "title": "There is a response plan in place for network outages.",
              "id": "5be928fb-5256-48be-ae77-6c7c932a7371",
              "heading": "Network outage"
            },
            {
              "answer_tooltip": "",
              "output": "Although automation is ideal, some strategies for disaster recovery require manual responses.\n\n### Alerts\n\nMonitor your application for warning signs that may require proactive intervention. For example, if Azure SQL Database or Azure Cosmos DB consistently throttles your application, you might need to increase your database capacity or optimize your queries. Even though the application might handle the throttling errors transparently, your telemetry should still raise an alert so that you can follow up.\n\nFor service limits and quota thresholds, we recommend configuring alerts on Azure resources metrics and diagnostics logs. When possible, set up alerts on metrics, which are lower latency than diagnostics logs.\n\nThrough [Resource Health](/azure/service-health/resource-health-checks-resource-types), Azure provides some built-in health status checks that can help you diagnose Azure service throttling issues.\n\n### Failover\n\nConfigure a disaster recovery strategy for each Azure application and its Azure services. Acceptable deployment strategies to support disaster recovery may vary based on the SLAs required for all components of each application.  \n\nAzure provides different features within many Azure services to allow for manual failover, such as [redis cache geo-replicas](/azure/azure-cache-for-redis/cache-how-to-geo-replication), or for automated failover, such as [SQL auto-failover groups](/azure/sql-database/sql-database-auto-failover-group). For example:\n\n- For an application that mainly uses virtual machines, you can use Azure Site Recovery for the web and logic tiers. For more information, see [Azure to Azure disaster recovery architecture](/azure/site-recovery/azure-to-azure-architecture). For SQL Server on VMs, use [SQL Server Always On availability groups](/azure/virtual-machines/windows/sql/virtual-machines-windows-portal-sql-availability-group-dr).\n- For an application that uses App Service and Azure SQL Database, you can use a smaller tier App Service plan configured in the secondary region, which autoscales when a failover occurs. Use failover groups for the database tier.\n\nIn either scenario, an [Azure Traffic Manager](/azure/traffic-manager/traffic-manager-overview) profile provides for the automated failover across regions. [Load balancers](/azure/load-balancer/load-balancer-overview) or [application gateways](/azure/application-gateway/overview) should be set up in the secondary region to support faster availability on failover.\n\n### Operational readiness testing\n\nPerform an operational readiness test for failover to the secondary region and for failback to the primary region. Many Azure services support manual failover or test failover for disaster recovery drills. Alternatively, you can simulate an outage by shutting down or removing Azure services.",
              "priority": "medium",
              "title": "You have manual responses defined where automation doesn't exist",
              "id": "2da87328-516c-4ff1-a653-3dcce115f12a",
              "heading": "Manual responses"
            },
            {
              "answer_tooltip": "",
              "output": "If a data store fails, there might be data inconsistencies when it becomes available again, especially if the data was replicated. Understanding the recovery time objective (RTO) and recovery point objective (RPO) of replicated data stores can help you predict the amount of data loss.\n\nTo understand whether the cross-regional failover is started manually or by Microsoft, review the Azure service SLAs. For services with no SLAs for cross-regional failover, Microsoft typically decides when to fail over and usually prioritizes recovery of data in the primary region. If data in the primary region is deemed unrecoverable, Microsoft fails over to the secondary region.\n\n### Restoring data from backups\n\nBackups protect you from losing a component of the application because of accidental deletion or data corruption. They preserve a functional version of the component from an earlier time, which you can use to restore it.\n\nDisaster recovery strategies are not a replacement for backups, but regular backups of application data support some disaster recovery scenarios. Your backup storage choices should be based on your disaster recovery strategy.\n\nThe frequency of running the backup process determines your RPO. For example, if you perform hourly backups and a disaster occurs two minutes before the backup, you will lose 58 minutes of data. Your disaster recovery plan should include how you will address lost data.\n\nIt's common for data in one data store to reference data in another store. For example, consider a SQL Database with a column that links to a blob in Azure Storage. If backups don't happen simultaneously, the database might have a pointer to a blob that wasn't backed up before the failure. The application or the disaster recovery plan must implement processes to handle this inconsistency after a recovery.\n\n> [!NOTE]\n> In some scenarios, such as that of VMs backed up using [Azure Backup](/azure/backup/backup-azure-vms-first-look-arm), you can restore only from a backup in the same region. Other Azure services, such as [Azure Cache for Redis](/azure/azure-cache-for-redis/cache-how-to-geo-replication), provide geo-replicated backups, which you can use to restore services across regions.\n\n### Azure Storage and Azure SQL Database\n\nAzure automatically stores Azure Storage and SQL Database data three times within different fault domains in the same region. If you use geo-replication, the data is stored three additional times in a different region. However, if the data is corrupted or deleted in the primary copy (for example, because of user error), the changes replicate to the other copies.\n\nYou have two options for managing potential data corruption or deletion:\n\n- **Create a custom backup strategy.** You can store your backups in Azure or on-premises, depending on your business requirements and governance regulations.\n- **Use the point-in-time restore option** to recover a SQL Database.\n\n#### Azure Storage recovery\n\nYou can develop a custom backup process for Azure Storage or use one of many third-party backup tools.\n\nAzure Storage provides data resiliency through automated replicas, but it doesn't prevent application code or users from corrupting data. Maintaining data fidelity after application or user error requires more advanced techniques, such as copying the data to a secondary storage location with an audit log. You have several options:\n\n- [**Block blobs.**](/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs) Create a point-in-time snapshot of each block blob. For each snapshot, you are charged only for the storage required to store the differences within the blob since the previous snapshot state. The snapshots are dependent on the original blob, so we recommend copying to another blob or even to another storage account. This approach ensures that backup data is protected against accidental deletion. Use [AzCopy](/azure/storage/common/storage-use-azcopy) or [Azure PowerShell](/azure/storage/common/storage-powershell-guide-full) to copy the blobs to another storage account.\n\n    For more information, see [Creating a Snapshot of a Blob](/rest/api/storageservices/creating-a-snapshot-of-a-blob).\n\n- [**Azure Files.**](/azure/storage/files/storage-files-introduction) Use [share snapshots](/azure/storage/files/storage-snapshots-files), AzCopy, or PowerShell to copy your files to another storage account.\n- [**Azure Table storage.**](/azure/storage/tables/table-storage-overview) Use AzCopy to export the table data into another storage account in another region.\n\n#### SQL Database recovery\n\nTo protect your business from data loss, SQL Database automatically performs a combination of full database backups weekly, differential database backups hourly, and transaction log backups every 5 to 10 minutes. For the Basic, Standard, and Premium SQL Database tiers, use point-in-time restore to restore a database to an earlier time. See the following articles for more information:\n\n- [Recover an Azure SQL database using automated database backups](/azure/sql-database/sql-database-recovery-using-backups)\n- [Overview of business continuity with Azure SQL Database](/azure/sql-database/sql-database-business-continuity)\n\nAnother option is to use active geo-replication for SQL Database, which automatically replicates database changes to secondary databases in the same or different Azure region. For more information, see [Creating and using active geo-replication](/azure/sql-database/sql-database-active-geo-replication).\n\nYou can also use a more manual approach for backup and restore:\n\n- Use the **DATABASE COPY** command to create a backup copy of the database with transactional consistency.\n- Use the Azure SQL Database Import/Export Service, which supports exporting databases to BACPAC files (compressed files containing your database schema and associated data) that are stored in Azure Blob storage. To protect against a region-wide service disruption, copy the BACPAC files to an alternate region.\n\n### SQL Server on VMs\n\nFor SQL Server running on VMs, you have two options: traditional backups and log shipping.\n\n- With traditional backups, you can restore to a specific point in time, but the recovery process is slow. Restoring traditional backups requires that you start with an initial full backup and then apply any incremental backups.\n- You can configure a log shipping session to delay the restore of log backups. This provides a window to recover from errors made on the primary replica.\n\n### Azure Database for MySQL and Azure Database for PostgreSQL\n\nIn Azure Database for MySQL and Azure Database for PostgreSQL, the database service automatically makes a backup every five minutes. You can use these automated backups to restore the server and its databases from an earlier point in time to a new server. For more information, see:\n\n- [How to back up and restore a server in Azure Database for MySQL by using the Azure portal](/azure/mysql/howto-restore-server-portal)\n- [How to back up and restore a server in Azure Database for PostgreSQL using the Azure portal](/azure/postgresql/howto-restore-server-portal)\n\n### Azure Cosmos DB\n\nCosmos DB automatically makes a backup at regular intervals. Backups are stored separately in another storage service and are replicated globally to protect against regional disasters. If you accidentally delete your database or collection, you can file a support ticket or call Azure support to restore the data from the last automatic backup. For more information, see [Online backup and on-demand restore in Azure Cosmos DB](/azure/cosmos-db/online-backup-and-restore).\n\n### Azure Virtual Machines\n\nTo protect Azure Virtual Machines from application errors or accidental deletion, use [Azure Backup](/azure/backup/). The created backups are consistent across multiple VM disks. In addition, the Azure Backup vault can be replicated across regions to support recovery from a regional loss.",
              "priority": "medium",
              "title": "You understand what to do when data is corrupted or deleted",
              "id": "fdbbf0bc-5fba-4a7e-bbb2-a0dabc22f7e6",
              "heading": "Data corruption and restoration"
            },
            {
              "answer_tooltip": "",
              "output": "Start by creating a recovery plan. The plan is considered complete after it has been fully tested. Include the people, processes, and applications needed to restore functionality within the service-level agreement (SLA) you've defined for your customers.\n\nConsider the following suggestions when creating and testing your disaster recovery plan:\n\n- In your plan, include the process for contacting support and for escalating issues. This information will help to avoid prolonged downtime as you work out the recovery process for the first time.\n- Evaluate the business impact of application failures.\n- Choose a cross-region recovery architecture for mission-critical applications.\n- Identify a specific owner of the disaster recovery plan, including automation and testing.\n- Document the process, especially any manual steps.\n- Automate the process as much as possible.\n- Establish a backup strategy for all reference and transactional data, and test backup restoration regularly.\n- Set up alerts for the stack of the Azure services consumed by your application.\n- Train operations staff to execute the plan.\n- Perform regular disaster simulations to validate and improve the plan.\n\nIf you're using [Azure Site Recovery](/azure/site-recovery/) to replicate virtual machines (VMs), create a fully automated recovery plan to fail over the entire application.",
              "priority": "medium",
              "title": "You have a disaster recovery plan",
              "id": "78b95e10-acc0-4749-8f1f-efa8cfb5eb89",
              "heading": "Disaster recovery plan"
            },
            {
              "answer_tooltip": "",
              "output": "Many alternative strategies are available for implementing distributed compute across regions. These must be tailored to the specific business requirements and circumstances of the application. At a high level, the approaches can be divided into the following categories:\n\n- **Redeploy on disaster**: In this approach, the application is redeployed from scratch at the time of disaster. This is appropriate for non-critical applications that don’t require a guaranteed recovery time.\n\n- **Warm Spare (Active/Passive)**: A secondary hosted service is created in an alternate region, and roles are deployed to guarantee minimal capacity; however, the roles don’t receive production traffic. This approach is useful for applications that have not been designed to distribute traffic across regions.\n\n- **Hot Spare (Active/Active)**: The application is designed to receive production load in multiple regions. The cloud services in each region might be configured for higher capacity than required for disaster recovery purposes. Alternatively, the cloud services might scale out as necessary at the time of a disaster and fail over. This approach requires substantial investment in application design, but it has significant benefits. These include low and guaranteed recovery time, continuous testing of all recovery locations, and efficient usage of capacity.\n\nA complete discussion of distributed design is outside the scope of this document. For more information, see [Disaster Recovery and High Availability for Azure Applications](https://aka.ms/drtechguide).",
              "priority": "medium",
              "title": "Backup strategy defined",
              "id": "c0f0af98-7b8f-49c1-823d-a7626f4abed8",
              "heading": "Backup strategy"
            },
            {
              "answer_tooltip": "",
              "output": "To protect Azure Virtual Machines (VMs) from application errors or accidental deletion, use [Azure Backup](/azure/backup/). Azure Backup enables the creation of backups that are consistent across multiple VM disks. In addition, the Backup Vault can be replicated across regions to provide recovery from region loss.",
              "priority": "medium",
              "title": "Virtual machines are protected from corruption and accidental deletion",
              "id": "15e506d4-11cb-476f-a1aa-e0d7699dcc76",
              "heading": "Backup VMs"
            },
            {
              "answer_tooltip": "",
              "output": "You can distribute compute instances across regions by creating a separate cloud service in each target region, and then publishing the deployment package to each cloud service. However, distributing traffic across cloud services in different regions must be implemented by the application developer or with a traffic management service.\n\nDetermining the number of spare role instances to deploy in advance for disaster recovery is an important aspect of capacity planning. Having a full-scale secondary deployment ensures that capacity is already available when needed; however, this effectively doubles the cost. A common pattern is to have a small, secondary deployment, just large enough to run critical services. This small secondary deployment is a good idea, both to reserve capacity, and for testing the configuration of the secondary environment.\n\n> [!NOTE]\n> The subscription quota is not a capacity guarantee. The quota is simply a credit limit. To guarantee capacity, the required number of roles must be defined in the service model, and the roles must be deployed.",
              "priority": "medium",
              "title": "Resource management",
              "id": "569d1907-4e1b-47a4-9edf-d663534ef164",
              "heading": "Resource management"
            },
            {
              "id": "7e063160-cb53-46c1-b111-380f77a7c848",
              "title": "Backup & restore operations are automatically scheduled and tested",
              "answer_tooltip": "Unscheduled and untested backup/restore operations are a risk to continued availability if they do not meet the recovery point objective (RPO).",
              "output": "Test failover and failback to verify that your application's dependent services come back up in a synchronized manner during disaster recovery. Changes to systems and operations may affect failover and failback functions, but the impact may not be detected until the main system fails or becomes overloaded. Test failover capabilities *before* they are required to compensate for a live problem. Also be sure that dependent services fail over and fail back in the correct order.\n\nIf you are using [Azure Site Recovery](/azure/site-recovery/) to replicate VMs, run disaster recovery drills periodically by doing test failovers to validate your replication strategy. A test failover does not affect the ongoing VM replication or your production environment. For more information, see [Run a disaster recovery drill to Azure](/azure/site-recovery/site-recovery-test-failover-to-azure).",
              "priority": "medium",
              "heading": "Failover and failback testing"
            },
            {
              "id": "b7793fe8-b2c3-4d79-94a6-85b1f5248f4f",
              "title": "Implementing and validating data backups",
              "answer_tooltip": "Regularly verify that your backup data is what you expect by running a script to validate data integrity, schema, and queries. There's no point having a backup if it's not useful to restore your data sources. Log and report any inconsistencies so the backup service can be repaired.",
              "output": "",
              "priority": "medium",
              "heading": "Validating backups"
            },
            {
              "id": "19ddf9ff-e487-40ad-8446-ad14e3be1f91",
              "title": "We conduct outage retrospectives",
              "answer_tooltip": "No amount of safeguards or preparation can prevent every possible incident, and sometimes simple human error can have significant consequences to a development project. You can't avoid it, but you can learn from it and take steps to minimize the chances of a similar incident in the future. The question is how software organizations can best go about learning from mistakes with agile postmortems.",
              "output": "",
              "priority": "medium",
              "heading": "Outage retrospectives"
            },
            {
              "id": "4cce6ac5-3939-48e4-8ff1-fd9fbaa08252",
              "title": "Regional failure plans are documented",
              "answer_tooltip": "When you run production workloads on cloud, you might use a globally distributed system so that if something goes wrong in one region, the application continues to provide service even if it's less widely available. In essence, that application invokes its DR plan.",
              "output": "Azure is divided physically and logically into units called regions. A region consists of one or more datacenters in close proximity.\n\nUnder rare circumstances, it is possible that facilities in an entire region can become inaccessible, for example due to network failures. Or facilities can be lost entirely, for example due to a natural disaster. This section explains the capabilities of Azure for creating applications that are distributed across regions. Such distribution helps to minimize the possibility that a failure in one region could affect other regions.\n\nReview [Recover from loss of an Azure region](/azure/architecture/resiliency/recovery-loss-azure-region.md) for guidance on specific Azure services.",
              "priority": "low",
              "heading": "Planning for regional failures"
            },
            {
              "id": "780d5d6c-f8d9-48d6-bc8b-ed75e60fbc06",
              "title": "Backups are stored securely",
              "answer_tooltip": "Backups are about protecting data, applications, and systems that are important to the organization. In operations environments, it’s easy to provide backups: pick the workload that needs hyper-availability and back it up. Operations environments are relatively static – in that, the systems and applications used remain relatively consistent, with only the data changing daily.",
              "output": "",
              "priority": "low",
              "heading": "Backup Storage"
            },
            {
              "id": "0713135c-52c2-4ee3-94ba-63224f54b3d9",
              "title": "Have application configuration and installations archivied",
              "answer_tooltip": "",
              "output": "It’s important to remember, that a DR plan is more than just an ordered restoration from backup and validation process. Applications may require post-restoration configuration due to site changes, or reinstallation may be necessary with restored data imported after.",
              "priority": "low",
              "heading": "Application archives"
            },
            {
              "id": "75fe27cc-08ce-44ab-bd0b-3843c2eeb0c9",
              "title": "Data retention policies are defined",
              "answer_tooltip": "",
              "output": "",
              "priority": "low",
              "heading": "Data retention"
            },
            {
              "id": "4c3d21fa-90ec-4124-995d-08b8877a1cee",
              "title": "Our backups are automated. ",
              "answer_tooltip": "",
              "output": "",
              "priority": "low",
              "heading": "Automating Backups"
            }
          ],
          "display_logic": [],
          "heading": "Backup and Recovery"
        },
        {
          "id": "4552394a-b987-4e98-a520-62db1a711f0e",
          "title": "How have you ensured that your application is resilient to failures?",
          "explanation": "Building *resiliency* (recovering from failures) and *availability* (running in a healthy state without significant downtime) into your apps begins with gathering requirements. For example, how much downtime is acceptable? How much does potential downtime cost your business? What are your customer's availability requirements? How much do you invest in making your application highly available? What is the risk versus the cost?",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Choose the right subscription and service features for your app by working through these tasks:\n\n- **Evaluate requirements against [Azure subscription and service limits](/azure/azure-subscription-service-limits/).** *Azure subscriptions* have limits on certain resource types, such as number of resource groups, cores, and storage accounts. If your application requirements exceed Azure subscription limits, create another Azure subscription and provision sufficient resources there. Individual Azure services have consumption limits &mdash; for example, limits on storage, throughput, number of connections, requests per second, and other metrics. Your application will fail if it attempts to use resources beyond these limits, resulting in service throttling and possible downtime for affected users. Depending on the specific service and your application requirements, you can often avoid these limits by scaling up (for example, choosing another pricing tier) or scaling out (such as adding new instances).\n- **Determine how many storage accounts you need.** Azure allows a specific number of storage accounts per subscription. For more information, see [Azure subscription and service limits, quotas, and constraints](/azure/azure-subscription-service-limits/#storage-limits).\n- **Select the right service tier for Azure SQL Database.** If your application uses Azure SQL Database, select the appropriate service tier. If the tier cannot handle your application's database transaction unit (DTU) requirements, your data use will be throttled. For more information on selecting the correct service plan, see [SQL Database options and performance: Understand what's available in each service tier](/azure/sql-database/sql-database-service-tiers/).\n- **Provision sufficient request units (RUs) in Azure Cosmos DB**. With Azure Cosmos DB, you pay for the throughput you provision and the storage you consume on an hourly basis. The cost of all database operations is normalized as RUs, which abstracts the system resources such as CPU, IOPS, and memory. For more information, see [Request Units in Azure Cosmos DB](/azure/cosmos-db/request-units).",
              "priority": "medium",
              "title": "You have identified your subscription and service requirements.",
              "id": "89034945-14d8-4763-a116-e1e40d949f5f",
              "heading": "Determine subscription and service requirements"
            },
            {
              "answer_tooltip": "",
              "output": "This section describes some common resiliency strategies. Most of these strategies are not limited to a particular technology. The descriptions summarize the general idea behind each technique and include links to further reading.\n\n- **Implement resiliency patterns** for remote operations, where appropriate. If your application depends on communication between remote services, follow [design patterns](../patterns/category/resiliency.md) for dealing with transient failures.\n\n- **Retry transient failures.** These can be caused by momentary loss of network connectivity, a dropped database connection, or a timeout when a service is busy. Often, a transient failure can be resolved by retrying the request.\n\n  - For many Azure services, the client software development kit (SDK) implements automatic retries in a way that is transparent to the caller. See [Retry guidance for specific services](../best-practices/retry-service-specific.md).\n  - Or implement the [Retry pattern](../patterns/retry.md) to help the application handle anticipated, temporary failures transparently when it tries to connect to a service or network resource.\n\n- **Use a circuit breaker** to handle faults that might take a variable amount of time to fix. The [Circuit Breaker pattern](../patterns/circuit-breaker.md) can prevent an application from repeatedly trying an operation that is likely to fail. The circuit breaker wraps calls to a service and tracks the number of recent failures. If the failure count exceeds a threshold, the circuit breaker starts returning an error code without calling the service. This gives the service time to recover and helps avoid cascading failures.\n- **Isolate critical resources.** Failures in one subsystem can sometimes cascade, resulting in failures in other parts of the application. This can happen if a failure prevents resources such as threads or sockets from being freed, leading to resource exhaustion. To avoid this, you can partition a system into isolated groups so that a failure in one partition does not bring down the entire system.\n\n    Here are some examples of this technique, which is sometimes called the [Bulkhead pattern](../patterns/bulkhead.md):\n\n  - Partition a database (for example, by tenant), and assign a separate pool of web server instances for each partition.\n  - Use separate thread pools to isolate calls to different services. This helps to prevent cascading failures if one of the services fails. For an example, see the Netflix [Hystrix library](https://medium.com/netflix-techblog/introducing-hystrix-for-resiliency-engineering-13531c1ab362).\n  - Use [containers](https://en.wikipedia.org/wiki/Operating-system-level_virtualization) to limit the resources available to a particular subsystem.\n\n      ![Diagram of the Bulkhead pattern](_images/bulkhead.png)\n\n- **Apply [*compensating transactions*](../patterns/compensating-transaction.md)**. A compensating transaction is a transaction that undoes the effects of another completed transaction. In a distributed system, it can be difficult to achieve strong transactional consistency. Compensating transactions help to achieve consistency by using a series of smaller, individual transactions that can be undone at each step. For example, to book a trip, a customer might reserve a car, a hotel room, and a flight. If one of these steps fails, the entire operation fails. Instead of trying to use a single distributed transaction for the entire operation, you can define a compensating transaction for each step.\n- **Implement asynchronous operations, whenever possible.** Synchronous operations can monopolize resources and block other operations while the caller waits for the process to complete. Design each part of your application to allow for asynchronous operations, whenever possible. For more information on how to implement asynchronous programming in C\\#, see [Asynchronous Programming](/dotnet/articles/csharp/async).",
              "priority": "medium",
              "title": "You have implemented appropriate resiliency strategies",
              "id": "06b2e64c-0f3d-417c-a603-05c024b7b6d6",
              "heading": "Resiliency strategies"
            },
            {
              "answer_tooltip": "",
              "output": "Identify differences in requirements during critical and non-critical periods. Are there certain critical periods when the system must be available? For example, a tax-filing application can't fail during a filing deadline and a video streaming service shouldn't lag during a live event. In these situations, weigh the cost against the risk.\n\n- To ensure uptime and meet service-level agreements (SLAs) in critical periods, plan redundancy across several regions in case one fails, even if it costs more.\n- Conversely, during non-critical periods, run your application in a single region to minimize costs.\n- In some cases, you can mitigate additional expenses by using modern serverless techniques that have consumption-based billing.",
              "priority": "medium",
              "title": "You have planned for the usage patterns you expect in your workload.",
              "id": "c160e662-2740-4bce-9ab8-c7ea1b88ae76",
              "heading": "Plan for usage patterns"
            },
            {
              "answer_tooltip": "",
              "output": "Cloud solutions typically consist of multiple application *workloads*. A workload is a distinct capability or task that is logically separated from other tasks in terms of business logic and data storage requirements. For example, an e-commerce app might have the following workloads:\n\n- Browse and search a product catalog.\n- Create and track orders.\n- View recommendations.\n\nEach workload has different requirements for availability, scalability, data consistency, and disaster recovery. Make your business decisions by balancing cost versus risk for each workload.\n\nAlso decompose workloads by service-level objective. If a service is composed of critical and less-critical workloads, manage them differently and specify the service features and number of instances needed to meet their availability requirements.",
              "priority": "medium",
              "title": "You have identified distinct workloads.",
              "id": "2617db5e-4cb1-4488-a30d-d1aaaafeb63c",
              "heading": "Identify distinct workloads"
            },
            {
              "id": "52f1a917-f368-439a-98f9-0262a25de762",
              "title": "Retry and Circuit Breaker patterns are used",
              "answer_tooltip": "Inability to serve user requests when remote service communication fails.",
              "output": "Use the Retry pattern and Circuit Breaker pattern. Use resiliency strategies and asynchronous programming with async and await.",
              "priority": "medium",
              "heading": "Handling transient failures"
            },
            {
              "id": "32696b2a-af6d-4c34-8301-aa4a420ecda1",
              "title": "Third-party services have documented SLAs and support information",
              "answer_tooltip": "Inability to serve user requests when third-party service goes down.",
              "output": "If your application has dependencies on third-party services, identify how these services can fail and what effect failures will have on your application.\n\nA third-party service might not include monitoring and diagnostics. Log calls to these services and correlate them with your application's health and diagnostic logging using a unique identifier. For more information on proven practices for monitoring and diagnostics, see [Monitoring and diagnostics guidance](../best-practices/monitoring.md).\n\nSee the [Health Endpoint Monitoring pattern](/azure/architecture/patterns/health-endpoint-monitoring) for a solution to track this with code samples.",
              "priority": "medium",
              "heading": "Managing 3rd party services"
            },
            {
              "id": "5a3257b3-1668-4edf-949c-717de2efc1bb",
              "title": "Third-party services are monitored",
              "answer_tooltip": "Inability to gauge the effect of third-party services on the application.",
              "output": "If your application has dependencies on third-party services, identify where and how these services can fail and what effect those failures will have on your application. Keep in mind the service-level agreement (SLA) for the third-party service and the effect it might have on your disaster recovery plan.\n\nA third-party service might not provide monitoring and diagnostics capabilities, so it's important to log your invocations of them and to correlate them with your application's health and diagnostic logging using a unique identifier. For more information on proven practices for monitoring and diagnostics, see [Monitoring and diagnostics guidance](../best-practices/monitoring.md).",
              "priority": "medium",
              "heading": "Monitoring third-party services"
            },
            {
              "id": "b58e95eb-a88b-469c-9842-9adf4f2b56ed",
              "title": "Health probes/checks are implemented for load balancers (LB) and application gateways (AGW)",
              "answer_tooltip": "Inability to prevent user requests going to faulty instance of service.",
              "output": "Use the Health Endpoint Monitoring pattern to implement health probes/checks for load balancers (LB) and application gateways (AGW)",
              "priority": "medium",
              "heading": "Load balancer health probes"
            },
            {
              "id": "5d3517be-f6f3-4c0c-8f0d-bc534d0c509a",
              "title": "Storage is replicated locally utilizing RAID or equivialnt technologies to protect against disk failure",
              "answer_tooltip": "Inability to serve user requests when an instance of a service goes down.",
              "output": "Deploy the application in multiple Azure paired regions. Deploy multiple instances of the web app.",
              "priority": "medium",
              "heading": "Protecting from disk failures"
            },
            {
              "id": "cb5b6fb3-ef56-490c-b8a9-42d66b71f1b9",
              "title": "Load balancing is implemented",
              "answer_tooltip": "Inability to distribute an application's requests to healthy service instances by removing unhealthy instances from rotation.",
              "output": "To load balance traffic across regions requires a traffic management solution. Azure provides [Azure Traffic Manager](https://azure.microsoft.com/services/traffic-manager). You can also take advantage of third-party services that provide similar traffic management capabilities.\n\nProper load-balancing allows you to meet availability requirements and to minimize costs associated with availability.\n\n- **Use load-balancing to distribute requests.** Load-balancing distributes your application's requests to healthy service instances by removing unhealthy instances from rotation. If your service uses Azure App Service or Azure Cloud Services, it's already load-balanced for you. However, if your application uses Azure VMs, you need to provision a load-balancer. For more information, see [What is Azure Load Balancer?](/azure/load-balancer/load-balancer-overview/)\n\n  You can use Azure Load Balancer to:\n\n  - Load-balance incoming Internet traffic to your VMs. This configuration is known as a [*public Load Balancer*](/azure/load-balancer/load-balancer-overview#publicloadbalancer).\n  - Load-balance traffic across VMs inside a virtual network. You can also reach a Load Balancer front end from an on-premises network in a hybrid scenario. Both scenarios use a configuration that is known as an [*internal Load Balancer*](/azure/load-balancer/load-balancer-overview#internalloadbalancer).\n  - Port forward traffic to an itemized port on specific VMs with inbound network address translation (NAT) rules.\n  - Provide [outbound connectivity](/azure/load-balancer/load-balancer-outbound-connections) for VMs inside your virtual network by using a public Load Balancer.\n\n- **Balance loads across regions with a traffic manager, such as Azure Traffic Manager.** To load-balance traffic across regions requires a traffic management solution, and Azure provides [Traffic Manager](https://azure.microsoft.com/services/traffic-manager/). You can also take advantage of third-party services that provide similar traffic-management capabilities.",
              "priority": "medium",
              "heading": "Load balancing"
            },
            {
              "id": "ebd007eb-55b7-4189-af5a-388cf30e318f",
              "title": "Throttling is implemented",
              "answer_tooltip": "Load continues to hit the application running with stretched resources, affecting even further loss of available resources.",
              "output": "Use the Throttling pattern.",
              "priority": "medium",
              "heading": "Application throttling"
            },
            {
              "id": "ea92ec48-0524-41e1-ae33-f0b7fd901059",
              "title": "Message brokers are utilized",
              "answer_tooltip": "Transactions may get lost during congestion/load while waiting to get processed.",
              "output": "Use Service Bus Queues between the front end and back end.",
              "priority": "medium",
              "heading": "Message brokers"
            },
            {
              "id": "46c01ec1-43ae-424b-9ef7-f328cea862fa",
              "title": "Each application component has an SLA defined",
              "answer_tooltip": "Not decomposing services based on their SLAs makes it difficult to manage these services for their availability.",
              "output": "Consider adopting a microservices architecture.",
              "priority": "medium",
              "heading": "Defining service SLA's"
            },
            {
              "id": "38d1d690-c16a-4343-8147-f24a5b3df0d5",
              "title": "Multiple instances of the app & database are running",
              "answer_tooltip": "Individual virtual machines (VMs) become single points of congestion. Database becomes a single point of failure.",
              "output": "Deploy the application in multiple Azure paired regions. Use auto-failover and active geo-replication for SQL Database. Use Azure Managed Database Services for turnkey global distribution. Deploy multiple instances of the web app.",
              "priority": "medium",
              "heading": "Single points of failure (SPOF)"
            },
            {
              "id": "6cbc14af-ed03-4ad4-929e-1b4905c2cdbc",
              "title": "Performed a failure mode analysis of the application.",
              "answer_tooltip": "",
              "output": "Failure mode analysis (FMA) is a process for building resiliency into a system, by identifying possible failure points in the system. The FMA should be part of the architecture and design phases, so that you can build failure recovery into the system from the beginning.\n*Failure mode analysis* (FMA) builds resiliency into a system by identifying possible failure points and defining how the application responds to those failures. The FMA should be part of the architecture and design phases, so failure recovery is built into the system from the beginning. The goals of an FMA are to:\n\n- Determine what types of failures an application might experience and how the application detects those failures.\n- Capture the potential effects of each type of failure and determine how the app responds.\n- Plan for logging and monitoring the failure and identify recovery strategies.\n\nHere are some examples of failure modes and detection strategies for a specific failure point &mdash; a call to an external web service:\n\n| Failure mode           | Detection strategy           |\n|------------------------|------------------------------|\n| Service is unavailable | HTTP 5xx                     |\n| Throttling             | HTTP 429 (Too Many Requests) |\n| Authentication         | HTTP 401 (Unauthorized)      |\n| Slow response          | Request times out            |\n\nFor more information about the FMA process, with specific recommendations for Azure, see [Failure mode analysis][failure-mode-analysis].\n\n<!-- links -->\n[failure-mode-analysis]: /azure/architecture/resiliency/failure-mode-analysis",
              "priority": "low",
              "heading": "Failure mode analysis"
            },
            {
              "id": "8ad5e66c-bdc0-40ef-93cb-bcb5787fff8c",
              "title": "Availability Sets are used for each application tier",
              "answer_tooltip": "An availability set helps keep your virtual machines available during downtime, such as during maintenance.",
              "output": "Placing two or more similarly configured virtual machines in an availability set creates the redundancy needed to maintain availability of the applications or services that your virtual machine runs.",
              "priority": "medium",
              "heading": "Availability Sets"
            },
            {
              "id": "4fc5dd89-3067-49ed-a65a-42d4f9182964",
              "title": "VMs are replicated",
              "answer_tooltip": "",
              "output": "Azure Site Recovery orchestrates and manages disaster recovery for Azure VMs, and on-premises VMs and physical servers.",
              "priority": "medium",
              "heading": "VM Replication"
            },
            {
              "id": "61c17190-428b-47ca-b09f-68daca74faf2",
              "title": "Deployed the application across multiple regions",
              "answer_tooltip": "A multi-region deployment can use an active-active pattern or an active-passive pattern",
              "output": "If your application is deployed to a single region, in the rare event the entire region becomes unavailable, your application will also be unavailable. This may be unacceptable under the terms of your application's SLA. If so, consider deploying your application and its services across multiple regions. A multi-region deployment can use an active-active pattern (distributing requests across multiple active instances) or an active-passive pattern (keeping a \"warm\" instance in reserve, in case the primary instance fails)\n\nMany failures are manageable within the same Azure region. However, in the unlikely event of a region-wide service disruption, the locally redundant copies of your data aren't available. If you've enabled geo-replication, there are three additional copies of your blobs and tables in a different region. If Microsoft declares the region lost, Azure remaps all the DNS entries to the secondary region.\n\n> [!NOTE]\n> This process occurs only for region-wide service disruptions and is not within your control. Consider using [Azure Site Recovery](/azure/site-recovery/) to achieve better RPO and RTO. Using Site Recovery, you decide what is an acceptable outage and when to fail over to the replicated VMs.\n\n>[!NOTE]\n>The selection of the Resource Group location is important. In the event of a regional outage, you will be unable to control resources inside that Resource Group, regardless of what region those resources are actually in (i.e., the resources in the other region(s) will continue to function, but management plane operations will be unavailable.\n\nYour response to a region-wide service disruption depends on your deployment and your disaster recovery plan.\n\n- As a cost-control strategy, for non-critical applications that don't require a guaranteed recovery time, it might make sense to redeploy to a different region.\n- For applications that are hosted in another region with deployed roles but don't distribute traffic across regions (*active/passive deployment*), switch to the secondary hosted service in the alternate region.\n- For applications that have a full-scale secondary deployment in another region (*active/active deployment*), route traffic to that region.\n\nTo learn more about recovering from a region-wide service disruption, see [Recover from a region-wide service disruption](../resiliency/recovery-loss-azure-region.md).\n\n### VM recovery\n\nFor critical apps, plan for recovering VMs in the event of a region-wide service disruption.\n\n- Use Azure Backup or another backup method to create cross-region backups that are application consistent. (Replication of the Backup vault must be configured at the time of creation.)\n- Use Site Recovery to replicate across regions for one-click application failover and failover testing.\n- Use Traffic Manager to automate user traffic failover to another region.\n\nTo learn more, see [Recover from a region-wide service disruption, Virtual machines](../resiliency/recovery-loss-azure-region.md#virtual-machines).\n\n### Storage recovery\n\nTo protect your storage in the event of a region-wide service disruption:\n\n- Use geo-redundant storage.\n- Know where your storage is geo-replicated. This affects where you deploy other instances of your data that require regional affinity with your storage.\n- Check data for consistency after failover and, if necessary, restore from a backup.\n\nTo learn more, see [Designing highly available applications using RA-GRS](/azure/storage/common/storage-designing-ha-apps-with-ragrs).\n\n### SQL Database and SQL Server\n\nAzure SQL Database provides two types of recovery:\n\n- Use geo-restore to restore a database from a backup copy in another region. For more information, see [Recover an Azure SQL database using automated database backups](/azure/sql-database/sql-database-recovery-using-backups).\n- Use active geo-replication to fail over to a secondary database. For more information, see [Creating and using active geo-replication](/azure/sql-database/sql-database-active-geo-replication).\n\nFor SQL Server running on VMs, see [High availability and disaster recovery for SQL Server in Azure Virtual Machines](/azure/virtual-machines/windows/sql/virtual-machines-windows-sql-high-availability-dr/).",
              "priority": "medium",
              "heading": "Operating in multiple regions"
            }
          ],
          "display_logic": [],
          "heading": "App Design"
        },
        {
          "id": "8526c2c0-74b7-4669-908e-cf452c6d64cd",
          "title": "How are you ensuring failures are resolved quickly?",
          "explanation": "Monitoring and diagnostics are crucial for resiliency. If something fails, you need to know *that* it failed, *when* it failed &mdash; and *why*.\n\n*Monitoring* is not the same as *failure detection*. For example, your application might detect a transient error and retry, avoiding downtime. But it should also log the retry operation so that you can monitor the error rate to get an overall picture of application health.\n\nThink of the monitoring and diagnostics process as a pipeline with four distinct stages: Instrumentation, collection and storage, analysis and diagnosis, and visualization and alerts.",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Monitor your application for warning signs that might require proactive intervention. Tools that assess the overall health of the application and its dependencies help you to recognize quickly when a system or its components suddenly become unavailable. Use them to implement an early warning system.\n\n1. Identify the key performance indicators of your application's health, such as transient exceptions and remote call latency.\n1. Set thresholds at levels that identify issues before they become critical and require a recovery response.\n1. Send an alert to operations when the threshold value is reached.\n\nConsider [Microsoft System Center 2016](https://www.microsoft.com/cloud-platform/system-center) or third-party tools to provide monitoring capabilities. Most monitoring solutions track key performance counters and service availability. [Azure resource health](/azure/service-health/resource-health-checks-resource-types) provides some built-in health status checks, which can help diagnose throttling of Azure services.",
              "priority": "medium",
              "title": "You have an early warning system for workloads where that makes sense.",
              "id": "4e396a34-d281-498a-aa04-7e568c0302fa",
              "heading": "Early warning system"
            },
            {
              "answer_tooltip": "",
              "output": "Track and report remote call statistics in real time and provide an easy way to review this information, so the operations team has an instantaneous view into the health of your application. Summarize remote call metrics, such as latency, throughput, and errors in the 99 and 95 percentiles.",
              "priority": "medium",
              "title": "You track and act on your remote call statistics",
              "id": "ba131573-d162-41ae-b7b3-965b756c4b8c",
              "heading": "Remote call statistics"
            },
            {
              "answer_tooltip": "",
              "output": "Long-running workflows often include multiple steps, each of which should be independent.\n\nTrack the progress of long-running processes to minimize the likelihood that the entire workflow will need to be rolled back or that multiple compensating transactions will need to be executed.\n\n>[!TIP]\n> Monitor and manage the progress of long-running workflows by implementing a pattern such as [Scheduler Agent Supervisor](../patterns/scheduler-agent-supervisor.md).",
              "priority": "medium",
              "title": "You monitor your long-running workflows for failures.",
              "id": "e1c866fc-2813-4dd8-9cb1-6e3994725f48",
              "heading": "Long-running workflow failures"
            },
            {
              "answer_tooltip": "",
              "output": "Present telemetry data in a format that makes it easy for an operator to notice problems or trends quickly, such as a dashboard or email alert.\n\nGet a full-stack view of application state by using [Azure dashboards](/azure/azure-portal/azure-portal-dashboards) to create a consolidated view of monitoring graphs from Application Insights, Log Analytics, Azure Monitor metrics, and Service Health. And use [Azure Monitor alerts](/azure/azure-monitor/platform/alerts-overview) to create notifications on Service Health, resource health, Azure Monitor metrics, logs in Log Analytics, and Application Insights.\n\nFor more information about monitoring and diagnostics, see [Monitoring and diagnostics](../best-practices/monitoring.md).",
              "priority": "medium",
              "title": "You have built visualization and alerts so your monitoring is actionable.",
              "id": "0b2db038-ce56-4969-839c-52eff60d43ca",
              "heading": "Visualization and alerts"
            },
            {
              "answer_tooltip": "",
              "output": "Include monitoring systems in your test plan. Automated failover and failback systems depend on the correct functioning of monitoring and instrumentation. Dashboards to visualize system health and operator alerts also depend on having accurate monitoring and instrumentation. If these elements fail, miss critical information, or report inaccurate data, an operator might not realize that the system is unhealthy or failing.",
              "priority": "medium",
              "title": "You have validated that your monitoring system is functional.",
              "id": "cdd26956-73b7-4c13-843c-744a5a7de41d",
              "heading": "Test Monitoring"
            },
            {
              "id": "8e97698d-b6ed-45d0-bf98-89f5cd7e6fb1",
              "title": "The process to contact Azure support is documented and understood",
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "heading": "Azure support"
            },
            {
              "id": "c910bad6-cd8f-42ce-8076-edca3f197bd6",
              "title": "Azure subscription/service limits are documented and known",
              "answer_tooltip": "Possibility of poor customer experience when hit with subscription limits.",
              "output": "Azure subscriptions have limits on certain resource types, such as number of resource groups, cores, and storage accounts. To ensure that your application doesn't run up against Azure subscription limits, create alerts that poll for services nearing their limits and quotas.\n\nAddress the following subscription limits with alerts.\n\n### Individual services\n\nIndividual Azure services have consumption limits on storage, throughput, number of connections, requests per second, and other metrics. Your application will fail if it attempts to use resources beyond these limits, resulting in service throttling and possible downtime.\n\nDepending on the specific service and your application requirements, you can often stay under these limits by scaling up (choosing another pricing tier, for example) or scaling out (adding new instances).\n\n### Azure storage scalability and performance targets\n\nAzure allows a maximum number of storage accounts per subscription. If your application requires more storage accounts than are currently available in your subscription, create a new subscription with additional storage accounts. For more information, see [Azure subscription and service limits, quotas, and constraints](/azure/azure-subscription-service-limits/#storage-limits).\n\nIf you exceed Azure storage scalability and performance targets, your application will experience storage throttling. For more information, see [Azure Storage scalability and performance targets](/azure/storage/storage-scalability-targets/).\n\n### Scalability targets for virtual machine disks\n\nAn Azure infrastructure as a service (IaaS) VM supports attaching a number of data disks, depending on several factors, including the VM size and the type of storage account. If your application exceeds the scalability targets for virtual machine disks, provision additional storage accounts and create the virtual machine disks there. For more information, see [Scalability and performance targets for VM disks on Windows](/azure/virtual-machines/windows/disk-scalability-targets).\n\n### VM size\n\nIf the actual CPU, memory, disk, and I/O of your VMs approach the limits of the VM size, your application may experience capacity issues. To correct the issues, increase the VM size. VM sizes are described in [Sizes for virtual machines in Azure](/azure/virtual-machines/virtual-machines-windows-sizes/?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).\n\nIf your workload fluctuates over time, consider using virtual machine scale sets to automatically scale the number of VM instances. Otherwise, you need to manually increase or decrease the number of VMs. For more information, see the [virtual machine scale sets overview](/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-overview/).\n\n### Azure SQL Database\n\nIf your Azure SQL Database tier isn't adequate to handle your application's Database Transaction Unit (DTU) requirements, your data use will be throttled. For more information on selecting the correct service plan, see [Azure SQL Database purchasing models](/azure/sql-database/sql-database-service-tiers/).",
              "priority": "medium",
              "heading": "Azure subscription and service limits"
            },
            {
              "id": "91777c9f-832f-4380-8167-8d952eadef81",
              "title": "Multiple people are trained for monitoring",
              "answer_tooltip": "Possibility of a single person being unavailable, resulting in a single point of failure.",
              "output": "Train multiple people on Azure Monitor. Send alerts and notifications to multiple recipients.",
              "priority": "medium",
              "heading": "Monitor Training"
            },
            {
              "id": "1f5ab437-af5d-45b0-8954-ad5843769e00",
              "title": "Operators are assigned for system alerts",
              "answer_tooltip": "Possibility of unidentified issues becoming critical.",
              "output": "Use action groups to ensure people receive alerts.",
              "priority": "medium",
              "heading": "Alert ownership"
            },
            {
              "id": "0c294da9-897f-4e40-9eb9-e683ebf3b548",
              "title": "You have implemented the necessary instrumentation to monitor your workload.",
              "answer_tooltip": "Inability to detect failures in an application and alert an operator to fix them.",
              "output": "It's not practical to monitor your application directly, so instrumentation is key. A large-scale distributed system might run on dozens of virtual machines (VMs), which are added and removed over time. Likewise, a cloud application might use a number of data stores and a single user action might span multiple subsystems.\n\nProvide rich instrumentation:\n\n- For failures that are likely but have not yet occurred, provide enough data to determine the cause, mitigate the situation, and ensure that the system remains available.\n- For failures that have already occurred, the application should return an appropriate error message to the user but should attempt to continue running, albeit with reduced functionality.\n\nMonitoring systems should capture comprehensive details so that applications can be restored efficiently and, if necessary, designers and developers can modify the system to prevent the situation from recurring.\n\nThe raw data for monitoring can come from a variety of sources, including:\n\n- Application logs, such as those produced by the [Azure Application Insights](/azure/azure-monitor/app/app-insights-overview) service.\n- Operating system performance metrics collected by [Azure monitoring agents](/azure/azure-monitor/platform/agents-overview).\n- [Azure resources](/azure/azure-monitor/platform/metrics-supported), including metrics collected by Azure Monitor.\n- [Azure Service Health](/azure/service-health/service-health-overview), which offers a dashboard to help you track active events.\n- [Azure AD logs](/azure/active-directory/reports-monitoring/howto-integrate-activity-logs-with-log-analytics) built into the Azure platform.\n\nMost Azure services have metrics and diagnostics that you can configure to analyze and determine the cause of problems. To learn more, see [Monitoring data collected by Azure Monitor](/azure/azure-monitor/platform/data-collection).",
              "priority": "medium",
              "heading": "Instrumentation"
            },
            {
              "id": "b06b8e73-1ed8-491f-ab1d-32b7e429f958",
              "title": "Monitoring tools are used to collect and view historical statistics",
              "answer_tooltip": "Inability to provide sufficient data to enable operations staff to determine the cause, mitigate the situation, and ensure availability.",
              "output": "Raw instrumentation data can be held in various locations and formats, including:\n\n- Application trace logs\n- IIS logs\n- Performance counters\n\nThese disparate sources are collected, consolidated, and placed in reliable data stores in Azure, such as Application Insights, Azure Monitor metrics, Service Health, storage accounts, and Azure Log Analytics.",
              "priority": "medium",
              "heading": "Collection and storage"
            },
            {
              "id": "f8d28ca2-bf0a-43a5-bd1b-e801bb6537ff",
              "title": "Health probes are implemented to validate application functionality",
              "answer_tooltip": "Inability to validate response, measure latency, and extract information on availability.",
              "output": "The health and performance of an application can degrade over time, and degradation might not be noticeable until the application fails.\n\nImplement probes or check functions, and run them regularly from outside the application. These checks can be as simple as measuring response time for the application as a whole, for individual parts of the application, for specific services that the application uses, or for separate components.\n\nCheck functions can run processes to ensure that they produce valid results, measure latency and check availability, and extract information from the system.",
              "priority": "medium",
              "heading": "Creating good health probes"
            },
            {
              "id": "59fb1292-709c-469a-a57c-68760210f30c",
              "title": "Errors and failures are captured and reported",
              "answer_tooltip": "Inability to provide sufficient data to enable operations staff to determine the cause, mitigate the situation, and ensure availability.",
              "output": "Application logs are an important source of diagnostics data. To gain insight when you need it most, follow best practices for application logging.\n\n### Log data in the production environment\n\nCapture robust telemetry data while the application is running in the production environment, so you have sufficient information to diagnose the cause of issues in the production state.\n\n### Log events at service boundaries\n\nInclude a correlation ID that flows across service boundaries. If a transaction flows through multiple services and one of them fails, the correlation ID helps you track requests across your application and pinpoints why the transaction failed.\n\n### Use semantic (structured) logging\n\nWith structured logs, it's easier to automate the consumption and analysis of the log data, which is especially important at cloud scale. Generally, we recommend storing Azure resources metrics and diagnostics data in a Log Analytics workspace rather than in a storage account. This way, you can use Kusto queries to obtain the data you want quickly and in a structured format. You can also use Azure Monitor APIs and Azure Log Analytics APIs.\n\n### Use asynchronous logging\n\nSynchronous logging operations sometimes block your application code, causing requests to back up as logs are written. Use asynchronous logging to preserve availability during application logging.\n\n### Separate application logging from auditing\n\nAudit records are commonly maintained for compliance or regulatory requirements and must be complete. To avoid dropped transactions, maintain audit logs separately from diagnostic logs.",
              "priority": "medium",
              "heading": "Application logs"
            },
            {
              "id": "df688b67-0f2e-4d32-9f07-2748a48829cd",
              "title": "Telemetric information is captured",
              "answer_tooltip": "Possibility of not having sufficient information for issues while they are actively serving users.",
              "output": "Use Azure Application Insights to log and monitor application events and exceptions.",
              "priority": "medium",
              "heading": "Telemetry Data"
            },
            {
              "id": "4aeb4f36-445d-43cb-9e7c-57bc716c0712",
              "title": "Log information is collected and correlated across all tiers",
              "answer_tooltip": "Inability of tracking user requests across multiple tiers/Azure services.",
              "output": "Analyze data consolidated in these data stores to troubleshoot issues and gain an overall view of application health. Generally, you can [search for and analyze](/azure/azure-monitor/log-query/log-query-overview) the data in Application Insights and Log Analytics using Kusto queries or view preconfigured graphs using [management solutions](/azure/azure-monitor/insights/solutions-inventory). Or use Azure Advisor to view recommendations with a focus on [resiliency](/azure/advisor/advisor-high-availability-recommendations) and [performance](/azure/advisor/advisor-performance-recommendations).",
              "priority": "medium",
              "heading": "Analysis and diagnosis"
            }
          ],
          "display_logic": [],
          "heading": "Monitoring"
        },
        {
          "id": "578b72ba-eb77-440b-856b-e6fc30f91c25",
          "title": "What Resiliency trade-offs are you making?",
          "explanation": "",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "id": "6039de59-8907-4beb-a4a2-652a90d19d25",
              "title": "Balanced alterting frequency with operations fatigue",
              "answer_tooltip": "Don't alert so much that people start to ignore them.",
              "output": "",
              "priority": "medium",
              "heading": "Balanced alterting frequency with operations fatigue"
            },
            {
              "id": "ae2bd1f4-8fe9-46ba-9e23-18c01d5b2e40",
              "title": "Balanced automation of failure handling with the ability to respond to transient failures",
              "answer_tooltip": "Don't restart services when they might recover quickly on their own",
              "output": "",
              "priority": "medium",
              "heading": "When to take action automatically"
            },
            {
              "id": "ce137ac6-4b7f-4934-9ba0-bc6a9b7fd8c7",
              "title": "Chosen a recovery point that aligns with our cost requirements",
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "heading": "Balancing cost & RPO"
            },
            {
              "id": "9669fd68-3eeb-4d28-8af7-0516039e422e",
              "title": "Chosen a recovery time that aligns with our cost goals",
              "answer_tooltip": "Active-active is expensive and may not be what you need",
              "output": "",
              "priority": "medium",
              "heading": "Balancing cost and RTO"
            }
          ],
          "display_logic": null,
          "heading": "Tradeoffs"
        },
        {
          "choices": [
            {
              "answer_tooltip": "Simulation testing involves creating small, real-life situations. Simulations demonstrate the effectiveness of the solutions in the recovery plan and highlight any issues that weren't adequately addressed.",
              "output": "Simulation testing involves creating small, real-life situations. Simulations demonstrate the effectiveness of the solutions in the recovery plan and highlight any issues that weren't adequately addressed.\n\nAs you perform simulation testing, follow best practices:\n\n- Conduct simulations in a manner that doesn't disrupt actual business but feels like a real situation.\n- Make sure that simulated scenarios are completely controllable. If the recovery plan seems to be failing, you can restore the situation back to normal without causing damage.\n- Inform management about when and how the simulation exercises will be conducted. Your plan should detail the time frame and the resources affected during the simulation.",
              "priority": "medium",
              "title": "You perform testing in small, real-life situations.",
              "id": "e251193a-37cb-4116-a001-1bb74c08649a",
              "heading": "Simulation testing"
            },
            {
              "answer_tooltip": "",
              "output": "For fault injection testing, check the resiliency of the system during failures, either by triggering actual failures or by simulating them. Here are some strategies to induce failures:\n\n- Shut down virtual machine (VM) instances.\n- Crash processes.\n- Expire certificates.\n- Change access keys.\n- Shut down the DNS service on domain controllers.\n- Limit available system resources, such as RAM or number of threads.\n- Unmount disks.\n- Redeploy a VM.\n\nYour test plan should incorporate possible failure points identified during the design phase, in addition to common failure scenarios:\n\n- Test your application in an environment as close to production as possible.\n- Test failures in combination.\n- Measure the recovery times, and be sure that your business requirements are met.\n- Verify that failures don't cascade and are handled in an isolated way.\n\nFor more information about failure scenarios, see [Failure and disaster recovery for Azure applications](./disaster-recovery.md).",
              "priority": "medium",
              "title": "You are testing your workload by injecting faults.",
              "id": "1e7674b5-00c7-4ff8-9c43-269cc7c29680",
              "heading": "Perform fault injection testing"
            },
            {
              "answer_tooltip": "Test applications under strain to ensure they stay available",
              "output": "Load testing is crucial for identifying failures that only happen under load, such as the back-end database being overwhelmed or service throttling. Test for peak load and anticipated increase in peak load, using production data or synthetic data that is as close to production data as possible. Your goal is to see how the application behaves under real-world conditions.",
              "priority": "medium",
              "title": "Perform Load Testing",
              "id": "0b1b8335-a3a2-405c-91dc-526abd02d841",
              "heading": "Test under peak loads"
            }
          ],
          "title": "How do you test your applications to ensure they're fault tolerant?",
          "type": "checkbox",
          "id": "e8fe553a-50ef-4a6f-a3b6-273bc3fe340a",
          "heading": "Testing",
          "explanation": "To test resiliency, you should verify how the end-to-end workload performs under intermittent failure conditions.\n\nRun tests in production using both synthetic and real user data. Test and production are rarely identical, so it's important to validate your application in production using a [blue-green](https://martinfowler.com/bliki/BlueGreenDeployment.html) or [canary deployment](https://martinfowler.com/bliki/CanaryRelease.html). This way, you're testing the application under real conditions, so you can be sure that it will function as expected when fully deployed.\n\nAs part of your test plan, include:\n\n- Automated predeployment testing\n- Fault injection testing\n- Peak load testing\n- Disaster recovery testing\n- Third-party service testing",
          "context": "Testing is an iterative process. Test the application, measure the outcome, analyze and address any failures that result, and repeat the process."
        }
      ]
    },
    {
      "id": "a10c1cdc-92f7-43b7-aae5-53bcaa402887",
      "name": "Cost",
      "questions": [
        {
          "id": "c8d69da5-4ca4-4b38-b4d2-cd13d2d5e75e",
          "title": "What actions are you taking to optimize cloud costs?",
          "explanation": "When the first calculation is more or less close to on-prem analog in terms of the cost - customers prefer to migrate to cloud. And after migration continue to optimize the infrastructure in a way of using right types/sizes of VMs and arrange snoozing and scale-down of the resources which are not used in the production environment.\n\nAs an example look at the VM from SAP on Azure project can show you how initially the VM was sized based on the size of existing hardware server (with cost around €1K per month), but the real utilization of VM was not more than 25% - but simple choosing the right VM size in the cloud we can achieve 75% saving (resize saving). And by applying the snoozing you can get additional 14% of economy:\n\n![](./images/run-cost-optimization.png)\n\nIt is easy to handle cost comparison when you well equipped and for this Microsoft provides the set of specific services and tools that help you to understand and plan costs. These include the TCO Calculator, Azure Pricing Calculator, Azure Cost Management (Cloudyn), Azure Migrate, CosmosDB Sizing Calculator, and the Azure Site Recovery Deployment Planner.\n\nAs we are talking about financial things - the way how you purchase cloud services, in which selling channel, may also bring the difference into the final cost. Consider the following methods of purchasing Azure and ways of modifying your pricing:\n\n- Enterprise Agreement\n\n- Enterprise Dev Test Subscription\n\n- Cloud Service Provider (Partner Program)\n\n- Azure Hybrid Use Benefit\n\n- Azure Reserved Instances",
          "context": "Describes how to best take advantage of the benefits of the cloud to minimize your cost.",
          "type": "checkbox",
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Understand Azure support plans. Refer to Azure Support FAQs.",
              "priority": "medium",
              "title": "Support",
              "id": "d23fb40d-6edd-4b3b-a857-1a00348f90e0",
              "heading": "Support"
            },
            {
              "answer_tooltip": "Adopt Advisor cost recommendations to optimize and reduce overall Azure spend.",
              "output": "Azure Advisor enables you to act on cost management recommendations from within the Azure Portal, such as resizing virtual machines. https://docs.microsoft.com/en-us/azure/cost-management/tutorial-acm-opt-recommendations. Make sure that all stakeholders are in agreement regarding the implementation and timing of this change. Note that resizing a virtual machine does require the VM to be shut down and restarted, causing a period of time when it will be unavailable, so time this carefully for minimal business impact.",
              "priority": "medium",
              "title": "How do you identify opportunities to reduce overall cost?",
              "id": "f613e1b7-3966-4d3f-9d03-8a5879cfaaec",
              "heading": "Act on recommendations"
            },
            {
              "id": "6e5acb21-c3f7-4b10-97ad-45d2634baf4d",
              "title": "Define metric-based requirements.",
              "answer_tooltip": "",
              "output": "Detailed metrics about your application translate to better cost estimates while planning.",
              "priority": "medium",
              "heading": "Metric based requirements"
            },
            {
              "id": "d36d1484-4d5f-4c66-9ffb-21dadf4387bf",
              "title": "Use cost management tools.",
              "answer_tooltip": "",
              "output": "How do you get visibility into the costs you're already accruing?",
              "priority": "medium",
              "heading": "Cost management tools"
            },
            {
              "id": "d8e0cd5d-6409-4cfd-a7ff-e49bc2acf96c",
              "title": "Rely on a cloud operating model.",
              "answer_tooltip": "",
              "output": "A cloud operating model summarizes the organizations goals for cloud adoption. Having a cloud operating model clarifies areas of focus that are readily translate to subjects of cost optimization.",
              "priority": "high",
              "heading": "Cloud operating model"
            },
            {
              "id": "27fcd721-ffea-40e2-a219-c44eb61ccb63",
              "title": "Tagging",
              "answer_tooltip": "",
              "output": "Tags allow for a logical grouping of cloud resources that work together to achieve a business goal. Tags facilitate fine-grain instrumentation, tracking and forecast of cloud spending.",
              "priority": "low",
              "heading": "Tagging"
            }
          ],
          "display_logic": [],
          "heading": "Optimizing"
        },
        {
          "id": "25db8d5c-b067-4642-9ce0-6ec233179dbd",
          "title": "How do you ensure that cloud resources are appropriately provisioned?",
          "explanation": "While governance is usually thought of in terms of compliance and security, components of Azure governance can also be laid down as a foundational scaffold to assist with Cloud cost management. This work will benefit your ongoing cost review process and will offer a level of protection for newly introduced resources.\n\nMore specifically, plan for the different cloud services that will be made available to the different solutions stakeholders via some sort of corporate self-service catalog or directly from the Azure portal itself to follow a \"T-shirt size\" approach. This way you can open on-demand consumption of cloud services while enforcing proven solutions that comply with expected performance and costs requirements. For example, consumers will choose T-shirt sizes for on-demand Virtual Machine resources in the x-small, small, medium, large, x-large range vs. allowing the consumers to pick specific virtual machine or managed disks SKU sizes which, for the uniformed consumer, if put together in non-optimal combination would yield poor performance and higher costs.\n\nT-shirt size-based offerings are one option, however organizations that would like a more open model for building cloud solutions while keeping cost savings in mind, should explore Azure Policy. Policies can set rules on management groups, subscriptions and resources groups which control which clouds service resource SKU size, replication features, and locations are allowed. Identify which Azure built-in policies can aid in cost savings and build new Azure custom policies for additional control requirements. For more information, visit https://docs.microsoft.com/en-us/azure/governance/management-groups/create?toc=%2Fazure%2Fbilling%2FTOC.json\n\nIn this way, developers can be enabled to self-service their resource creation, while preventing expensive resources from being provisioned and ensuring compliance against other identified cost boundaries. This eliminates the need for manual resource approval and speeds up the total provisioning time.\n\nIn addition, the enforcement of a resource tagging strategy can make it easier to link incurred costs back to an owner, an application, a business department or a project initiative, even if the costs span multiple resources, locations and subscriptions. Tagging in Azure is available at the resource group and single-resource (atomic unit) scope. Consider that tags are not inheritable, therefore when tagging resource groups, each single resource won't inherit those tags. This will become apparent when reviewing raw invoices or via cost analysis tooling in the Azure portal which allows for filtering options based on tagging. When filtering, if the tag was assigned on a per-resource basis then you would be able to get more granular reporting on which Azure resource belongs to which tag. On the other hand, when assigning tags at the resource group scope, additional efforts would be needed to map single resource to a specific tag as you would have to look for the parent resource group and obtain the tagging info there. Finally, it's important to understand a few limitations that [not all Azure resources can be tagged](https://docs.microsoft.com/en-us/azure/azure-resource-manager/tag-support) and not all taggable resources in Azure flow into the cost analysis tool in Azure. Identifying the clouds services meters that cannot be tagged or viewed in the cost analysis tool in Azure portal is an early step to consider.\n\nMicrosoft provides guidance on building out your governance needs and controls with the [Enterprise Scaffold](../cloud-adoption/appendix/azure-scaffold.md). This pivotal exercise, in conjunction with good review processes, enables your business to implement the controls and strategies that are relevant to your organization. It also helps you to structure your Azure tenancy and Subscriptions according to your business needs.",
          "context": "Describes how to best take advantage of the benefits of the cloud to minimize your cost.",
          "type": "checkbox",
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Does your service run seasonally or follow long-term patterns?\n\nSome Azure services offer the concept of ‘reserved instances', that can be utilized to yield further cost savings on predictable VM sizes that will be utilized for the solution's components.\n\nIf these are available, consider purchasing Seasonal/one-off or long term (RIs). Reserved instances are purchased in one-year or three-year terms, with payment required for the full term up front. After purchase, the reservation is matched up to running instances of the same SKU size or same-family SKU sizes and hours from your reservation will be decremented accordingly.\n\nNote that while Reservation expects you to do similar planning that you'd perform before a hardware purchase, it is still an Operational Expense with all corresponding benefits. Consider purchasing reserved instances to fine tune cost savings on solutions that have been running in the cloud for a period to more confidently forecast the reserved instance sizes that will need to be purchased.",
              "priority": "medium",
              "title": "You know what your application lifespan is and automate appropriately",
              "id": "cb0dd4e3-ac82-4a4b-9035-35c1882d5b29",
              "heading": "Application lifespan and what should be automated"
            },
            {
              "answer_tooltip": "",
              "output": "In an on premises environment, we do not typically have the luxury of being able to access APIs to turn on/off, or scale up/down workloads, in the same way as we do in the cloud.\n\nAs we have a rich set of APIs, SDKs and automation technology that we can call upon, utilizing DevOps and even more classical automation principles enables us to ensure that the workload is available at an appropriate level of scale as needed.\n\nYou may not have to leave the service running all of the time, incurring a consistent cost, consider whether it is actually a business requirement to leave the service online permanently 24x7, or could we save cost by shutting down the service or scaling it down outside normal business hours?\n\nIf so, could we repurpose some compute / data resources for other tasks that run out of business hours? See the [Compute Resource Consolidation](../patterns/compute-resource-consolidation.md) pattern and consider containers or elastic pools for more compute and data cost flexibility, more on this below.",
              "priority": "medium",
              "title": "You use DevOps and deployment automation.",
              "id": "9f69d5b5-65a2-4219-b2f8-684a628e01ba",
              "heading": "DevOps and deployment automation"
            },
            {
              "id": "bd86ce31-53c4-467a-9f6c-d9e3876a09c2",
              "title": "Perform capacity planning iteratively.",
              "answer_tooltip": "",
              "output": "One of the largest benefits of cloud computing is its rapid adaptability to changes in capacity. Having a good understanding of capacity requirements allows organizations to allocate resources with minimal overhead and grow/shrink on demand.",
              "priority": "high",
              "heading": "Iterative capacity planning"
            },
            {
              "id": "e4bea6ce-0d7b-4f0f-b8b7-21875543ce54",
              "title": "Configure auto-scaling policies (both in and out).",
              "answer_tooltip": "",
              "output": "For certain application, capacity requirements may swing over time. Auto-scaling policies allow for less error-prone operations and cost savings through robust automation.",
              "priority": "medium",
              "heading": "Auto-scaling policies"
            },
            {
              "id": "52d9d2fb-823e-485e-8186-d31bfde58bd4",
              "title": "Select right resource offering size (VM, disk, database).",
              "answer_tooltip": "",
              "output": "Certain infrastructure resources are delivered to customers as fix-sized building blocks. Ensuring that these building blocks are adequately sized is important to meet capacity demand as well as to eliminate waste.",
              "priority": "high",
              "heading": "Right-sizing your resources"
            },
            {
              "id": "e927b1b7-431f-4aac-9ac6-0d9a7693093d",
              "title": "Choose appropriate service that matches requirements.",
              "answer_tooltip": "The number of cloud services in Azure continues to grow and services have similar overlapping features, capabilities and functionality. It is important for customers to select the service that match requirements at the most favorable cost profile.",
              "output": "Any architecture planning starts with a careful enumeration of requirements. Architecting a Cloud solution is no different. As with any technology system, requirements are in place to ensure that the needs of the stakeholders are addressed and it is vital to remember that optimal design does not equal the lowest cost design.\n\nCost is another requirement lever that can be adjusted as other requirements dictate, resulting in a scenario of complex tradeoffs between the areas that architects are seeking to optimize for: such as security, scalability, resilience and operability. A solution architecture could perfectly address the challenges of these areas, however if the solution costs too much, the business will quickly look for alternate options to reduce cost. In some cases, the cost of not meeting expectations, internal or external, will outweigh the solution cost and the architecture is accepted, whereas in others the risk may be accepted in favor of a cheaper solution architecture.\n\nIt is also important to remember that designing the application to meet business requirements of scalability, resilience, recoverability and more, is still the responsibility of the customer, and is not handed off to the cloud vendor.\n\nCommunication between stakeholders is vital in these scenarios. The overall team must be clear on **and aligned on** the requirements. If not, then this potentially risks the success of the overall solution, not just from a cost perspective. If the requirements are not shared amongst all members, then there is a potential that the solution may not meet the intended business goals. For example, members of the development team believe that the resilience requirements of a monthly batch processing job are low, so design it to work as a single node without scaling capabilities. However, the architecture team had considered that component to be vital, and requires the ability to automatically scale out, and route requests to worker nodes. This lack of clarity potentially introduces a point of failure into the system, putting at risk the Service Level Agreement of the solution and would likely cause an increase in operational cost once the component is rearchitected.\n\n### Compliance and regulatory requirements\n\nIn highly regulated industries, compliance requirements play an important part in architecture decisions. In the cloud, these decisions will usually have an impact on cost. For example, requiring that all data stores be encrypted can be achieved via the use of Azure Policy at no extra charge, however Azure regions built specifically for high compliance needs (e.g. Azure Germany or Azure Government (USA) have higher service costs.\n\nRegulatory requirements can also dictate where data must or cannot reside, which in turn also impacts your data replication options for resiliency & redundancy. The ultimate location of your cloud solution is known as the landing zone, typically consisting of logical containers such as a subscription and resource group, where your cloud infrastructure components exist. The location of these resources (known as the Azure region) impacts the cost of these resources. Care should also be taken that cost tradeoffs such as locating resources in a cheaper region, are not then negated by the cost of network ingress & egress or by a lower application performance due to increased latency.\n\nUnderstand your compliance and regulatory requirements and how both the cloud services and your architecture decisions support these, as both compliance and the effects of non-compliance remain the responsibility of your organization.\n\n### Security requirements\n\n- Authentication, MFA, conditional access, information protection, JIT/PIM (premium Azure AD features=\\$)\n\n### Availability, business continuity and disaster recovery requirements\n\n- Availability requirements will have significant cost impact. For example, does the application have a given Service Level Agreement that it must meet?\n\n- An application hosted in a single region may cost less than an application hosted across regions, due to replication costs or extra nodes being required. However, it may be that the overall Service Level Agreement, Recovery Time Objective (RTO) and Recovery Point Objective (RPO) drive towards more costly design choices that must support higher availability requirements.\n\nIf your service SLA, RTOs and RPOs allow for this, then it can be dramatically cheaper to consider options such as pre-building automation scripts and packages that would re-deploy the DR components of the solution from the ground-up in the event of a disaster or – simply resort to using Azure platform managed replication. Both options should yield cost savings against dual deployments as less cloud services need to be pre-deployed and managed, leading to less wastage. Which data/workloads\n\n- RTO/RPO\n- Availability sets\n- Geo redundancy\n- Azure backup\n- Cost angle on these\n\nYou need to consider costings when defining your strategy for HA and DR and how the cost of downtime will impact your overall spend. In general - if the cost of HA exceeds the cost of downtime of the application you are probably over-engineering your HA strategy. If your cost of HA is significantly less than the cost of a reasonable period of downtime for your service (given the current service levels, RPO, RTO and 9's SLA, then you may well need to invest more.\n\nIn summary, if downtime is likely to cost you \\$1 per hour, you probably don't need to spend a large amount of money on HA and can resort to recovery from your backup and DR processes in the event of a major issue. However, if your downtime is likely to cost \\$1m per hour then you would more than likely be prepared to invest more in the HA and DR processing of the service, it's a 3 way trade-off between cost of service provision, your availability requirements and your organization's attitude to risk.\n\nFor strong alignment of business goals, start with your high-level requirements. In addition to traditional requirements, be sure to consider the unique opportunities afforded by cloud services. What are you aiming to achieve by building your architecture in the cloud?\n\nTypical answers are:\n\n- Taking advantage of features only available in the cloud, such as intelligent security systems, regions footprint, or resiliency features;\n\n- Using the on-demand nature of the Cloud to meet peak or seasonal requirements, then releasing that cost investment when it is no longer needed;\n\n- Consolidating physical systems;\n\n- Retiring on-premises infrastructure;\n\n- Reducing hardware or data center management costs;\n\n- Increasing performance or processing capabilities, including services like big data and machine learning;\n\nMeeting regulatory considerations, including taking advantage of certified infrastructure\n\nUnderstand that requirements may vary over time as the solution is optimized once in the cloud. For example, one key differentiator of cloud architectures is the ability to scale dynamically, making it possible to realize cost savings through automatic scaling. Consider as part of the requirements for the components of the solution, the metrics that each resource exposes in Azure and build your alerts on baseline thresholds for each metric. This way the solution can be built to alert the admins on when the solution is utilizing its supporting cloud services at capacity thus allowing the admin to better fine tune the solution's resources to target SKUs based on current load. Over time, the solution can be optimized to auto-heal itself when alerts are triggered. Additional alerts can be set on allowed budgets for the different solutions either at the resource group or management groups scopes. This way, both cloud services performance and budget requirements can meet at a happy medium via alerting on metrics and budgets.",
              "priority": "medium",
              "heading": "Requirements"
            }
          ],
          "display_logic": [],
          "heading": "Provisioning"
        },
        {
          "id": "0d983b47-0da8-4200-b6b6-12f97ce49b0f",
          "title": "How is your organization modelling cloud costs?",
          "explanation": "Mapping your organization's hierarchy with each business units' cloud consumption needs is likely one of the first requirements that business leaders care about on their journey of understanding the value of cloud. Most importantly, it allows business leaders to have a structured view on how cloud services are being governed and consumed. The cloud provider often provides different options to build hierarchy and grouping (via management groups, subscriptions, resource groups, etc.) across the cloud services it provides. Being able to map these logical containers with your organization's hierarchy will yield a clear path towards defining where cloud services are deployed and how they are governed.",
          "context": "Describes strategies to model your cloud costs",
          "type": "checkbox",
          "choices": [
            {
              "answer_tooltip": "",
              "output": "From the real experience, it is not easy to estimate cloud cost at the initial step of migration to the cloud. In many cases first calculation of cloud resources is quite inaccurate and relays more on more familiar and common on-prem approach. Two main pitfalls of the initial calculation are: either cloud is much more expensive comparing to on-prem or it is cheaper but some services are missed from the calculation:\n\n- Let us first take a look at the situation when cloud calculation is shocking you comparing to on-prem calculation, being much more costly – in most cases, if you build you on-prem DC using best-practices and guidance, on-prem DC cost would be close to cloud. But you have to be aware that you haven't missed in your on-prem calculation important parts of the solution such: cooling, electricity, IT labor cost, maintain security and disaster recovery etc. In order to be sure that your on-prem calculation contain all important part – we strongly suggest you to check with Total Cost of Ownership (TCO) Calculator for Azure (https://azure.microsoft.com/en-us/pricing/tco/calculator/)\n\n- As for the second pitfall – it is easy to forget to add or choose the right storage type for the solution, or skip networking cost (especially downloading big amount of data – which is chargeable). Or simply – choose the cheaper and smaller VMs which simply cannot provide expected performance for the solution.\n\nOf course, there are number of the real experts on the market within Microsoft Partners eco-system who have real experience with real complex projects. But sometimes in very big and complex project even for them difficult accurately calculate the real cloud services cost. Good thing here is that even the real cost is hardly estimated at the very first theoretical level - first POC (proof of concept) brings a lot of texture and helps to produce more accurate calculation.",
              "priority": "medium",
              "title": "You estimate and compare your costs.",
              "id": "08da9263-155b-452f-a0ea-b5390108cf39",
              "heading": "Estimating & comparing costs"
            },
            {
              "answer_tooltip": "",
              "output": "Ensure that your Cloud environments are integrated into any IT-related operational processes, from the provisioning of users and their application access through to your business continuity & disaster recovery processes. This process mapping may uncover areas where additional Cloud spend is needed, to facilitate the desired business outcomes in a timely manner or highlight where existing processes can be changes to take advantage of new cloud capabilities and tooling.",
              "priority": "medium",
              "title": "You are standardizing",
              "id": "b694680e-b870-45dd-a3c3-46e8521cf543",
              "heading": "Standardization"
            },
            {
              "answer_tooltip": "",
              "output": "The Cloud brings a new set of technical capabilities and tools for your organization's technical staff, as well as your business users. Identify training requirements & associated costs for technical staff to perform Cloud migration projects and Cloud application development or re-architecture. Also include training initiatives to enable ongoing Cloud management (such as identity management, security configuration and response and systems monitoring and automation) or consider outsourcing alternatives.\n\nEnsure that staff have access to ongoing training and relevant Cloud announcements for the duration of your Cloud investment, as the pace of Cloud will see tooling updates and new capabilities released continuously. Real experience of many customers across the globe shows that after attending specific cloud training and passing dedicated [Microsoft Exams](https://www.microsoft.com/en-us/learning/exam-list.aspx) (AZ, MS, MB etc,) the cost starts to goes down because of more optimal usage of the services based on the received knowledge.\n\nConsider onboarding offerings that may be available for free for your organization to leverage, such as [FastTrack for Azure](https://azure.microsoft.com/en-us/programs/azure-fasttrack/partners/), to help speed up your adoption, build your confidence in the platform and set you up for success.",
              "priority": "medium",
              "title": "You are educating your employees about the cloud and how it's priced?",
              "id": "e7bac9f9-2bab-4dbf-9317-f1a83ec2c8c9",
              "heading": "Education"
            },
            {
              "answer_tooltip": "",
              "output": "You will need to think about how to implement cloud cost governance controls (Azure Policy, Resource Tags, Budgets), including the Enterprise Scaffold (more details and links to docs) https://docs.microsoft.com/en-us/azure/cost-management/tutorial-acm-create-budgets?toc=/azure/billing/TOC.json",
              "priority": "medium",
              "title": "You have appropriate governance in place.",
              "id": "69a1160c-df4a-41dd-ab8a-8e14e4d6ef62",
              "heading": "Governance"
            },
            {
              "id": "869aca36-5055-43fe-8a04-18070c46900f",
              "title": "Resources as utility.",
              "answer_tooltip": "",
              "output": "The cloud is all about change. Your resources in the cloud are virtual, and unless built for durability, are to be considered ephemeral. Your provider may reboot or replace your resources for troubleshooting or updates. Mapping resources as 1 to 1 drop in replacement for on-prem resources is not recommended.",
              "priority": "high",
              "heading": "Treat resources as a utility"
            },
            {
              "id": "f72fc7d3-60f3-4b2e-8a25-99444e6da05f",
              "title": "Reserved Capacity",
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "heading": "Leverage reserved capacity"
            },
            {
              "id": "1125f616-9f4e-4f88-b07f-4d91cfce30a7",
              "title": "Low priority VMs.",
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "heading": "Low-priority VMs"
            },
            {
              "id": "d8036b2f-c4e5-4ba3-ab5a-c32409be14d0",
              "title": "Platform as a Service.",
              "answer_tooltip": "",
              "output": "",
              "priority": "high",
              "heading": "Platform as a Service"
            }
          ],
          "display_logic": [],
          "heading": "Modelling"
        },
        {
          "id": "77e0c51d-5ab9-4fdb-8648-13c230aca0cf",
          "title": "How do you manage the storage footprint of your digital assets?",
          "explanation": "Describes some of the decisions you may need to make when optimizaing your data footprint for cost.",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "answer_tooltip": "Data Transfer costs, most notably, network egress from Azure are areas typically not factored in to multi-cloud and hybrid architecture decisions. Not understanding the true cost of network transfer is a key area often overlooked.",
              "output": "Bandwidth refers to data moving in and out of Azure datacenters. Most of the time inbound data transfers (data going into Azure datacenters) are free. For outbound data transfers, data going out of Azure datacenters, the data transfer pricing is based on Billing Zones.",
              "priority": "medium",
              "title": "You are tracking how your data is moving in and out of Azure.",
              "id": "b76caf2e-81b2-4a8a-88bc-8215420dca0c",
              "heading": "Optimize data transfer"
            },
            {
              "id": "83214f4c-2386-4758-adba-ba9ad15ba0ec",
              "title": "Define and enforce data retention and archival requirements.",
              "answer_tooltip": "",
              "output": "In typical systems, not all data needs to be available for online processing. Data no longer needed can be removed, freeing up underlying storage.",
              "priority": "medium",
              "heading": "Data retention and archival"
            },
            {
              "id": "c04d8335-05b6-494e-8689-5b68ef743069",
              "title": "You have appropriately specced out your data storage and partitioned it correctly",
              "answer_tooltip": "In typical systems, not all data needs to be available for online processing. For the data that needs to be retained long-term, finding the right tradeoffs on durability and latency against costs allows for cost changes.",
              "output": "Choosing an inappropriate data store or one that is mis-configured can have a huge cost impact on your design and you can save tens of thousands of dollars by getting the data tier correct.\n\nConsider storing binary image data, whilst it is certainly possible to store it in Azure SQL Database as a varbinary(MAX) column type, you should strongly consider storing the data in Azure Blob Storage Block Blobs and if your design requires SQL, then store a lookup table in SQL Database, and retrieve the document on the fly to serve it to the user in your application middle tier.\n\nThis way the majority of your data capacity is consumed with fast, cost-effective blob storage that is a good few orders of magnitude cheaper than Azure SQL Database, but where needed – your use of SQL Database is highly targeted to high speed data lookups and set-based operations, where SQL really shines. This pattern is called polyglot persistence and it doesn't just apply to PaaS services, consider that Azure ‘Hot' Block Blob Storage costs around 1/50 of the cost of the equivalent size of the Premium SSD volume you would be placing the database on, and this model still applies even in an IaaS world.\n\nNote that you **don't need to make all of your decisions up-front** and cost-design is more than just an app-dev concern, many Azure services can be used to enhance applications ability to scale dynamically, even where they may not have been originally designed to do so.\n\nFor example, many asp.net stateful web applications can be made stateless (and hence can be auto scaled for a dramatic cost-benefit) by using [Azure Redis Cache](https://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-aspnet-session-state-provider), or CosmosDB as a back-end session state store via a [Session State Provider](https://github.com/aspnet/AspNetSessionState).\n\nConsider also, that you can reduce load on servers that repeatedly serve the same content, intelligent use of CDN and caching services can dramatically decrease load on front-end servers that are continually rendering dynamic content which doesn't change frequently. Be aware that every render cycle of a page or payload consumes both compute and memory, and with CDN you trade this off for pre-canned storage and bandwidth costs, and the savings can be dramatic, especially for static content services (such as JavaScript Single-Page apps and media streaming content), refer to the [Static Content Hosting Pattern](../patterns/static-content-hosting.md).\n\nThink about infrastructure costs too, for example consider using https for securing content transmission of a service instead of VPN based traffic transmission which requires a VPN Gateway, note that this is a very specific optimization that should be considered in the wider security context, as well as a cost design consideration.\n\nIn both cases the traffic is encrypted and passing over a public network. In the first case (https) your cost for encryption and decryption is absorbed into the web service costs, in the second case (VPN) the cost is clear and discrete, but is additional to the web service itself.\n\nIt is important to not view the cost component of technology architecture design as a one-off consideration. While it does hold an important place in Cloud design principles for new or migrating capabilities, you should also establish controls, business processes and a regular cadence for reviewing your Cloud spend. This approach will minimize reactive, unplanned cost optimization drives/efforts/initiatives.\n\nWhile you may already have processes in place for reviewing I.T. costs, there are some factors to managing Cloud costs that are unique, even when compared to other \"as a service\" utility spends. The flexibility of your Cloud architecture, the business benefits of your Cloud spend and the opportunities from new Cloud capabilities must all be taken into consideration.\n\nCosts are –\n\n- **An operational metric** An increase in cost can be an indicator of a problem, such as a code change leading to a dramatic increase in chargeable CPU cycles. If left unchecked, costs can scale out of proportion to the resulting business benefit, profit or transaction volume.\n\n- **A success indicator**.  An increase in cost can be an indicator of an increase in demand or sales, especially if the Cloud applicating is autoscaling to maintain performance. In this case, the cost can be proportionate to or even less than the resulting business benefit, profit or transaction volume.\n\nTherefore, a bigger bill is not always indicative of a problem that needs to be fixed. The Cloud is a fundamental facilitator of business success: giving an organization the ability to quickly take advantage of new opportunities and the flexibility to quickly scale to meet demand. Is the cost increase in alignment with a greater volume of customer transactions or website traffic? Is the ratio of costs vs customer transactions acceptable? Sometimes it's not easy to make a direct correlation between increased cost and increased business benefit but having an awareness of business initiatives or business performance can help.",
              "priority": "medium",
              "heading": "Use the right type of data store and partition or scale it appropriately"
            },
            {
              "id": "38958b65-504d-4d6f-bdeb-8b37c4cb4ed7",
              "title": "Selection of appropriate storage service.",
              "answer_tooltip": "",
              "output": "Some types of storage are better suited to different workloads.",
              "priority": "high",
              "heading": "Appropriate storage services"
            }
          ],
          "display_logic": [],
          "heading": "Data management"
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "It's natural to reach for the invoice as the source of truth for your costs. Stakeholders should review this information in relation to other factors and data sources. Do you understand what your Cloud costs usually are? Do you know if there have been significant changes in either the business or the I.T. capabilities that may have contributed to a change in these costs? Make sure you identify which business-related data is relevant to your Cloud cost conversation, to ensure that all aspects are considered.\n\nIn addition to detailed usage information on your invoice, Azure provides you with tools that surface information such as incurred costs and makes recommendations on cost savings (such as downsizing virtual machines). Nonetheless, it's important to consider building additional custom solutions to fully maximize cost savings where identifiable:\n\n### Azure Cost Management – Cost Analysis\n\nCost Analysis allows you to view aggregated costs to understand where they have occurred over time and to identify your spending trends. This can be broken down into time periods and viewed against your budgets. You can also set the scope of the costs, for example view all costs incurred by resources with a certain tag or within a specific resource group. Cost Analysis provides inbuilt charts but also supports custom views and the ability to download grouped and filtered data in CSV format. For more information: https://docs.microsoft.com/en-us/azure/cost-management/quick-acm-cost-analysis\n\n### Azure Cost Management - Advisor\n\nAdvisor cost management recommendations proactively highlight areas of service underuse and steps that you can take to realize cost savings. This includes virtual machines which could be resized to a lower SKU, un-provisioned ExpressRoute circuits and idle virtual network gateways. You can act on these recommendations, postpone them or download them as a CSV or PDF file. To get started with Advisor cost management recommendations, visit: https://docs.microsoft.com/en-gb/azure/advisor/advisor-cost-recommendations. It's important to note, while Azure Advisor is a great tool offered at no additional cost, it does not provide an exhaustive list of recommendations across all underutilized or orphaned resources in Azure.\n\n### Power BI and Azure Consumption Insights\n\nFor more flexibility with your cost reports, Azure Consumption Insights can be read into Microsoft's Power BI service. With Power BI, you can then create custom dashboards and reports, ask questions of your data and publish & share your work. Note: Sharing requires Power BI Premium licenses. To connect your billing data to Power BI, visit: https://docs.microsoft.com/en-gb/power-bi/service-connect-to-azure-consumption-insights\n\n#### Azure Billing API and Azure Consumption API\n\nIf you have another preferred data analysis tool, Azure comes with Billing APIs (in Preview) that you can connect to. Import invoices, resource usage and rate cards: https://docs.microsoft.com/en-us/azure/billing/billing-usage-rate-card-overview or connect to additional usage data including Budgets and Reserved Instances consumption: https://docs.microsoft.com/en-us/azure/billing/billing-consumption-api-overview\n\n>[!NOTE]\n> There are several different ways of purchasing Azure services and not all of them are supported by Azure Cost Management. For example, detailed billing information for services purchased through a Cloud Solution Provider must be obtained directly from your CSP. For more information on supported cost data, visit https://docs.microsoft.com/en-us/azure/cost-management/understand-cost-mgt-data\n\n### Custom Solutions\n\nLeverage Azure APIs to build custom scripts that can run on a schedule and identify resources that are orphaned, such as unattached managed disks, load balancers, application gateways, etc. These orphaned resources, while unused, still incur a monthly flat rate. Further resources that are empty such as Azure SQL servers with no user databases. Just as orphaned resources, these empty resources incur a monthly flat fee separate from the pay-per-GB fees. Finally, resources that are stale in utilization, for example, blobs or tables in storage accounts being a result of VM diagnostics. Check for timestamps of last-use or modification of the item to determine if it should deleted.\n\nRegardless of how you gather your cost information, make sure you develop and communicate a consistent business process for capturing and communicating this incurred cost information to your stakeholders.",
              "priority": "medium",
              "title": "Do you understand what your Cloud costs usually are and what they are today?",
              "id": "016f1b7b-663c-450b-a56a-60a502cb14c5",
              "heading": "Gather information"
            },
            {
              "answer_tooltip": "",
              "output": "In the Implementation section, you established your cost budget amounts, time periods and email alerts. Like any systems monitoring, this may need refining over time. So what happens when one of those alerts lands in your Inbox?\n\nFirst, check the current consumption data. Budget alerts do not fire in real time and there may be a delay (up to 8 hrs) between this alert and your current incurred cost actuals. Is there any significant difference between the alert level and what you're seeing now?\n\nThen you'll need to gather the appropriate people to discuss the cost trend, possible causes and any required action. That is likely to include people like application owners, who should be clearly identifiable through appropriate resource tagging. Who else do you need to involve (for example, business department owners) to provide the bigger picture of why a budget limit has nearly been reached? Remember this isn't just a systems question – it may also involve taking into account recent business activities which have been unexpectedly high.\n\nAnd finally, you'll need to reach an agreement on any action that is required, either short term or long term. Do you need to temporarily increase the budget alert threshold? Will you diarise to reduce it again for the next billing period or does it need to remain at a new, higher level? Who needs to sign off that increase in spend? Who will make the business decision that the increased budget it justified, for the business value it delivers or the demand it meets?\n\nWere the costs due to the creation or overrunning of unnecessary or expensive resources? Do you need to implement additional Azure Policy controls to prevent this in the future, or add automation to ensure scheduled virtual machine shutdowns?",
              "priority": "medium",
              "title": "You monitor your cloud costs and respond to alerts.",
              "id": "f60a846f-90af-4d4c-9873-39da78d6f678",
              "heading": "Responding to cost alerts"
            },
            {
              "answer_tooltip": "",
              "output": "Effective ongoing cost management includes informed cost reviews with key stakeholders. These can happen on a regular cadence but may also be required reactively if the cost management tools alert to a potential cost issue, such as a budget limit being reached.\n\n### Identify stakeholders\n\nWhich of your regular financial stakeholders need visibility of and input into your Cloud costs? Do they have a fundamental knowledge of Cloud billing, capabilities and business benefits to enable them to understand both the financial metrics and the impact of their recommendations or decisions? Consider what additional knowledge or training they may need to help them understand Cloud cost metering and Cloud architectures.\n\nWhich additional stakeholders also need to be present? This could include key application owners, systems administrators who monitor and back-up Cloud systems, and business unit representatives.\n\nWhich other stakeholders may be required but only when necessary? Are your Cloud resources appropriately tagged so you can easily identify owners of systems or applications that are contributing to cost noise? Are they aware that their participation in cost reviews may be required and what is expected of them?\n\n### Determine frequency\n\nWithin your existing business processes, there may be other cost reviews that occur which would align with adding a review of your Cloud costs, or maybe you need to schedule an additional meeting. Cloud costs can be reviewed:\n\n- During the billing period – for an awareness of the estimated pending billing;\n\n- After the billing period – to review actual spend with activity that occurred that month or\n\n- On an ad-hoc basis – usually triggered by a [budget alert](https://docs.microsoft.com/en-us/azure/cost-management/cost-mgt-alerts-monitor-usage-spending) or Azure Advisor recommendation.\n\nWeb Direct (pay as you go) and CSP billing occurs monthly. Note that while Enterprise Agreement (EA) billing occurs annually, each month's consumption counts towards your EA budget, so these costs should still be reviewed monthly.",
              "priority": "medium",
              "title": "You perform cost reviews",
              "id": "4cc848d2-a203-4d78-a54a-c46b2a673ec9",
              "heading": "Performing cost reviews"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Setup alarms",
              "id": "c0331fd0-5e4b-4b41-86dd-8d8c4701361c",
              "heading": "Setup alarms"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Define budgets",
              "id": "32a9a478-00b1-4754-ab44-d0587668f350",
              "heading": "Define budgets"
            }
          ],
          "title": "How are you monitoring your costs?",
          "type": "checkbox",
          "id": "79869b1f-b91c-4c75-a961-785da7a687a8",
          "heading": "monitoring",
          "explanation": "Describes strategies that you can leverage to monitor your cloud costs, and act on them appropriately."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Both TCO and ROI are key financial metrics that organizations may wish to quantify for their Cloud investment. In some cases, they are also looking to compare these metrics to existing on-premises equivalents.\n\nIt can be a challenge to get these figures accurate though, due to several reasons:\n\n- On-premises TCO may not accurately account for ‘hidden' expenses, such as under-utilization of purchased hardware or network maintenance costs (labor & equipment failure).\n\n- Cloud TCO may not accurately account for a drop in the organization's operational labor hours, due to the Cloud provider's infrastructure or platform management services being included in the cloud service pricing or the additional operational efficiences of cloud tools. This is especially true at a smaller scale, where the Cloud provider's services do not result in the ability for the organization to reduce IT labor head count.\n\n- ROI may not account accurately for new organizational benefits due to cloud capabilities, e.g. improved collaboration, reduced time to service customers, fast scaling with minimal or no downtime.\n\n- ROI may not account for organizational business process re-engineering, which may be necessary to fully embrace cloud benefits. In some cases, this re-engineering may not occur at all, leaving an organization in a state where they are using new technology in old ways, therefore stifling the full benefits of their cloud investment.\n\nIt is worth examining the TCO and ROI in full, exploring all costs and potential benefits. For migration projects, the [Microsoft Azure Total Cost of Ownership Calculator](https://azure.microsoft.com/en-au/pricing/tco/calculator/) may assist, as it pre-populates some common cost but allows you to modify the cost assumptions.",
              "priority": "medium",
              "title": "You are monitoring your cost versus the value obtained",
              "id": "739b9d97-3a0e-49fa-b9ea-f6c324dffe51",
              "heading": "Cost versus value"
            },
            {
              "answer_tooltip": "",
              "output": "Azure has datacenters all over the world and each datacenter is placed within a zone within a region or just a region. Usage costs vary between locations that offer Azure products, services, and resources based on popularity, demand, and local infrastructure costs. Azure also offers differentiated cloud regions for specific security and compliance requirements.\n\nFor example, you might want to build your Azure solution by provisioning resources in locations that offer the lowest prices, but this would require transferring data between locations if dependent resources and their users are in different parts of the world. If there are meters tracking the volume of data that moves between the resources you provision, any potential savings you make from choosing the cheapest location could be offset by the additional cost of transferring data between those resources.\n\nA location can be thought of as a region or a zone within a region in Azure. Cross regional traffic and cross-zonal traffic incur additional costs on the solution. The cross regional or cross zone additional costs does not apply to services labeled as global services in Azure such as Azure Active Directory. In addition, not all Azure services support zones and not all regions in Azure support zones. Choosing a region or a zone within a region for an Azure service is an opt-in activity. Before opting-in, consider how mission critical is the application to have footprint of its resources cross zones and/or cross regions. If it's non-mission critical or dev/test, consider keeping the solution and its dependencies in a single region or single zone to leverage the advantages of choosing the lower cost region.",
              "priority": "medium",
              "title": "You are leveraging location appropriately",
              "id": "77316384-d88d-4f0c-85e1-950153e4ec47",
              "heading": "Location"
            },
            {
              "answer_tooltip": "",
              "output": "The Azure Marketplace offers both the Azure team's first-party products and services, as well as, services from third-party vendors. Different billing structures apply to each of these categories. The billing structures can range from free, PAYG, one-time purchase fee, or a managed offering with support and licensing monthly costs.",
              "priority": "medium",
              "title": "You are leveraging the Azure Marketplace appropriately.",
              "id": "68c27b15-6f4a-4048-a6e3-539507feb0a9",
              "heading": "Marketplace"
            },
            {
              "answer_tooltip": "",
              "output": "Costs are resource-specific, so the usage that a meter tracks and the number of meters associated with a resource depend on the resource type. The usage that a meter tracks correlates to several billable units. Those are charged to your account for each billing period, and the rate per billable unit depends on the resource type you are using. In some cases, there are also choices to be made about a resource type that impact the pricing, for example choosing to use a Standard HDD hard disk or a Premium SSD hard disk. Resource types will vary in features such as performance or availability, the design implications of which must be considered along with their cost. As a rule of thumb, start small for resources then scale the resource up as needed. For example, it's more cost effective to start with a small size in GB for a managed disk as you are incurring costs on the allocated storage vs. pay-per-GB model. Likewise, with ExpressRoute circuits start with a smaller bandwidth circuit and scale up as needed. In Azure it's generally easier to grow a service with little to no downtime vs. downscale a service, which usually requires deprovisioning or downtime.\n\nThe same applies to compute infrastructure, it's much easier to deploy an additional smaller instance of compute to work alongside a smaller unit in parallel than it is to restart an instance to scale it up, in general take the mentality of scale out – not up.",
              "priority": "medium",
              "title": "You are tracking which resource tier you are in and optimizing appropriately",
              "id": "9cf72641-6859-4f7f-a0d1-6d7257526f78",
              "heading": "Resource tier"
            },
            {
              "answer_tooltip": "",
              "output": "Azure usage rates and billing periods can differ between Enterprise, Web Direct, and Cloud Solution Provider (CSP) customers based on specific subscription types as described [here](https://azure.microsoft.com/en-us/support/legal/offer-details/). Some subscription types also include usage allowances or lower prices, which affect costs. For example, Azure [Dev/Test subscription](https://azure.microsoft.com/en-us/offers/ms-azr-0148p/) types offers lower prices on Azure services such as specific VM sizes, PaaS web apps and VM images with pre-installed software. On the other hand, Visual Studio subscribers obtain as part of their benefits access to [Azure subscriptions](https://azure.microsoft.com/en-us/offers/ms-azr-0063p/) with monthly allowances.",
              "priority": "medium",
              "title": "You are taking advantage of appropirate subscription offer types",
              "id": "351f3aa2-3785-4656-82d7-c21e040dc211",
              "heading": "Subscription offer type"
            }
          ],
          "title": "What trade-offs have you made to optimize for cost?",
          "type": "checkbox",
          "id": "e7408753-d4f2-4ef3-8a45-72bc07670ec1",
          "heading": "tradeoffs",
          "explanation": "Just like your on-premises equipment costs, there are several elements that will affect your monthly costs when using Azure services. These should be taken into consideration when planning your architecture, planning the location of your resources and when estimating your costs:",
          "context": "Describes some of the trade-offs you may decide to make when optimizing a workload for cost."
        }
      ]
    },
    {
      "id": "ed597c56-5599-4e14-a148-9575b6dc0102",
      "name": "Scalability",
      "questions": [
        {
          "id": "6a7775ac-4ae5-49ef-9ae8-ceafd7516376",
          "title": "How are you designing your applications to scale?",
          "explanation": "Application design is critical to handling scale as load increases",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "id": "f4fea556-8c81-4e49-9715-39a7f8fcfca8",
              "title": "Chose the right data store to match usage",
              "answer_tooltip": "Possibility of data use getting throttled.",
              "output": "Review SQL Database choices and performance guidance. Use Azure Managed Database Services for built-in automatic tuning.",
              "priority": "medium",
              "heading": "Choosing the right database"
            },
            {
              "id": "2284ac26-7be3-458a-bb96-d8c4695f5a02",
              "title": "Using dynamic service discovery for micro-services applications",
              "answer_tooltip": "In a dynamic environment, clients of a service will need to know about the available resources to call.",
              "output": "Clients of a service use either Client-side discovery or Server-side discovery to determine the location of a service instance to which to send requests.\n\nLearn More: [https://microservices.io/patterns/service-registry.html](https://microservices.io/patterns/service-registry.html)",
              "priority": "medium",
              "heading": "Dynamic service discovery for micro-services applications"
            },
            {
              "id": "fe142e43-c782-467a-af9a-b84262f65ef3",
              "title": "Utilize connection pooling",
              "answer_tooltip": "Inability to limit connection resources.",
              "output": "Use SQL Database connection pooling.",
              "priority": "medium",
              "heading": "Connection pooling"
            },
            {
              "id": "35e74034-cf86-4c27-9cfe-8b3c20ce9bbf",
              "title": "Compress data when appropriate",
              "answer_tooltip": "Reduce load on the network by compressing data in flight",
              "output": "Use GZip compression in web.config. Utilize bundling and minification.",
              "priority": "medium",
              "heading": "Data Compression"
            },
            {
              "id": "8e3058be-8434-46f9-b6db-de41c8e69d75",
              "title": "Use locking to ensure consistancy",
              "answer_tooltip": "Possibility of poor performance from services with high latency.",
              "output": "Ensure appropriate consistency and isolation level while making database connection.",
              "priority": "medium",
              "heading": "Data Locking"
            },
            {
              "id": "59218e34-f303-4b96-80b0-f2432dba7317",
              "title": "Use async calls and waits to prevent locks",
              "answer_tooltip": "Possibility of locking the thread while accessing resources with higher latency, limited I/O, or network bandwidth.",
              "output": "Use the Asynchronous Programming pattern available in your programming language.",
              "priority": "medium",
              "heading": "Asynchronous Programming"
            },
            {
              "id": "1a6f861d-df86-4b8b-86bd-7846f639346d",
              "title": "Utilize Microservices",
              "answer_tooltip": "Inability to distribute application components to maximize the use of each compute unit.",
              "output": "Consider adopting a microservices architecture.",
              "priority": "medium",
              "heading": "Microservices"
            },
            {
              "id": "f5b436a2-aad6-43b7-9d6e-563713263d7d",
              "title": "Using queues",
              "answer_tooltip": "Inability to route requests and balance application load.",
              "output": "Employ the Load Levelling pattern using Azure queues/questions.",
              "priority": "medium",
              "heading": "Queuing and batching requests"
            },
            {
              "id": "48e520f4-3ae0-4185-8e61-0b194cab0f9e",
              "title": "Avoid sticky sessions and client affinity",
              "answer_tooltip": "Possibility of overhead in storing, retrieving, and maintaining state information.",
              "output": "Avoid using Application Request Routing (ARR) Affinity in the App Service Environment (ASE).",
              "priority": "medium",
              "heading": "Session affinity"
            },
            {
              "id": "dfe40589-8d10-40c4-85cd-e15ab0ce3b19",
              "title": "Automatically scale when load increses",
              "answer_tooltip": "Possibility of delay in additional resources/capacity increase.",
              "output": "Use Autoscaling guidance. Implement autoscale for services.",
              "priority": "medium",
              "heading": "Autoscaling"
            },
            {
              "id": "1fadc07b-c206-4e16-bf2c-67aa3c5bec6a",
              "title": "Utilize background jobs",
              "answer_tooltip": "Possibility of application becoming unresponsive and not taking requests.",
              "output": "Review background jobs guidance. Use Azure Logic Apps to create and schedule regularly running tasks.",
              "priority": "medium",
              "heading": "Background jobs"
            }
          ],
          "display_logic": [],
          "heading": "App Design"
        },
        {
          "id": "b698bef8-a070-4190-8e31-848b135e2ac0",
          "title": "How are you thinking about Performance?",
          "explanation": "",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "id": "91563675-7699-46b8-a60d-311e92aede83",
              "title": "Have well defined performance goals (eg: throughput and latency)",
              "answer_tooltip": "Understand what sort of needs your application and users need when using your system.",
              "output": "Defining performance goals allows you to build architecture that aligns to your business needs.  Learn More: [https://docs.microsoft.com/en-us/azure/architecture/antipatterns/](https://docs.microsoft.com/en-us/azure/architecture/antipatterns/)",
              "priority": "medium",
              "heading": "Defining performance goals"
            },
            {
              "id": "17dbf926-0d77-459e-a44c-76895adcb2d4",
              "title": "Using horizontal scaling when possible",
              "answer_tooltip": "Also called scaling out and in, means adding or removing instances of a resource. The application continues running without interruption as new resources are provisioned. When the provisioning process is complete, the solution is deployed on these additional resources. If demand drops, the additional resources can be shut down cleanly and deallocated.",
              "output": "Horizontal scaling allows you to dynamically add and remove resources based on demand.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling](https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling)",
              "priority": "medium",
              "heading": "Horizontal Scaling"
            },
            {
              "id": "1eab2007-a8bd-440d-87eb-09a42e320ad5",
              "title": "Have policies to scale in (down) when your load decreases?",
              "answer_tooltip": "In addition to adding resources when additional load arises, you will need to remove resources when that load goes away for cost and performance reasons.",
              "output": "Scaling in removes unused resources when they become idle.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling](https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling)",
              "priority": "medium",
              "heading": "Scaling down when load decreases"
            },
            {
              "id": "aebd1ae8-4d79-4ed5-a625-9c8dc270b18f",
              "title": "Understand your performance bottlenecks? (components or goals)",
              "answer_tooltip": "Know all of the components used in your system and when they might fail or become a bottleneck when additional load is placed on them.",
              "output": "Understanding your performance bottlenecks is critical for designing systems that will have variable throughput over time.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/antipatterns/](https://docs.microsoft.com/en-us/azure/architecture/antipatterns/)",
              "priority": "medium",
              "heading": "Performance Bottlenecks"
            },
            {
              "id": "fa7fe27a-fb7a-47bf-84ec-ae009ed95a54",
              "title": "Gracefully handle throttling",
              "answer_tooltip": "Your application needs to gracefully degrade it's experience if being throttled by a dependent service",
              "output": "Learn More: [https://docs.microsoft.com/en-us/azure/architecture/patterns/throttling](https://docs.microsoft.com/en-us/azure/architecture/patterns/throttling)",
              "priority": "medium",
              "heading": "Throttling"
            },
            {
              "id": "1bc6e483-efd2-41c8-8415-ad02c95caa97",
              "title": "Use idempotent operations",
              "answer_tooltip": "When possible, design operations to be idempotent. That way, they can be handled using at-least-once semantics.",
              "output": "If a worker crashes in the middle of an operation, another worker simply picks up the work item.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/scale-out](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/scale-out)",
              "priority": "medium",
              "heading": "Idempotency"
            },
            {
              "id": "e769b84e-1c9a-4e51-9ff1-3546acda036d",
              "title": "Gracefully handle failures",
              "answer_tooltip": "Design an application to be self healing when failures occur. This requires a three-pronged approach: Detect failures. Respond to failures gracefully. Log and monitor failures, to give operational insight.",
              "output": "In a distributed system, failures happen. Hardware can fail. The network can have transient failures.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/self-healing](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/self-healing)",
              "priority": "high",
              "heading": "Failure handling"
            }
          ],
          "display_logic": [],
          "heading": "Performance"
        },
        {
          "id": "7352bd39-678a-4a2b-89df-3972c24ae9df",
          "title": "How are you handling user load?",
          "explanation": "",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "id": "821e7b7e-2432-4136-a708-dc0544187150",
              "title": "Have you done testing at expected peak load?",
              "answer_tooltip": "You should load test your application.",
              "output": "You should load test your application at the expected peak load to ensure there are no challenges around performance or stability when operating at full capacity. Learn More: [https://docs.microsoft.com/en-us/azure/devops/test/load-test/app-service-web-app-performance-test?view=azure-devops](https://docs.microsoft.com/en-us/azure/devops/test/load-test/app-service-web-app-performance-test?view=azure-devops)",
              "priority": "high"
            },
            {
              "id": "57718622-cd7d-416a-8731-a0a542e8e4ab",
              "title": "Know the limits of Azure resources as it relates to your workload",
              "answer_tooltip": "Different azure services have soft and hard limits associated with them.",
              "output": "Different azure services have soft and hard limits associated with them. Understand the limits for the services you consume so that you are not blocked if you need to exceed them. \n\nLearn More: [https://docs.microsoft.com/en-us/azure/azure-subscription-service-limits](https://docs.microsoft.com/en-us/azure/azure-subscription-service-limits)",
              "priority": "medium",
              "heading": "Azure service limits"
            },
            {
              "id": "b4e28c41-ba4d-4e60-9de8-07c7c25049c3",
              "title": "Understand the user load the system can handle",
              "answer_tooltip": "You must know the capabilities of your system to be able to meet demand.",
              "output": "Load test your application to understand how it performs at various scales.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling](https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling)",
              "priority": "medium",
              "heading": "Understanding application behavior under load"
            },
            {
              "id": "09d9771d-5f07-4004-b9e0-a9dc04d98efa",
              "title": "Know base & peak loads",
              "answer_tooltip": "Knowing the typical and maximum loads on your system help you understand when something is operating outside of it's designed limits.",
              "output": "Knowing the typical and maximum loads on your system help you understand when something is operating outside of it's designed limits.  Monitor traffic to your application to understand user behavior. Learn More: [https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling](https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling)",
              "priority": "medium",
              "heading": "Measuring typical loads"
            },
            {
              "id": "28613f56-5cd0-4318-b1b2-28bfd2a2643c",
              "title": "Know how long it takes scaling to respond to events",
              "answer_tooltip": "Knowing how long it takes to scale allows you to understand how quickly you can adapt to changes in demand.",
              "output": "Knowing how long it takes to scale allows you to understand how quickly you can adapt to changes in demand. Scale quick enough to meet user demand, but not so quick that you end up with many unused resources. Learn More: [https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling](https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling)",
              "priority": "low",
              "heading": "Responding quickly to additional load"
            },
            {
              "id": "df8b22d8-3b9b-49a5-bd43-bc18d4baa2ac",
              "title": "Leverage appropriate caching",
              "answer_tooltip": "Caching can improve performance and helps to maintain consistency between data held in the cache and data in the underlying data store.",
              "output": "Applications should implement a strategy that helps to ensure that the data in the cache is as up-to-date as possible but can also detect and handle situations that arise when the data in the cache has become stale.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside)",
              "priority": "high",
              "heading": "Caching"
            },
            {
              "id": "89270a39-dc22-496e-bf13-0fa221e6dae7",
              "title": "Database storage is sharded when appropriate",
              "answer_tooltip": "Sharding can improve scalability when storing and accessing large volumes of data.",
              "output": "Divide a data store into a set of horizontal partitions or shards.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/patterns/sharding](https://docs.microsoft.com/en-us/azure/architecture/patterns/sharding)",
              "priority": "medium",
              "heading": "Database sharding"
            },
            {
              "id": "373984b8-a555-4958-a7a8-378ef6f7e53b",
              "title": "Do you know the availability of your SKU's?",
              "answer_tooltip": "Certain Azure SKU's are only available in certain regions.",
              "output": "Certain Azure SKU's are only available in certain regions. Understand which SKUs are available in the regions you operate in so you can plan accordingly. Learn More: [https://azure.microsoft.com/en-us/global-infrastructure/services/](https://azure.microsoft.com/en-us/global-infrastructure/services/)",
              "priority": "low"
            }
          ],
          "display_logic": [],
          "heading": "Load Testing"
        },
        {
          "id": "a504e528-b5e6-45b0-9d56-ae722e123666",
          "title": "How are you ensuring you have sufficient Capacity?",
          "explanation": "",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "id": "617f326b-536a-424d-8f64-789782d5fe7a",
              "title": "Using a Content Delivery Networks (CDN) if applicable",
              "answer_tooltip": "Content Delivery Networks (CDN) let you reduce load times, save bandwidth, and improve responsiveness.",
              "output": "With CDNs, you can cache static objects loaded from Azure Blob storage, a web application, or any publicly accessible web server, by using the closest point of presence (POP) server. CDNs can also accelerate dynamic content, which cannot be cached, by leveraging various network and routing optimizations.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/cdn/](https://docs.microsoft.com/en-us/azure/cdn/)",
              "priority": "medium",
              "heading": "Content Delivery Networks (CDN)"
            },
            {
              "id": "a38b7b87-7964-48e0-9ae3-a0ff9c6a2c7d",
              "title": "Aware of any events that will cause spikes in user load",
              "answer_tooltip": "Knowing if there will be sudden spikes in traffic (Superbowl, Black Friday, or Marketing pushes) can allow you to prepare your infrastructure ahead of time.",
              "output": "Work with your business and marketing teams to prepare for large scale events",
              "priority": "high",
              "heading": "Large scale event management"
            },
            {
              "id": "fe7458fa-7e8f-4fe5-a992-9f0c740e1430",
              "title": "Optimized resource choices (vm, database sizing, etc) to match the needs of your application",
              "answer_tooltip": "Identify the needs of your application and choose the resources that best fit those needs.",
              "output": "Right sizing your infrastructure to meet the needs of your applications can save you considerably as opposed to a 'one size fits all' solution often employed with on-premises hardware. Identify the needs of your application and choose the resources that best fit those needs. Learn More: [https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes)",
              "priority": "high",
              "heading": "Choosing the right resources"
            },
            {
              "id": "0f4d6be8-a93d-4a8a-bf92-d801c26f427b",
              "title": "Configured scaling policies using the appropriate metrics",
              "answer_tooltip": "Scaling on the correct metrics is critical to responding quickly to changes in demands and adding resources at the appropriate time.",
              "output": "Autoscaling rules that use a detection mechanism based on a measured trigger attribute (such as CPU usage or queue length) use an aggregated value over time, rather than instantaneous values, to trigger an autoscaling action. By default, the aggregate is an average of the values. This prevents the system from reacting too quickly, or causing rapid oscillation.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling](https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling)",
              "priority": "high",
              "heading": "Choosing metrics for scaling policies"
            },
            {
              "id": "309d9359-996b-4959-9783-330c9899b770",
              "title": "Automatically schedule autoscaling to add resources based on time of day trends",
              "answer_tooltip": "Preemptively scaling based on historical data can ensure your application is performant.",
              "output": "Preemptively scaling based on historical data can ensure your application is performant before your metrics indicate additional resources are required. Consider whether you can predict the load on the application well enough to use scheduled autoscaling, adding and removing instances to meet anticipated peaks in demand.\n\nLearn More: [https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling](https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling)",
              "priority": "medium",
              "heading": "Preemptively scaling based on trends"
            }
          ],
          "display_logic": [],
          "heading": "Capacity"
        },
        {
          "id": "f8101d60-5786-4798-a482-e90980e13d5a",
          "title": "How are you managing your data to handle scale?",
          "explanation": "",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "id": "5b9914bc-0b30-4867-b62e-91eaa3f68323",
              "title": "Using multiple databases",
              "answer_tooltip": "Possibility of poor query performance, complex scalability, poor management, and poor availability.",
              "output": "Implement partitioning guidance to meet scalability requirements.",
              "priority": "medium",
              "heading": "Managing database load"
            },
            {
              "id": "c459273e-7262-452f-8703-be19f7a36d16",
              "title": "Managing data consistency appropriately",
              "answer_tooltip": "Inability to improve scalability by reducing time needed for data synchronization.",
              "output": "Use Data Consistency Primer guidance. Ensure appropriate consistency and isolation level while making database connections.",
              "priority": "medium",
              "heading": "Data consistency"
            },
            {
              "id": "4e8be95d-47b7-4a4c-b165-635e713722bd",
              "title": "Optimize database queries & indexes",
              "answer_tooltip": "Possibility of poor query/database performance.",
              "output": "Review Query Tuning guidance. Utilize automatic tuning in Azure SQL Database. Use Azure Managed Database Services for automatic tuning.",
              "priority": "medium",
              "heading": "Database query optimimization"
            },
            {
              "id": "5de0cf80-7d09-40de-a0a3-7447fb2f1a6f",
              "title": "Documented plans for data growth and retention",
              "answer_tooltip": "Possibility of increased latency, and reduced application throughput and performance.",
              "output": "Manage future growth with choices provided by Azure SQL Database. Use Azure Managed Database Services for built-in security, automatic monitoring, threat detection, automatic tuning, and turnkey global distribution.",
              "priority": "medium",
              "heading": "Planning for data growth"
            }
          ],
          "display_logic": [],
          "heading": "Data Management"
        },
        {
          "id": "e589906f-a6c2-451c-b7da-b5186c612cd0",
          "title": "What scalability tradeoffs are you making?",
          "explanation": "",
          "context": "",
          "type": "checkbox",
          "choices": [
            {
              "id": "1c28b8ad-c6e4-4d8d-881b-fcb0609ee928",
              "title": "Balance performance and cost appropriately",
              "answer_tooltip": "While you can build a very high performance system, it can also be very expensive and not match the business requirements.",
              "output": "When architecting, strike a balance between performance and cost.",
              "priority": "low",
              "heading": "Balancing performance and cost"
            },
            {
              "id": "2ba81c3e-794e-4b7f-b834-dbce290d3225",
              "title": "Choosing the correct data store based on Capacity, Availability, and Performance requirements (CAP)",
              "answer_tooltip": "The CAP theorem implies that a distributed system must make trade-offs between availability and consistency.",
              "output": "Often, you can achieve higher availability by adopting an _eventual consistency_ model. Learn More: [https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/use-the-best-data-store](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/use-the-best-data-store)",
              "priority": "low",
              "heading": "CAP theorem"
            }
          ],
          "display_logic": [],
          "heading": "Tradeoffs"
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Do you know how long requests typically take?  Do you know when they take longer than normal?",
              "priority": "medium",
              "title": "Monitor and alert on user experience",
              "id": "c3d89c92-c78c-4b15-ae1b-cbf3dfe731f8",
              "heading": "Monitoring user performance"
            },
            {
              "answer_tooltip": "",
              "output": "You need to be able to view system utilization (CPU, Memory, Disk, etc) to troubleshoot system performance and look for additional metrics to scale on.",
              "priority": "medium",
              "title": "Graph system utilization metrics",
              "id": "df880e97-e112-45c9-980d-6c2bb48d4717",
              "heading": "Monitoring system utilization"
            },
            {
              "answer_tooltip": "",
              "output": "Do you know when resources were added to handle scale?  Do you know when they were removed?",
              "priority": "medium",
              "title": "Track when resources scale in and out",
              "id": "1f7261c1-d0cf-48e0-b3b7-b1f8e6b00054",
              "heading": "Monitoring scaling events"
            },
            {
              "answer_tooltip": "",
              "output": "You should know trends of increased load over time",
              "priority": "medium",
              "title": "Monitor and graph historical usage of the system",
              "id": "2cbb21af-0a9c-40b1-8aee-74aa66403bb3",
              "heading": "Historical monitoring"
            }
          ],
          "title": "How are you monitoring to ensure you're scaling appropriately?",
          "type": "checkbox",
          "id": "d6014104-7718-406a-b19b-c75d0038970b",
          "heading": "Monitoring"
        }
      ]
    },
    {
      "questions": [
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Containerization",
              "id": "3eeeeaf3-4d70-4c35-a411-85c1511ff7ab",
              "heading": "Containerization"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure Resource Manager templates to ensure that investments for one location are reusable in another.",
              "priority": "medium",
              "title": "Infrastructure as code",
              "id": "0d45ae83-8ea1-4f04-a83b-3c83fa7dd0b4",
              "heading": "Infrastructure as code"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure Resource Manager templates to define dependencies for resources that are deployed in the same template.",
              "priority": "medium",
              "title": "Dependency tracking",
              "id": "31188bee-846c-4d43-93cf-3ada6f7518d4",
              "heading": "Dependency tracking"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Limits",
              "id": "240f20b5-ee85-45a3-a951-fd0eca13c93a",
              "heading": "Limits"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Tagging and resource naming",
              "id": "65e191e0-c76b-42a5-ac8e-c1bd0281960a",
              "heading": "Tagging and resource naming"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Workload isolation",
              "id": "ae970f05-6ba1-4f88-8e05-6f26e98ddbd0",
              "heading": "Workload isolation"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Orchestration system",
              "id": "1574f6b7-a8b9-4026-8860-6cce055c3bd9",
              "heading": "Orchestration system"
            }
          ],
          "title": "How are you designing your applications to take into account DevOps?",
          "type": "checkbox",
          "id": "b5f44676-2a77-44fb-b34a-54c2658946b9",
          "heading": "App Design",
          "explanation": "Describes considerations that you should take into account while doing application design to optimize for DevOps."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Use automation runbooks with hybrid runbook worker to unify management by orchestrating across on- premises environments. Use webhooks to provide a way to fulfill requests and ensure continuous delivery and operations by triggering automation from ITSM, DevOps, and monitoring systems.",
              "priority": "medium",
              "title": "Automation of manual tasks",
              "id": "95a0d778-665a-4c88-b2e3-df6f8772330f",
              "heading": "Automation of manual tasks"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure Automation State Configuration to provide configuration management required for enterprise environments.",
              "priority": "medium",
              "title": "Monitor and update machine configuration",
              "id": "0c7ac8e2-5d4b-48c3-ae49-745c8164440b",
              "heading": "Monitor and update machine configuration"
            },
            {
              "answer_tooltip": "",
              "output": "Employ Update Management to manage VM updates in Azure, on- premises, or in other cloud providers.",
              "priority": "medium",
              "title": "Schedule deployments",
              "id": "68d5ce88-c768-4093-b2e6-dc5f1f43a72c",
              "heading": "Schedule deployments"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure Advisor to follow best practices to optimize Azure deployments and analyze your resource configuration and usage telemetry.",
              "priority": "medium",
              "title": "Monitor resource configuration",
              "id": "f63e08da-ea98-46f3-99e9-571e592d3299",
              "heading": "Monitor resource configuration"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure Resource Manager to define the dependencies between resources so they're deployed in the correct order.",
              "priority": "medium",
              "title": "How do you deploy, manage, and monitor a solution as a group rather than handling its components individually?",
              "id": "0cd8576e-f0cd-44e6-a451-8ba85b6a2af2",
              "heading": "How do you deploy, manage, and monitor a solution as a group rather than handling its components individually?"
            },
            {
              "answer_tooltip": "",
              "output": "Utilize Azure Resource Manager deployment modes to provision all resources specified in the template.",
              "priority": "medium",
              "title": "Repeated deployment of resources",
              "id": "b828376b-9cc5-4482-a222-5fd30491bf25",
              "heading": "Repeated deployment of resources"
            },
            {
              "answer_tooltip": "",
              "output": "Employ VSTS configuration management for visibility to dev and operations teams. Use PowerShell DSC for configuration management.",
              "priority": "medium",
              "title": "Configuration management",
              "id": "081bf246-3ce6-4ec3-9520-0792a518fa4a",
              "heading": "Configuration management"
            },
            {
              "answer_tooltip": "",
              "output": "Utilize VSTS bug tracking tool for establishing links between code and bugs. Use Bugzilla integration with VSTS.",
              "priority": "medium",
              "title": "Bug tracking",
              "id": "40671c39-02bc-4a79-8066-8ae3c26e9d0e",
              "heading": "Bug tracking"
            },
            {
              "answer_tooltip": "",
              "output": "Employ VSTS history and auditing for a consolidated view of changes to code and infrastructure.",
              "priority": "medium",
              "title": "Audit and track changes",
              "id": "91b0bbc2-06f2-4b23-b1e0-23220cacef70",
              "heading": "Audit and track changes"
            }
          ],
          "title": "How are you managing the configuration of your workload?",
          "type": "checkbox",
          "id": "2dd45adb-b0ac-4743-882c-2a0731423a9e",
          "heading": "configuration",
          "explanation": "Describes how to manage your configuration for your workload."
        },
        {
          "choices": [],
          "title": "How are you managing the data of your deployment infrastructure?",
          "type": "checkbox",
          "id": "2e34c82d-d769-4482-831b-6589744feb16",
          "explanation": "Describes considerations you should make for the data management of your DevOps infrastructure.",
          "heading": "Data management"
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Without detailed release process documentation, an operator might deploy a bad update or might improperly configure settings for your application. Clearly define and document your release process, and ensure that it's available to the entire operations team.",
              "priority": "medium",
              "title": "Your release process is documented",
              "id": "d2478642-4e83-436d-b98e-39d241ee8f08",
              "heading": "Document release process"
            },
            {
              "answer_tooltip": "Utilize VSTS continuous testing. Use VSTS Test Case Management for documenting and fixing bugs after test execution. Practice VSTS Unit, Integration, and UAT testing for code coverage.",
              "output": "To activate resources on demand, deploy solutions rapidly, minimize human error, and produce consistent and repeatable results, be sure to automate deployments and updates.\n\n### Automate as many processes as possible\n\nThe most reliable deployment processes are automated and *idempotent* &mdash; that is, repeatable to produce the same results.\n\n- To automate provisioning of Azure resources, you can use [Terraform](/azure/virtual-machines/windows/infrastructure-automation#terraform),\n    [Ansible](/azure/virtual-machines/windows/infrastructure-automation#ansible), [Chef](/azure/virtual-machines/windows/infrastructure-automation#chef), [Puppet](/azure/virtual-machines/windows/infrastructure-automation#puppet),\n    [Azure PowerShell](/powershell/azure/overview), [Azure CLI](/cli/azure), or [Azure Resource Manager templates](/azure/azure-resource-manager/template-deployment-overview).\n- To configure VMs, you can use [cloud-init](/azure/virtual-machines/windows/infrastructure-automation#cloud-init) (for Linux VMs) or [Azure Automation State Configuration](/azure/automation/automation-dsc-overview) (DSC).\n- To automate application deployment, you can use [Azure DevOps Services](/azure/virtual-machines/windows/infrastructure-automation#azure-devops-services), [Jenkins](/azure/virtual-machines/windows/infrastructure-automation#jenkins), or other CI/CD solutions.\n\nAs a best practice, create a repository of categorized automation scripts for quick access, documented with explanations of parameters and examples of script use. Keep this documentation in sync with your Azure deployments, and designate a primary person to manage the repository.\n\nAutomation scripts can also activate resources on demand for disaster recovery.\n\n### Use code to provision and configure infrastructure\n\nThis practice, called *infrastructure as code,* may use a declarative approach or an imperative approach (or a combination of both).\n\n- [Resource Manager templates](/azure/azure-resource-manager/template-deployment-overview) are an example of a declarative approach.\n- [PowerShell](/powershell/azure/overview) scripts are an example of an imperative approach.\n\n### Practice immutable infrastructure\n\nIn other words, don't modify infrastructure after it's deployed to production. After ad hoc changes have been applied, you might not know exactly what has changed, so it can be difficult to troubleshoot the system.\n \n### Automate and test deployment and maintenance tasks\n\nDistributed applications consist of multiple parts that must work together. Deployment should take advantage of proven mechanisms, such as scripts, that can update and validate configuration and automate the deployment process. Test all processes fully to ensure that errors don't cause additional downtime.\n\n### Implement deployment security measures\n\nAll deployment tools must incorporate security restrictions to protect the deployed application. Define and enforce deployment policies carefully, and minimize the need for human intervention.",
              "priority": "medium",
              "title": "Automation",
              "id": "66c191c5-fdd6-40e1-9a52-8f6e30deb2fd",
              "heading": "Automation"
            },
            {
              "answer_tooltip": "",
              "output": "Use VSTS release management for end-to-end traceability. Utilize VSTS history and auditing for a consolidated view of changes to code and infrastructure.\n\nOne of the challenges with automating deployment is the cut-over itself, taking software from the final stage of testing to live production. You usually need to do this quickly in order to minimize downtime. The blue-green deployment approach does this by ensuring you have two production environments, as identical as possible.\n\nUse VSTS Release Management for continuous delivery of software at a faster pace and with lower risk.",
              "priority": "medium",
              "title": "Release process",
              "id": "47634bb2-adab-4f08-98d4-1a439a4e250f",
              "heading": "Release process"
            },
            {
              "answer_tooltip": "",
              "output": "Use staging slots in Azure App Service.\n\nDeployment to various stages and running tests/validations at each stage before moving on to the next ensures friction free production deployment.\n\nWith good use of staging and production environments, you can push updates to the production environment in a highly controlled way and minimize disruption from unanticipated deployment issues.\n\n- [*Blue-green deployment*](https://martinfowler.com/bliki/BlueGreenDeployment.html) involves deploying an update into a production environment that's separate from the live application. After you validate the deployment, switch the traffic routing to the updated version. One way to do this is to use the [staging slots](/azure/app-service/web-sites-staged-publishing) available in Azure App Service to stage a deployment before moving it to production.\n- [*Canary releases*](https://martinfowler.com/bliki/CanaryRelease.html) are similar to blue-green deployments. Instead of switching all traffic to the updated application, you route only a small portion of the traffic to the new deployment. If there's a problem, revert to the old deployment. If not, gradually route more traffic to the new version. If you're using Azure App Service, you can use the Testing in production feature to manage a canary release.",
              "priority": "medium",
              "title": "Stage your workloads",
              "id": "41a8ff0d-69ae-461c-8265-974cf3c85d9a",
              "heading": "Stage your workloads"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure Automation for complete control during deployment, operations, and decommissioning of workloads and resources.",
              "priority": "medium",
              "title": "Manual activities",
              "id": "c7b88785-27f2-4812-99dc-39a37229e53a",
              "heading": "Manual activities"
            },
            {
              "answer_tooltip": "",
              "output": "Utilize Azure Resource Manager templates and scripts for automated resource provisioning.",
              "priority": "medium",
              "title": "Deployment scripts",
              "id": "06e234ab-f368-4054-a8c1-1c3885208e87",
              "heading": "Deployment scripts"
            },
            {
              "answer_tooltip": "",
              "output": "Use the blue/green or canary release deployment technique.",
              "priority": "medium",
              "title": "Deployment strategies",
              "id": "d909d488-9995-4335-96c2-2a5667dc95e0",
              "heading": "Deployment strategies"
            },
            {
              "answer_tooltip": "",
              "output": "Use VSTS release management for end-to-end traceability. Utilize VSTS history and auditing for a consolidated view of changes to code and infrastructure.\n\nTo capture as much version-specific information as possible, implement a robust logging strategy. If you use staged deployment techniques, more than one version of your application will be running in production. If a problem occurs, determine which version is causing it.",
              "priority": "medium",
              "title": "Logging and auditing",
              "id": "f5996176-1f18-49a0-bed5-7d112c9e9cc2",
              "heading": "Logging and auditing"
            },
            {
              "answer_tooltip": "",
              "output": "Use App Service deployment slots to fall back on last-known good menu. Run VSTS conditional rollback.\n\nThe most important step is to implement an architecture that supports the need to rollback. For instance, componentized, service based architectures lend themselves well to this. Persistent message queues and asynchronous services allow you to bring components down for rollback without impacting the main user base. Work towards something like the Blue-Green release pattern such that your application can stay available whilst you are working on one half of the system.\n\nIf a deployment fails, your application could become unavailable. To minimize downtime, design a rollback process to go back to a last-known good version. Include a strategy to roll back changes to databases and any other services your app depends on.\n\nIf you're using Azure App Service, you can set up a last-known good site slot and use it to roll back from a web or API app deployment.",
              "priority": "medium",
              "title": "Rollback plan",
              "id": "f069b073-28d0-49f1-8871-35c7326e2d8f",
              "heading": "Rollback plan"
            },
            {
              "answer_tooltip": "Use an App Service plan that offers multiple instances. Use virtual machine scale set. Deploy multiple instances of the web app.",
              "output": "An application that depends on a single instance of a service creates a single point of failure. To improve resiliency and scalability, provision multiple instances.\n\n- For [Azure App Service](/azure/app-service/app-service-value-prop-what-is/), select an [App Service plan](/azure/app-service/azure-web-sites-web-hosting-plans-in-depth-overview/) that offers multiple instances.\n- For [Azure Cloud Services](/azure/cloud-services/cloud-services-choose-me), configure each of your roles to use [multiple instances](/azure/cloud-services/cloud-services-choose-me/#scaling-and-management).\n- For [Azure Virtual Machines](/azure/virtual-machines/virtual-machines-windows-about/?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json), ensure that your architecture includes more than one VM and that each VM is included in an [availability set](/azure/virtual-machines/virtual-machines-windows-manage-availability/).\n\n### Consider deploying across multiple regions\n\nWe recommend deploying all but the least critical applications and application services across multiple regions. If your application is deployed to a single region, in the rare event that the entire region becomes unavailable, the application will also be unavailable.\n\nIf you choose to deploy to a single region, consider preparing to redeploy to a secondary region as a response to an unexpected failure.\n\n### Redeploy to a secondary region\n\nIf you run applications and databases in a single, primary region with no replication, your recovery strategy might be to redeploy to another region. This solution is affordable but most appropriate for non-critical applications that can tolerate longer recovery times.\n\nIf you choose this strategy, automate the redeployment process as much as possible and include redeployment scenarios in your disaster response testing.\n\nTo automate your redeployment process, consider using [Azure Site Recovery](/azure/site-recovery/).",
              "priority": "medium",
              "title": "High availability considerations",
              "id": "e195f544-3bee-4e9d-9647-18b0b4db46f7",
              "heading": "Deploy to multiple regions and instances"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Hotfixes",
              "id": "65e910a7-e12e-41ee-825d-bcd547a1c6d3",
              "heading": "Hotfixes"
            },
            {
              "answer_tooltip": "",
              "output": "Use VSTS extensions to create documentation from source code. Utilize VSTS history and auditing for a consolidated view of changes to code and infrastructure.",
              "priority": "medium",
              "title": "Logging",
              "id": "13048dff-6c81-4829-a831-e31f61da7585",
              "heading": "Logging"
            },
            {
              "answer_tooltip": "",
              "output": "Use VSTS access management to grant or restrict access to resources and features you want to control. Use Azure Automation Change Tracking.",
              "priority": "medium",
              "title": "Manage infrastructure after deployment",
              "id": "30fd7ac2-b7c6-498b-a53b-969abaee8cbb",
              "heading": "Manage infrastructure after deployment"
            }
          ],
          "title": "What considerations are you making around the deployment of your infrastructure?",
          "type": "checkbox",
          "id": "08d06243-5d2b-4b33-b7b9-8892673cc7c4",
          "heading": "deployment",
          "explanation": "As you provision and update Azure resources, application code, and configuration settings, a repeatable and predictable process will help you avoid errors and downtime. We recommend automated processes for deployment that you can run on demand and rerun if something fails. After your deployment processes are running smoothly, process documentation can keep them that way.",
          "context": "Describes deployment considerations to make to enable DevOps in your organization."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Use VSTS load testing with cloud scale and mimic real- life, peak-usage scenario.",
              "priority": "medium",
              "title": "Production like environment for Dev/Test",
              "id": "05d95078-f555-430c-9d2e-beac7a16a555",
              "heading": "Production like environment for Dev/Test"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure Monitor, Azure Advisor, Azure Service Health, Activity Log, Azure Application Insights, Log Analytics, ExpressRoute monitor, Service Map, availability tests, and general monitoring Azure applications and resources.",
              "priority": "medium",
              "title": "Application instrumentation for insight",
              "id": "39f3796e-76d5-43dd-923b-ac832189ecae",
              "heading": "Application instrumentation for insight"
            },
            {
              "answer_tooltip": "",
              "output": "Track technical debt using SonarQube with Visual Studio Team Services (VSTS).",
              "priority": "medium",
              "title": "Technical debt",
              "id": "4677e163-cb3e-4c8a-a3c1-91bb97861b05",
              "heading": "Technical debt"
            },
            {
              "answer_tooltip": "",
              "output": "Use feature toggles and canary releases. Utilize App Service deployment slots to safely deploy applications.\n\nContinuous Integration (CI) is a development practice that requires developers to integrate code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect problems early. The key details to note are that you need to run code integration multiple times a day, every day, and you need to run the automated verification of the integration. What’s the motivation for this? Well, in the development process, the earlier we surface errors, the better. And one source of frequently occurring errors is the code integration step.\n\nEmploy VSTS continuous integration to build, test, and deploy applications quickly.\n\nUse VSTS continuous delivery to deploy tested code automatically.",
              "priority": "medium",
              "title": "Continous deployment / continous integration",
              "id": "e0dd6a56-7736-4fe8-9eaf-ff9d1e6443b0",
              "heading": "Continous deployment / continous integration"
            },
            {
              "answer_tooltip": "",
              "output": "Using feature flags is a technique that will help you integrate code into a shared repository at least once a day and ship it, even if you haven't finished the feature yet. You'll be able to deploy at any time but defer the decision to release for another day.",
              "priority": "medium",
              "title": "Feature flags",
              "id": "0810967f-46c8-4c7e-b4ca-6e5fab943173",
              "heading": "Feature flags"
            }
          ],
          "title": "How is development done on this workload?",
          "type": "checkbox",
          "id": "fde7746e-5c1a-4183-94cd-47e659a45813",
          "heading": "development",
          "explanation": "Describes considerations to make when enabling DevOps for your workload."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Review monitoring and diagnostics guidance. Review monitoring Azure applications and resources guidance.\n\nTrain multiple people on Azure Monitor. Send alerts and notifications to multiple recipients.\n\nAutomation around alerts is critical due to the highly collaborative nature of DevOps and the inherent speed needed for effective incident management. Earlier this year, a report from DevOps.com came out stating that 80% of IT teams are alerted to critical incidents via email. Email is an effective form of communication, but it shouldn’t be the most common notification method for a critical issue.\n\nCreate, view, and manage alerts using Azure Monitor. Use Log Analytics Alerts based on conditions in Log Analytics data.\n\nUse Azure Monitor, Azure Advisor, Azure Service Health, Activity Log, Azure Application Insights, Log Analytics, ExpressRoute monitor, Service Map, availability tests, and general monitoring Azure applications and resources.",
              "priority": "medium",
              "title": "Implemented alerting and monitoring",
              "id": "b320ea2e-2f4d-4915-8f7c-561fd4316666",
              "heading": "Implemented alerting and monitoring"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure alerts to get proactive notifications. Employ action groups to notify recipients to respond to alerts.",
              "priority": "medium",
              "title": "Notification",
              "id": "b52bb24b-2b98-4e19-a1dd-6ba48c028c4f",
              "heading": "Notification"
            },
            {
              "answer_tooltip": "",
              "output": "This enables you to trace communications between applications distributed across systems/containers to identify errors and exceptions from your applications and resolve latency quickly.\n\nUse Azure Log Analytics for viewing data for a particular application. Utilize Service Map and Application Map for logs across multiple components.",
              "priority": "medium",
              "title": "Event correlation",
              "id": "6e63633f-6903-45bc-a836-16d6b365aa54",
              "heading": "Event correlation"
            },
            {
              "answer_tooltip": "",
              "output": "Employ usage analysis with Application Insights.",
              "priority": "medium",
              "title": "Remote API/Database call statistics",
              "id": "05aa07ce-3f28-49a6-8557-e51d4bea0cea",
              "heading": "Remote API/Database call statistics"
            },
            {
              "answer_tooltip": "",
              "output": "Review retry service-specific guidance.",
              "priority": "medium",
              "title": "Retries",
              "id": "75c9201f-ff2d-4bb9-9dc9-7989a735f7cf",
              "heading": "Retries"
            },
            {
              "answer_tooltip": "",
              "output": "Use action groups to ensure people receive alerts.",
              "priority": "medium",
              "title": "System alerts",
              "id": "dc9df9d4-98e5-49f9-8e82-3f2aa5499722",
              "heading": "System alerts"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "On-call",
              "id": "a74dcdf2-c987-4821-a1f8-7bb108351560",
              "heading": "On-call"
            },
            {
              "answer_tooltip": "",
              "output": "Review Azure subscription limits.",
              "priority": "medium",
              "title": "Service limits",
              "id": "1088d61c-a4b5-4446-b230-860594b868c1",
              "heading": "Service limits"
            },
            {
              "answer_tooltip": "",
              "output": "Understand Azure support plans. Refer to Azure support FAQs. Familiarize your team with Azure support.",
              "priority": "medium",
              "title": "Support",
              "id": "cad7d05f-4caa-4b35-bae4-8d50227e5313",
              "heading": "Support"
            },
            {
              "answer_tooltip": "",
              "output": "A logging tool should collect logs from each system component, application-side or server-side, and provide access to them in one centralized location. Not all logging platforms are capable of maintaining speed as the amount of logs they process grows. Therefore, you should keep a critical eye on the process when trying out different solutions.",
              "priority": "medium",
              "title": "Logging",
              "id": "8022d5d2-3820-4de7-83a4-8cd9876dcb1c",
              "heading": "Logging"
            },
            {
              "answer_tooltip": "",
              "output": "Dev ops metrics are critical for measuring production and project management\n\nUse Azure Monitor for collection of metrics, activity logs, and diagnostic logs.",
              "priority": "medium",
              "title": "Metrics",
              "id": "061bb950-af5d-433c-9eed-b7d0545e38a9",
              "heading": "Metrics"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "KPIs",
              "id": "d7c36cea-973d-461c-b9ee-16c91f2d179f",
              "heading": "KPIs"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure dashboards to combine data into a single pane and share it with multiple stakeholders.\n\nExport Log Analytics data to Power BI to create additional visualizations.",
              "priority": "medium",
              "title": "Reporting",
              "id": "9fd68bd0-8fa4-4c53-834b-83a5eea9249e",
              "heading": "Reporting"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure VM expire and certificate monitoring.",
              "priority": "medium",
              "title": "Expiration",
              "id": "218cdca1-7c45-4c6a-8efd-d830f49aaf2c",
              "heading": "Expiration"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure Service Health to identify issues with services affecting application and plan for scheduled maintenance.",
              "priority": "medium",
              "title": "Underlying services",
              "id": "24316347-8a0a-48b8-8c01-e66e74adbb53",
              "heading": "Underlying services"
            },
            {
              "answer_tooltip": "",
              "output": "Utilize Activity Log for detecting configuration changes, health incidents, better utilization, and autoscale operations.",
              "priority": "medium",
              "title": "Configuration changes",
              "id": "53a33ddb-4890-4d98-8d9f-f3b8d1e04bc8",
              "heading": "Configuration changes"
            }
          ],
          "title": "How are you monitoring your deployments?",
          "type": "checkbox",
          "id": "7ee0b0bf-3337-4723-8108-da63ab2d7a13",
          "explanation": "Describes how to monitor your workload to ensure your DevOps infrastructure is working as intended.",
          "heading": "monitoring",
          "context": ""
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Human intervention",
              "id": "d959fb93-33ff-41b2-b02e-0fd6ab6dc941",
              "heading": "Human intervention"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Build times",
              "id": "07b17873-ce9a-445b-a969-2e865b7cbed8",
              "heading": "Build times"
            }
          ],
          "title": "How have your optimized your workload to ensure get the best performance during your deployments?",
          "type": "checkbox",
          "id": "e99ed9f3-e395-4f93-8d42-01e4353ed9a3",
          "heading": "Performance",
          "explanation": "Describes considerations to make in regards to your deployment infrastructure."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Source code policies and licensing",
              "id": "59eab58c-387f-4cb4-82c7-76b8e3d94a4b",
              "heading": "Source code policies and licensing"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Repository creation and usage policies",
              "id": "a564cd7d-62bf-4a86-9996-bea9345564de",
              "heading": "Repository creation and usage policies"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Branching strategies",
              "id": "6364412b-f2a2-4c96-9917-81531e076708",
              "heading": "Branching strategies"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Monitor for bugs in dependecies",
              "id": "4267ba31-567a-443c-b904-b1dee3139826",
              "heading": "Monitor for bugs in dependecies"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Naming conventions",
              "id": "770b7a70-03c1-4c28-959c-397817beef7b",
              "heading": "Naming conventions"
            },
            {
              "answer_tooltip": "",
              "output": "Adopt VSTS Agile as a single source of truth for all stakeholders to avoid mismatched expectations and to give an accurate picture of the current status.",
              "priority": "medium",
              "title": "Roadmap",
              "id": "5358b112-0cf4-4637-908f-46a7347f1ffc",
              "heading": "Roadmap"
            },
            {
              "answer_tooltip": "",
              "output": "Use VSTS dashboards and VSTS Power BI integration for data-driven reporting and improvement.",
              "priority": "medium",
              "title": "Continous improvement",
              "id": "3a0939d8-499e-4f33-becb-a97bd70e40f8",
              "heading": "Continous improvement"
            },
            {
              "answer_tooltip": "",
              "output": "Track ideas to implementation using VSTS work-item tracking. Implement DevOps using VSTS.\n\nUse Azure Operations Management Suite (OMS) for process automation.\n\nEmploy third-party extensions such as Remedy OnDemand to notify on-call responders for critical VSTS work items.\n\nPractice agile planning and portfolio management with VSTS for a full view of the work escalation and decomposition of tasks.",
              "priority": "medium",
              "title": "Documented procedures",
              "id": "036b5b16-88c6-44ea-942d-3b19c5fe2761",
              "heading": "Documented procedures"
            },
            {
              "answer_tooltip": "",
              "output": "Use VSTS Wiki to distribute information, share knowledge, and collaborate.",
              "priority": "medium",
              "title": "Knowledge sharing",
              "id": "959bcb28-91f6-4830-a01b-81b3eb9a85df",
              "heading": "Knowledge sharing"
            }
          ],
          "title": "What processes and procedures have you adopted to optimize your workload for DevOps?",
          "type": "checkbox",
          "id": "87411133-27c1-46ba-913a-272a0a4ddd89",
          "explanation": "Describes how to optimize your processes and procedures to enable DevOps in your organization.",
          "heading": "process"
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Experimentation on a system to uncover its weaknesses. Breaking things on purpose is preferable to being surprised when things break.",
              "priority": "medium",
              "title": "DR and fault injection",
              "id": "1408011b-b509-4d27-bc1a-5c7a118228c5",
              "heading": "DR and fault injection"
            },
            {
              "answer_tooltip": "",
              "output": "Use App Service deployment slots for testing in production.",
              "priority": "medium",
              "title": "Testing in production",
              "id": "148e18a3-8b7e-4654-9ce1-45d287f6735c",
              "heading": "Testing in production"
            },
            {
              "answer_tooltip": "",
              "output": "Review guidance on disaster recovery for Azure applications. Use Azure Site Recovery drills.",
              "priority": "medium",
              "title": "BCP (Business Continuity Process)/DR (Disaster Recovery) drills",
              "id": "f2340a3b-2046-47d2-8a35-f58045fb94b3",
              "heading": "BCP (Business Continuity Process)/DR (Disaster Recovery) drills"
            }
          ],
          "title": "How are you testing your DevOps infrastructure?",
          "type": "checkbox",
          "id": "87d56f90-e505-4e42-9207-4f98334a9f8c",
          "heading": "Testing",
          "explanation": "Describes testing considerations to make when in regards to DevOps when desinging your workload."
        },
        {
          "choices": [],
          "title": "How does your business prioritize devops?",
          "type": "checkbox",
          "id": "033b7558-a87a-418a-8b4d-1ca75667da08",
          "heading": "tradeoffs",
          "explanation": "Describes some of the trade-offs you may decide to make when optimizing a workload for DevOps."
        }
      ],
      "name": "DevOps",
      "id": "7d04c8cd-3aff-4120-880b-db07a3b91feb"
    },
    {
      "questions": [
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Use WAF in front of a web app. Review Azure DDoS Protection guidance. Utilize Azure Key Vault to manage secrets, such as connectionstring.",
              "priority": "medium",
              "title": "Distributed denial-of-service protection",
              "id": "234e0781-2cb6-43c2-a21e-b8e6270d3e72",
              "heading": "Distributed denial-of-service protection"
            },
            {
              "answer_tooltip": "",
              "output": "Use RBAC with Azure AD for Azure subscription. Utilize Azure Security Center for threat detection and protection.",
              "priority": "medium",
              "title": "Role-based access controls (RBAC)",
              "id": "6bb71142-3759-443a-8cc6-9b78cb88e175",
              "heading": "Role-based access controls (RBAC)"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Secret management",
              "id": "63ed4f9d-7227-4c4d-96eb-ce9bedc9d6f9",
              "heading": "Secret management"
            },
            {
              "answer_tooltip": "",
              "output": "This includes passwords / API keys / etc.",
              "priority": "medium",
              "title": "Keys in source code",
              "id": "0846d834-ff2c-4c6a-91f7-5f1a9920d59b",
              "heading": "Keys in source code"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Infosec team",
              "id": "d950321f-21b9-4b8d-b6ed-18dc1cf43912",
              "heading": "Infosec team"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Seperation of duties",
              "id": "e566ca8d-b98f-4c81-93df-1985f2082814",
              "heading": "Seperation of duties"
            }
          ],
          "title": "What design considerations did you make in your workload in regards to security?",
          "type": "checkbox",
          "id": "579a0cc5-5ac6-411e-a501-780a99e98b55",
          "heading": "app-design",
          "explanation": "Applications and the data associated with them ultimately act as the primary\nstore of business value on a cloud platform. While the platform components like\nidentity and storage are critical elements of the security environment,\napplications play an outsize role in risks to the business because:\n\n-   **Business Processes** are encapsulated and executed by applications and\n    services need to be available and provided with high integrity\n\n-   **Business Data** is stored and processed by application workloads and\n    requires high assurances of confidentiality, integrity, and availability.\n\nThis section focuses on applications written by your organization or by others\non behalf of your organization vs. SaaS or commercially available applications\ninstalled on IaaS VMs.\n\n![Diagram of Application Models](images/appmodels.png)\n\nModern cloud platforms like Azure can host both legacy and modern generations of\napplications\n\n-   **Legacy** applications are hosted on Infrastructure as a Service (IaaS)\n    virtual machines that typically include all dependencies including OS,\n    middleware, and other components.\n\n-   **Modern** Platform as a Service (PaaS) applications don’t require the\n    application owner to manage and secure the underlying server operating\n    systems (OSes) and are sometimes fully “Serverless” and built primarily\n    using functions as a service.\n\n    **Notes:** Popular forms of modern applications are application code hosted\n    on Azure App Services and containerized applications (though containers can\n    also be hosted on IaaS VMs or on-premises as well).\n\n-   **Hybrid** – While hybrid applications can take many forms, the most common\n    is an “IaaS plus” state where legacy applications are transitioning to a\n    modern architecture with modern services replacing legacy components or\n    being added a legacy application.\n\nSecuring an application requires security assurances for three different\ncomponent types:\n\n-   **Application Code** – This is the logic that defines the custom application\n    that you write. The security of this code is the application owners’\n    responsibility in all generations of application architecture including any\n    open-source snippets or components included in the code. Securing the code\n    requires identifying and mitigating risks from the design and implementation\n    of the application as well as assessing supply chain risk of included\n    components. Note that the evolution of applications into [microservices\n    architectures](https://docs.microsoft.com/azure/service-fabric/service-fabric-overview-microservices)\n    will break various aspects of application code into smaller services vs. a\n    single monolithic codebase.\n\n-   **Application Services** – These are the various standardized components\n    that the application uses such as databases, identity providers, event hubs,\n    IoT device management, and so on. For cloud services this is a shared\n    responsibility:\n\n    -   **Cloud Provider -** The security of the underlying service is the\n        responsibility of the cloud provider\n\n    -   **Application Owner** - The application owner is responsible for\n        security implications of the configuration and operation of the service\n        instance(s) used by the application including any data stored and\n        processed on the service.\n\n-   **Application Hosting Platform** – This is the computing environment where\n    the application actually executes and runs. In an enterprise with\n    applications hosted on premises, in Azure and in third-party clouds like\n    Amazon Web Services (AWS), this could take many forms with significant\n    variations on who is responsible for security:\n\n    -   **Legacy Applications** typically require a full operating system (and\n        any middleware) hosted on physical or virtualized hardware. The virtual\n        hardware can be hosted on premises or on Infrastructure as a Service\n        (IaaS) VMs. This operating system and installed middleware/other\n        components are operated and secured by the application owner or their\n        infrastructure team(s).  \n        The responsibility for the physical hardware and OS virtualization\n        components (virtualization hosts, operating systems, and management\n        services) varies:\n\n        -   **On premises** - The application owner or their organization is\n            responsible for maintenance and security.\n\n        -   **IaaS** – The cloud provider is responsible for maintenance and\n            security of the underlying infrastructure and the application\n            owner’s organization is responsible for the VM configuration,\n            operating system, and any components installed on it.\n\n    -   **Modern Applications** are hosted on Platform as a Service (PaaS)\n        environments such as an Azure application service. In most application\n        service types, the underlying operating system is abstracted from the\n        application owner and secured by the cloud provider. Application owners\n        are responsible for the security of the application service\n        configurations that are provided to them.\n\n    -   **Containers** are an application packaging mechanism in which\n        applications are abstracted from the environment in which they run.\n        These containerized applications fit into either the legacy or modern\n        models above depending on whether they are run on a container service by\n        the cloud provider (Modern Applications) or on a server managed by the\n        organization (on premises or in IaaS). See the [container security\n        section](#follow-best-practices-for-container-security) below for more details.",
          "context": "Describes how to design your application while focusing on security."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Conducting internal security audits help companies keep their compliance programs up to date and aimed in the right direction. An effective security risk assessment can prevent breaches, reduce the impact of realized breaches, and keep your company's name from appearing in the spotlight for all the wrong reasons.",
              "priority": "medium",
              "title": "Auditing authorization",
              "id": "c0ffe50a-3932-488c-9a31-af921afce8e7",
              "heading": "Auditing authorization"
            },
            {
              "answer_tooltip": "",
              "output": "Authorization is a security mechanism used to determine user/client privileges or access levels related to system resources, including computer programs, files, services, data and application features. Authorization is normally preceded by authentication for user identity verification.",
              "priority": "medium",
              "title": "Common authorization patterns",
              "id": "69182b0a-889d-471d-b706-8f3c6696744e",
              "heading": "Common authorization patterns"
            },
            {
              "answer_tooltip": "",
              "output": "Use role-based access control (RBAC) to grant access based on Azure Active Directory identities and groups.\n\nRole-based: If you want to authorize based on users. A user can either be an administrator, creator, or reader. If you want to authorize an action based on a particular resource, consider resource based. For example, the app can check whether a user is the owner for of a resource.",
              "priority": "medium",
              "title": "Role-based access control(RBAC)",
              "id": "cddcb8c1-b117-406f-9b0e-435a84855530",
              "heading": "Role-based access control(RBAC)"
            }
          ],
          "title": "What considerations have you made when authorizing users to your workload?",
          "type": "checkbox",
          "id": "94a2e70e-0fe6-457c-9bae-43885febf6a1",
          "explanation": "Describes considerations to make when authorizing traffic into your workload.",
          "heading": "Authorization"
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Use Azure Security Center for security management and advanced threat protection across hybrid cloud workloads. Review Prevent, detect, and respond to threats guidance.",
              "priority": "medium",
              "title": "Have you defined security policies according to your company’s security needs, and tailored it to the type of applications or sensitivity of the data",
              "id": "96ada3c0-19c1-4204-bb31-a989813d838a",
              "heading": "Have you defined security policies according to your company’s security needs, and tailored it to the type of applications or sensitivity of the data"
            },
            {
              "answer_tooltip": "",
              "output": "Susceptibility to users that may abuse the service by creating more resources than they need.\n\nAction:\nControl resource creation using Resource Manager.",
              "priority": "medium",
              "title": "Resource creation management",
              "id": "e227632b-6837-4fab-b4ef-405fd482e8a5",
              "heading": "Resource creation management"
            },
            {
              "answer_tooltip": "",
              "output": "Data residency is a particular issue that businesses face as they move more and more information into the cloud. The cloud provides flexibility for data storage, file sharing and the use of different SaaS applications, but it presents challenges when it comes to establishing and maintaining data control. It is having clear control over their data that businesses need to be able to prove in order to meet internal and customer requirements, standards and legal obligations, such as the General Data Protection Regulation (GDPR).",
              "priority": "medium",
              "title": "Data residency",
              "id": "6bb1fb76-3fd7-42b3-b848-6571a620d7c2",
              "heading": "Data residency"
            },
            {
              "answer_tooltip": "",
              "output": "Standards provide people and organizations with a basis for mutual understanding, and are used as tools to facilitate communication, measurement, commerce and manufacturing.",
              "priority": "medium",
              "title": "Standards",
              "id": "250062ac-40b6-4bb5-bc0c-3e912261ac29",
              "heading": "Standards"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to provide a view to people with different responsibilities (financial controller, executives, project owners) in your organization.\n\nAction:\nUse Cost Management metrics with dashboards to view key cost metrics and business-trend highlights to help make important business decisions.",
              "priority": "medium",
              "title": "Usage and spending",
              "id": "510bcf15-1333-4dc6-a376-178643bf3555",
              "heading": "Usage and spending"
            },
            {
              "answer_tooltip": "",
              "output": "Risk of resources not staying compliant with corporate standards and service-level agreements (SLAs).\n\nAction:\nUse Custom Azure Policy to enforce different rules and effects over resources to ensure that resources stay compliant with corporate standards and SLAs.",
              "priority": "medium",
              "title": "Custom policies",
              "id": "9356b8e9-27b4-4c5e-a679-add6e7b7d7fa",
              "heading": "Custom policies"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to provide RBAC assignments over multiple subscriptions.\n\nAction:\nEmploy Azure Policy scoping to apply governance conditions to multiple subscriptions (management groups) all at once.",
              "priority": "medium",
              "title": "Policy to resources at scale",
              "id": "545ca865-29d8-48c6-ba49-323f66fd14b2",
              "heading": "Policy to resources at scale"
            },
            {
              "answer_tooltip": "",
              "output": "Risk of resources getting created in wrong location, enforcing common and consistent tag usage, or auditing existing resources for appropriate configurations and setting.\n\nAction:\nUse Azure Policy compliance monitoring to understand the compliance state of environment.",
              "priority": "medium",
              "title": "Audit policy compliance",
              "id": "690207f9-a387-4a00-90f5-52219f2d2c79",
              "heading": "Audit policy compliance"
            }
          ],
          "title": "How does compliance and governance impact your workload?",
          "type": "checkbox",
          "id": "0e4ff292-1c72-4d6c-9667-ba9f6860048a",
          "heading": "compliance"
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "The process of data classification allows organizations to categorize their stored data by sensitivity and business impact in order to determine the risks associated with the data.",
              "priority": "medium",
              "title": "You have a data classification strategy",
              "id": "0c440a61-f4c1-401b-b88c-330873757c5c",
              "heading": "Data classification strategy"
            },
            {
              "answer_tooltip": "",
              "output": "After the process is completed, organizations can manage their data in ways that reflect its value to them instead of treating all data the same way.",
              "priority": "medium",
              "title": "Ensure appropriate action after classification",
              "id": "b4332482-c29a-40a6-ac88-eda11ee68505",
              "heading": "Ensure appropriate action after classification"
            },
            {
              "answer_tooltip": "",
              "output": "You'll need to pin down exactly what kind of information was lost in the data breach",
              "priority": "medium",
              "title": "What to do if there is a breach",
              "id": "2a253dad-9285-44a8-a77a-c7d7110c1a35",
              "heading": "What to do if there is a breach"
            },
            {
              "answer_tooltip": "",
              "output": "Where customer data actually ends up has long been a concern. Doubts as to the answer are something often cited by opponents of the trend towards cloud services.",
              "priority": "medium",
              "title": "Where's the data",
              "id": "32c24951-c978-430b-8b81-542555a29fed",
              "heading": "Where's the data"
            }
          ],
          "title": "What is your data classification strategy?",
          "type": "checkbox",
          "id": "427fe136-0b85-4719-a62d-585b0b4090fe",
          "heading": "data classification",
          "explanation": "Describes how to classify the data that is being used in a given workload"
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Cloud service providers make multiple methods of access control over storage\nresources available. Examples include shared keys, shared signatures, anonymous\naccess, and identity provider-based methods.\n\nIdentify provider methods of authentication and authorization are the least\nliable to compromise and enable more fine-grained role-based access controls\nover storage resources.\n\nWe recommend that you use an identity-based option for storage access control.\n\nAn example of this is [Azure Active Directory Authentication to Azure blob and queue services](https://docs.microsoft.com/rest/api/storageservices/authenticate-with-azure-active-directory).",
              "priority": "medium",
              "title": "You use identity based storage access controls",
              "id": "8b505a34-2978-44bc-9feb-4baf39533635",
              "heading": "Use identity based storage access controls"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to grant access to databases based on the originating IP address of each request.\n\nAction:\nUse firewall rules to restrict database access. Utilize Virtual Network service endpoints to secure databases to only your virtual networks.",
              "priority": "medium",
              "title": "Database access",
              "id": "3d7dd02d-9fc6-4db0-8c04-72b060705e23",
              "heading": "Database access"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to prove a user’s identity.\n\nAction:\nUse database authentication. Employ Azure Managed Database Services for built-in security, automatic monitoring, threat detection, automatic tuning, and turnkey global distribution.",
              "priority": "medium",
              "title": "Database authentication",
              "id": "7f84b699-dc51-44d7-b5c2-f7a3f25f35d3",
              "heading": "Database authentication"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to maintain regulatory compliance, understand database activity, and gain insight into discrepancies and anomalies.\n\nAction:\nEnable database auditing. Use Azure Managed Database Services for built-in security, automatic monitoring, threat detection, automatic tuning, and turnkey global distribution.",
              "priority": "medium",
              "title": "Database auditing",
              "id": "ddb71284-9a19-406b-8ab4-3473e4d71357",
              "heading": "Database auditing"
            }
          ],
          "title": "How are you securely managing your data for this workload?",
          "type": "checkbox",
          "id": "c5c9047c-8a46-4e2b-a961-e7f22d10cfc6",
          "heading": "data management",
          "explanation": "Granting access to Azure storage is possible through Azure Active Directory\n(Azure AD) as well as key based authentication mechanisms (Symmetric Shared Key\nAuthentication, or Shared Access Signature (SAS))\n\nStorage in Azure includes a number of native security design attributes\n\n-   All data is encrypted by the service\n\n-   Data in the storage system cannot be read by a tenant if it has not been\n    written by that tenant (to mitigate the risk of cross tenant data leakage)\n\n-   Data will remain only in the region you choose\n\n-   The system maintains three synchronous copies of data in the region you choose.\n\n-   Detailed activity logging is available on an opt-in basis.\n\nAdditional security features can be configured such as a storage firewall to\nprovide an additional layer of access control as well as storage threat\nprotection to detect anomalous access and activities.",
          "context": "Describes security considerations to take into account for the management of the data in your workload."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "All public cloud service providers enable encryption that is done automatically\nusing provider-managed keys on their platform. In many cases, this is done for\nthe customer and no user interaction is required. In other cases, the provider\nmakes this an option that the customer can choose to use or not to use.\n\nThere is almost no overhead in enabling this type of encryption as it’s managed\nby the cloud service provider.\n\nWe recommend that for each service that supports service provider encryption\nthat you enable that option.\n\nAn example of service-specific service provider encryption is [Azure Storage Service encryption](https://docs.microsoft.com/azure/storage/common/storage-service-encryption).",
              "priority": "medium",
              "title": "You have enabled platform encryption services",
              "id": "3b071bca-019b-44e6-a9d4-0ad6b6efd682",
              "heading": "Enable platform encryption services"
            },
            {
              "answer_tooltip": "",
              "output": "Protecting your keys is essential to protecting your data in the cloud.",
              "priority": "medium",
              "title": "Key management strategy",
              "id": "5ecb1483-b8a4-49ae-8db6-320ca944d571",
              "heading": "Key management strategy"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Encryption policy",
              "id": "8f600cf8-2b7f-453d-b4d9-26a427ec74b1",
              "heading": "Encryption policy"
            },
            {
              "answer_tooltip": "",
              "output": "Protecting data at rest is required to maintain confidentiality, integrity, and\navailability assurances across all workloads. Storage in a cloud service like\nAzure is [architected and\nimplemented](https://azure.microsoft.com/blog/sosp-paper-windows-azure-storage-a-highly-available-cloud-storage-service-with-strong-consistency/)\nquite differently than on premises solutions to enable massive scaling, modern\naccess through REST APIs, and isolation between tenants.\n\nEncryption at rest provides data protection for stored data (at rest). Attacks against data at-rest include attempts to obtain physical access to the hardware on which the data is stored, and then compromise the contained data. In such an attack, a server’s hard drive may have been mishandled during maintenance allowing an attacker to remove the hard drive. Later the attacker would put the hard drive into a computer under their control to attempt to access the data.",
              "priority": "medium",
              "title": "Data at rest",
              "id": "f814cf06-a764-4af1-a84c-0cb920b933f5",
              "heading": "Data at rest"
            },
            {
              "answer_tooltip": "",
              "output": "If data moving over a network is not encrypted, there’s a chance that it can be captured and stolen by unauthorized users. When you're dealing with database services, make sure that data is encrypted between the database client and server. Also make sure that data is encrypted between database servers that communicate with each other and with middle-tier applications.",
              "priority": "medium",
              "title": "Data in transit",
              "id": "a1583f13-8bec-45fe-a42e-d0d481ec85f6",
              "heading": "Data in transit"
            },
            {
              "answer_tooltip": "",
              "output": "Organizations have to think about what type of threats they want to protect against, and that will determine the type of technology used.",
              "priority": "medium",
              "title": "Appropriate encryption algorithms",
              "id": "ce3d8eed-fddd-4e82-bd4b-5665d7ddf68d",
              "heading": "Appropriate encryption algorithms"
            },
            {
              "answer_tooltip": "",
              "output": "Risk of data leakage and lack of business insights monitor for abuse and prevent malicious access to files.\n\nAction:\nUse Azure Managed Disks for persistent and secure disk storage for Azure virtual machines. Enforce file-level data encryption.",
              "priority": "medium",
              "title": "File level encryption",
              "id": "e8a807d0-59d9-46ba-9884-e51355196d25",
              "heading": "File level encryption"
            },
            {
              "answer_tooltip": "Risk of data integrity issues, such as malicious or rogue users stealing data and compromised accounts gaining unauthorized access to data.",
              "output": "Virtual machines use virtual disk files as virtual storage volumes and exist in\na cloud service provider’s blob storage system. These files can be moved from\non-premises to cloud systems, from cloud systems to on-premises, or between\ncloud systems. Due to the mobility of these files, you need to make sure the\nfiles and their contents are not accessible to unauthorized users.\n\nAuthentication-based access controls should be in place to prevent potential\nattackers from downloading the files to their own systems. In the event of a\nflaw in the authentication and authorization system or its configuration, you\nwant to have a backup mechanism to secure the virtual disk files.\n\nYou can encrypt the virtual disk files to help prevent attackers from gaining\naccess to the contents of the disk files in the event that an attacker is able\nto download the files. When attackers attempt to mount an encrypted disk file,\nthey will not be able to because of the encryption.\n\nWe recommend that you enable virtual disk encryption.\n\nAn example of virtual disk encryption is [Azure Disk\nEncryption](https://docs.microsoft.com/azure/security/azure-security-disk-encryption-overview).",
              "priority": "medium",
              "title": "You encrypt your virtual disk files.",
              "id": "e0dc5c60-d25f-4199-b991-3d241f99ea2b",
              "heading": "Encrypt virtual disk files"
            }
          ],
          "title": "How are you managing encryption for this workload?",
          "type": "checkbox",
          "id": "bd2e10fd-94ee-47c6-bfe9-57e50c76421b",
          "heading": "encryption",
          "explanation": "Encryption is a powerful tool for security, but it's critical to understand its\nlimits in protecting data. Much like a safe, encryption restricts access to only\nthose with possession of a small item (a mathematical key). While it's easier to\nprotect possession of keys than larger datasets, it is imperative that you\nprovide the appropriate protections for the keys. Protecting cryptographic keys\nis not a natural intuitive human process (especially because electronic data\nlike keys can be copied perfectly without a forensic evidence trail), so it is\noften overlooked or implemented poorly.\n\nWhile encryption is available in many layers in Azure (and often on by default),\nwe have identified the layers that are most important to implement (high\npotential for data to move to another storage medium) and are easiest to\nimplement (near zero overhead).",
          "context": "Describes considerations to make when encrypting your workload"
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Safeguarding confidential source, connection strings, certificates, secrets etc. required in the deployment pipeline ensures security of end products.",
              "priority": "medium",
              "title": "Secrets",
              "id": "79de83f8-b5cd-4dd1-b39c-4b60fb26f23e",
              "heading": "Secrets"
            },
            {
              "answer_tooltip": "",
              "output": "With DevOps, organizations aim to improve operational processes in terms of security, reliability and efficiency. In DevOps environments, since software components are often developed in parallel but separately, you’ll require certain network configurations.",
              "priority": "medium",
              "title": "Isolation",
              "id": "de1c1879-8308-484a-aa73-6743b26bce09",
              "heading": "Isolation"
            },
            {
              "answer_tooltip": "",
              "output": "Many consider identity to be the primary perimeter for security. This is a shift from the traditional focus on network security. Network perimeters keep getting more porous, and that perimeter defense can’t be as effective as it was before the explosion of BYOD devices and cloud applications. Azure Active Directory (Azure AD) is the Azure solution for identity and access management. Azure AD is a multitenant, cloud-based directory and identity management service from Microsoft. It combines core directory services, application access management, and identity protection into a single solution.\n\nManage accounts from one single location, regardless of where an account is created. Enable single sign-on. Otherwise, it becomes an administrative problem not only for IT but also for users who have to remember multiple passwords. Turn on conditional access because focusing on who can access a resource is not sufficient anymore. Enable password management and use appropriate security policies to prevent abuse.",
              "priority": "medium",
              "title": "Identity strategy",
              "id": "cd05c86a-d4a1-41b9-a146-b555a82d7661",
              "heading": "Identity strategy"
            },
            {
              "answer_tooltip": "",
              "output": "Don't take this decision lightly, for the following reasons: It's the first decision for an organization that wants to move to the cloud. The authentication method is a critical component of an organization’s presence in the cloud. It controls access to all cloud data and resources. It's the foundation of all the other advanced security and user experience features in Azure AD. The authentication method is difficult to change after it's implemented.",
              "priority": "medium",
              "title": "System to manage identity",
              "id": "699e0bec-18d8-4f21-ab2a-fd1871148244",
              "heading": "System to manage identity"
            },
            {
              "answer_tooltip": "",
              "output": "Limiting the lifespan of a credential reduces the risk from and effectiveness of password-based attacks and exploits, by condensing the window of time during which a stolen credential is valid.\n\nPassword rotation, etc.",
              "priority": "medium",
              "title": "Credential policies in place",
              "id": "a982ae49-0817-4115-b0db-800cd00e12b7",
              "heading": "Credential policies in place"
            },
            {
              "answer_tooltip": "",
              "output": "Exposure to scenarios where users have multiple passwords, increasing the likelihood of users reusing passwords or using weak passwords.\n\nAction:\nEnable Single Sign-On (SSO).",
              "priority": "medium",
              "title": "Enabled Single Sign-on (SSO)",
              "id": "d3d7496f-438b-4cd8-9673-623271f49f08",
              "heading": "Enabled Single Sign-on (SSO)"
            },
            {
              "answer_tooltip": "",
              "output": "Susceptibility to a higher call volume to the service desk due to password issues.\n\nAction:\nDeploy password management.",
              "priority": "medium",
              "title": "Self-service password reset & password management",
              "id": "95b6e2ca-d1e3-4f6d-aef0-75721f239743",
              "heading": "Self-service password reset & password management"
            },
            {
              "answer_tooltip": "",
              "output": "Risk of not complying with industry standards, such as PCI DSS version 3.2 and credential theft type of attack, such as Pass-the- Hash (PtH).\n\nAction:\nEnforce MFA for users.",
              "priority": "medium",
              "title": "Multi-factor authentication (MFA)",
              "id": "8b457523-89e1-4b27-93ae-2c438b96f358",
              "heading": "Multi-factor authentication (MFA)"
            },
            {
              "answer_tooltip": "",
              "output": "Risk of a credential-theft type of attack, such as weak authentication and session management described in Open Web Application Security Project (OWASP) Top 10.\n\nAction:\nGuide developers to leverage identity capabilities for SaaS apps.",
              "priority": "medium",
              "title": "enforce identity for SaaS apps, integrating with custom apps",
              "id": "60b6b658-3a88-4370-8796-b005e80c07c9",
              "heading": "enforce identity for SaaS apps, integrating with custom apps"
            }
          ],
          "title": "How are you managing identity for this workload?",
          "type": "checkbox",
          "id": "4a320735-29fa-46a5-b416-f837d3eca165",
          "heading": "identity",
          "explanation": "In cloud-focused architecture, identity provides the basis of a large percentage\nof security assurances. While legacy IT infrastructure often heavily relied on\nfirewalls and network security solutions at the internet egress points for\nprotection against outside threats, these controls are less effective in cloud\narchitectures with shared services being accessed across cloud provider networks\nor the internet.\n\nIt is challenging or impossible to write concise firewall rules when you don’t\ncontrol the networks where these services are hosted, different cloud resources\nspin up and down dynamically, cloud customers may share common infrastructure,\nand employees and users expect to be able to access data and services from\nanywhere. To enable all these capabilities, you must manage access based on\nidentity authentication and authorization controls in the cloud services to\nprotect data and resources and to decide which requests should be permitted.\n\nAdditionally, using a cloud-based identity solution like Azure AD offers\nadditional security features that legacy identity services cannot because they\ncan apply threat intelligence from their visibility into a large volume of\naccess requests and threats across many customers.",
          "context": "Describes how to manage identities in your workload."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "You must think about security for all resources that take part in the workload. Azure Security Center provides integrated security monitoring and policy management across your Azure subscriptions. The events collected from the agents and from Azure are correlated in the security analytics engine to provide you tailored recommendations (hardening tasks), that you should follow to make sure your workloads are secure, and threat detection alerts. You should investigate such alerts as soon as possible to make sure malicious attacks aren't taking place on your workloads.",
              "priority": "medium",
              "title": "correlating calls across systems (end-to-end tracing)",
              "id": "814ddc02-75cb-4873-97ba-79248fad892e",
              "heading": "correlating calls across systems (end-to-end tracing)"
            },
            {
              "answer_tooltip": "",
              "output": "Risk of compromised admin accounts negating the value of all the other measures taken to ensure the confidentiality and integrity of data.\n\nAction:\nLimit and constrain administrative access.",
              "priority": "medium",
              "title": "Admin credential usage",
              "id": "62a2d031-ea56-4acf-b3c2-b939749564f0",
              "heading": "Admin credential usage"
            },
            {
              "answer_tooltip": "",
              "output": "Being on top of logs means a quicker response time to security events and better security program effectiveness. Not only will log analysis and daily monitoring demonstrate your willingness to comply with PCI DSS and HIPAA requirements, it will also help you defend against insider and outsider threats.\n\nRisk of compromised user credentials and suspicious activities occurring using these credentials.",
              "priority": "medium",
              "title": "Actively monitor logs for suspicious activity",
              "id": "67934fd1-d3ef-4850-a632-ec5b18e90541",
              "heading": "Actively monitor logs for suspicious activity"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Audit access log",
              "id": "02e4510e-5069-4569-ac53-c04431b986dd",
              "heading": "Audit access log"
            },
            {
              "answer_tooltip": "",
              "output": "Data security is essential for every business. Businesses rely on data storage and transactions to perform certain operations. Usage of data has increased business profitability and efficiency. At the same time, it also has potential security risks that could devastate a company.",
              "priority": "medium",
              "title": "Understand who has access to what data",
              "id": "b649d9e0-78a5-4ffe-89a1-16700e0ba36d",
              "heading": "Understand who has access to what data"
            },
            {
              "answer_tooltip": "",
              "output": "Use Azure Security Center for security management and advanced threat protection across hybrid cloud workloads. Review trace requests, analyze usage trends, and diagnose issues guidance.",
              "priority": "medium",
              "title": "Trace requests",
              "id": "00af55cf-b32d-49c5-9baf-84446d8f0dc1",
              "heading": "Trace requests"
            },
            {
              "answer_tooltip": "",
              "output": "Use Operations Management Suite (OMS) Security and Audit Solution to collect and processes data about resources.",
              "priority": "medium",
              "title": "How do you collect and process data about resources (security event log, Windows firewall log, antimalware assessment)?",
              "id": "51b68716-7d41-43f3-820b-f3086e5425bd",
              "heading": "How do you collect and process data about resources (security event log, Windows firewall log, antimalware assessment)?"
            },
            {
              "answer_tooltip": "",
              "output": "Utilize Azure Monitor to get the granular, up-to-date monitoring data all in one place. Review monitor, manage, and protect cloud infrastructure guidance.",
              "priority": "medium",
              "title": "Manage and protect infrastructure",
              "id": "27a38978-7a94-431a-80dd-a69dc2215f54",
              "heading": "Manage and protect infrastructure"
            },
            {
              "answer_tooltip": "",
              "output": "Utilize Azure Monitor to get the granular, up-to-date monitoring data all in one place. Use monitoring services.",
              "priority": "medium",
              "title": "Monitor resources",
              "id": "7f5f2838-cdd3-433e-8e73-1edd2ac47538",
              "heading": "Monitor resources"
            },
            {
              "answer_tooltip": "",
              "output": "Install and manage antimalware.",
              "priority": "medium",
              "title": "Manage antimalware",
              "id": "f309d196-d72c-4e10-b247-189c88888b40",
              "heading": "Manage antimalware"
            },
            {
              "answer_tooltip": "",
              "output": "Use Security Compliance Manager to import the current configuration by using either group policies based on Active Directory or configuration of a 'golden master' reference machine by using the LocalGPO tool. You can then import the local group policy into Security Compliance Manager.",
              "priority": "medium",
              "title": "Workload hardening",
              "id": "aaec024a-38e5-408b-aeb6-21936d363537",
              "heading": "Workload hardening"
            },
            {
              "answer_tooltip": "",
              "output": "Utilize DNS Analytics for gathering security, performance and operations-related insights of DNS servers.",
              "priority": "medium",
              "title": "DNS Monitoring",
              "id": "a5ef014a-105b-4677-927f-796add134010",
              "heading": "DNS Monitoring"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to have a single pane of visibility to prevent, detect, and respond to threats.\n\nAction:\nEmploy Azure Security Center for increased visibility into, and control over, the security of Azure resources, integrated security monitoring, and policy management across Azure subscriptions.",
              "priority": "medium",
              "title": "Threat response",
              "id": "757b2190-c991-4241-84ee-994b74489ba0",
              "heading": "Threat response"
            }
          ],
          "title": "How are you monitoring the security posture of your workload?",
          "type": "checkbox",
          "id": "ccd01d9d-a4db-49c3-8417-ae261fa4bf06",
          "heading": "monitoring",
          "explanation": "Describes monitoring strategies that you can use in your workload."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Potential of smaller-scale attack that doesn't trip the platform-level protection.\n\nAction:\nUse Azure DDoS Protection to prevent volumetric attacks, protocol attacks, and resource (application)-layer attacks.",
              "priority": "medium",
              "title": "DDos Protection",
              "id": "0b02951b-c60d-4593-a7c8-5a9e5ac645cd",
              "heading": "DDos Protection"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to provision VMs with private IP addresses for protection.\n\nAction:\nUse Azure Firewall for built-in high availability and unrestricted cloud scalability. Utilize Azure IP address to determine which traffic is passed in, and how and where it's translated on to the virtual network.",
              "priority": "medium",
              "title": "How do you configure public IPs for which traffic is passed in, and how and where it's translated",
              "id": "ef437d0c-6b4f-43e6-9679-9a6e6ff89b1e",
              "heading": "How do you configure public IPs for which traffic is passed in, and how and where it's translated"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to ensure VMs and communication between them remains private within a network boundary.\n\nAction:\nUse Azure Virtual Network to allow VMs to securely communicate with each other, the Internet, and on- premises networks.",
              "priority": "medium",
              "title": "Isolate network traffic",
              "id": "c7466891-5bdd-4a2b-b2c1-b26c78d18bbf",
              "heading": "Isolate network traffic"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to define different access policies based on the workload types, and to control traffic flows between them.\n\nAction:\nEmploy Azure Virtual Network Subnet to designate separate address spaces for different elements or 'tiers' within the workload, define different access policies, and control traffic flows between the tiers.",
              "priority": "medium",
              "title": "Traffic flow between tiers",
              "id": "8e3f022b-c5a8-468d-b3ea-c4b55b26b0ac",
              "heading": "Traffic flow between tiers"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to define communication paths between different tiers within a network boundary.\n\nAction:\nUse Azure Virtual Network User Defined Routes (UDR) to control next hop for traffic between Azure, on-premises, and Internet resources through virtual appliance, virtual network gateway, virtual network, or Internet.",
              "priority": "medium",
              "title": "Security appliances and boundary policy enforcement",
              "id": "d67104c2-6210-49ee-9fdc-318b454bff6b",
              "heading": "Security appliances and boundary policy enforcement"
            },
            {
              "answer_tooltip": "",
              "output": "Possibility of not being able to select comprehensive solutions for secure network boundaries.\n\nAction:\nUse Network Appliances from Azure Marketplace to deploy a variety of pre-configured network virtual appliances. Utilize Application Gateway WAF to detect and protect against common web attacks.",
              "priority": "medium",
              "title": "Firewalls, load balancers, and intrusion detection systems",
              "id": "1f4f8735-53e6-4966-8cc2-8eebb9ae181e",
              "heading": "Firewalls, load balancers, and intrusion detection systems"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to allow or deny inbound network traffic to, or outbound network traffic from, within larger network space.\n\nAction:\nUse network security groups (NSGs) to allow or deny traffic to and from single IP address, to and from multiple IP addresses, or even to and from entire subnets.",
              "priority": "medium",
              "title": "Segmenting address space",
              "id": "446b4aaa-3e24-4ca6-b68d-6fd73a186e91",
              "heading": "Segmenting address space"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to customize the routing configuration.\n\nAction:\nEmploy Azure Virtual Network User Defined Routes (UDR) to customize the routing configuration for deployments.",
              "priority": "medium",
              "title": "Routing",
              "id": "412b853a-eb19-4dc6-9449-dbbfd1ca40d4",
              "heading": "Routing"
            },
            {
              "answer_tooltip": "",
              "output": "Potential of outbound connections from any VM increasing attack surface area leveraged by attackers.\n\nAction:\nUtilize forced tunneling to ensure that connections to the Internet go through corporate network security devices.",
              "priority": "medium",
              "title": "Forced tunneling",
              "id": "6811873a-1805-4c4e-90c5-2a5f5d799ea8",
              "heading": "Forced tunneling"
            },
            {
              "answer_tooltip": "",
              "output": "Potential of access to company’s information assets on-premises.\n\nAction:\nUse Azure site-to-site VPN or ExpressRoute to set up cross- premises connectivity to on- premises networks.",
              "priority": "medium",
              "title": "Cross-site connectivity",
              "id": "268aea68-4831-47db-920e-b95d02c12e3c",
              "heading": "Cross-site connectivity"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to make services available even when datacenters might become unavailable.\n\nAction:\nUtilize Azure Traffic Manager to load balance connections to services based on the location of the user and/or other criteria.",
              "priority": "medium",
              "title": "Global load balancing",
              "id": "620290ad-c04b-44d2-9b42-3d8b03c0dc5b",
              "heading": "Global load balancing"
            },
            {
              "answer_tooltip": "",
              "output": "Potential for attackers to use brute force techniques to gain access and launch other attacks.\n\nAction:\nDisable RDP/SSH access to Azure Virtual Machines and use VPN/ExpressRoute to access these virtual machines for remote management.",
              "priority": "medium",
              "title": "Disable RDP/SSH Access",
              "id": "16a0b466-b486-42ae-bfcc-a03a2ee82bab",
              "heading": "Disable RDP/SSH Access"
            },
            {
              "answer_tooltip": "",
              "output": "Inability to identify the issue and use the detailed logs for further investigation.\n\nAction:\nUse Network Watcher troubleshooter to diagnose most common VPN gateway and connections issues.",
              "priority": "medium",
              "title": "VPN connectivity",
              "id": "b82b09b1-01b8-4dcd-85dd-4347fdca0807",
              "heading": "VPN connectivity"
            }
          ],
          "title": "How have you secured the network of your workload?",
          "type": "checkbox",
          "id": "0f7738e0-b918-49db-86ae-c57c6ee12e76",
          "heading": "Network",
          "explanation": "Network security has been the traditional lynchpin of enterprise security\nefforts. However, cloud computing has increased the requirement for network\nperimeters to be more porous and many attackers have mastered the art of attacks\non identity system elements (which nearly always bypass network controls). These\nfactors have increased the need to focus primarily on identity-based access\ncontrols to protect resources rather than network-based access controls.\n\nThese do diminish the role of network security controls, but do not eliminate it\nentirely. While network security is no longer the primary focus for securing\ncloud-based assets, it is still a top priority for the large portfolio of legacy\nassets (which were built with the assumption that a network firewall-based\nperimeter was in place). Many attackers still employ scanning and exploit\nmethods across public cloud provider IP ranges, successfully penetrating\ndefenses for those who don’t follow basic network security hygiene. Network\nsecurity controls also provide a defense-in-depth element to your strategy that\nhelp protect, detect, contain, and eject attackers who make their way into your\ncloud deployments.\n\nIn the category of network security and containment, we have the following best\npractice recommendations:\n\n-   Align network segmentation with overall strategy\n\n-   Centralize network management and security\n\n-   Build a network containment strategy\n\n-   Define an internet edge strategy",
          "context": "Describes security considerations to make when building out the network for your workload."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Pen testing provides a way determine if the system is resilient to security attacks. This also identifies the expose vulnerabilities and the extend of attack.",
              "priority": "medium",
              "title": "Penetration testing",
              "id": "98f068a5-a1ad-4e8f-9b5a-af57b1c98fd4",
              "heading": "Penetration testing"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Vulnerability scanning",
              "id": "09e1c678-fa9e-42b7-acb9-36b1f9988525",
              "heading": "Vulnerability scanning"
            }
          ],
          "title": "How are you testing the security of your workload?",
          "type": "checkbox",
          "id": "486a0691-464f-432b-a24f-233bf11147e4",
          "heading": "testing",
          "explanation": "Describes considerations to make when testing the security of a given workload."
        },
        {
          "choices": [
            {
              "answer_tooltip": "",
              "output": "Protecting identity, data, physical resources, and other assets is crucial.",
              "priority": "medium",
              "title": "Consider the impact of adversaries",
              "id": "87772841-b321-4bae-9c81-3bbf98af4303",
              "heading": "Consider the impact of adversaries"
            },
            {
              "answer_tooltip": "",
              "output": "Attacker can gain access to the system and change the authentication data, read user profile data, and gain access to user database.",
              "priority": "medium",
              "title": "Modelling both internal and external threats",
              "id": "b3b4ba92-898a-47ac-9cf3-1106844e4aed",
              "heading": "Modelling both internal and external threats"
            },
            {
              "answer_tooltip": "",
              "output": "",
              "priority": "medium",
              "title": "Catalog potential entry points for bad actors",
              "id": "67376cc2-d11d-4bc9-83c3-36a97d2f098b",
              "heading": "Catalog potential entry points for bad actors"
            },
            {
              "answer_tooltip": "",
              "output": "An attack surface is the total number of points or vectors through which an attacker can try to enter an environment or network. In terms of cybersecurity, this means how a bad actor could gain access to your network to either send, extract, or encrypt data through the holes they find in a network. Every network interaction point allows for a potential network attack surface. In other words, an attack on a surface occurs when bad actors can find holes in a network which then allows them to cause disruption. This disruption can come in the forms of data breaches, ransomware, and more.",
              "priority": "medium",
              "title": "Attack surface",
              "id": "85e22d16-d1c9-4182-adf4-e3176a03064b",
              "heading": "Attack surface"
            }
          ],
          "title": "How are you modelling the threats that will impact this workload?",
          "type": "checkbox",
          "id": "c30847ef-0489-4ad7-9e57-94eea7426afb",
          "explanation": "Describes how to model threats for your workload.",
          "heading": "threat modelling"
        },
        {
          "choices": [],
          "title": "What tradeoffs do you need to make to meet your security goals?",
          "type": "checkbox",
          "id": "158f50a2-4d8d-4816-8798-315a2191002a",
          "heading": "tradeoffs",
          "explanation": "Describes considerations you may need to make to meet your security goals."
        }
      ],
      "name": "Security",
      "id": "a921040b-a5a8-4467-861d-ecd4b68b4bfa"
    }
  ]
}